{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Coursework_2416218c.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "UiQWYyH7eMad",
        "APqToKUNdwa8",
        "bLoPOeZQaqfQ",
        "ztn89s8eawXd",
        "I6m2mdpua4Xb",
        "aDtJDDSla9cQ",
        "rPlpOkaobAkb",
        "LNHeEaxa3-5q",
        "eVW8Em2rY3_u",
        "i76TFQXli55I",
        "Doc_8Q1peus3",
        "jjlHoZ4le0DD",
        "7NrCl6DHe6gm",
        "nyGrYk4Lg-C4",
        "ZBuwD_4fhPRv"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "2gv2i9fNDvrg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part A: Subreddit Prediction ##"
      ]
    },
    {
      "metadata": {
        "id": "UiQWYyH7eMad",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Preparation"
      ]
    },
    {
      "metadata": {
        "id": "ZG9BpbQt3-ko",
        "colab_type": "code",
        "outputId": "ff1c904e-a222-48fa-c50e-0a470b4682d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "cell_type": "code",
      "source": [
        "subreddit_train = \"coursework_subreddit_train.json\"\n",
        "subreddit_test = \"coursework_subreddit_test.json\"\n",
        "\n",
        "!gsutil cp gs://textasdata/coursework/coursework_subreddit_train.json $subreddit_train \n",
        "!gsutil cp gs://textasdata/coursework/coursework_subreddit_test.json  $subreddit_test"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://textasdata/coursework/coursework_subreddit_train.json...\n",
            "/ [1 files][ 10.1 MiB/ 10.1 MiB]                                                \n",
            "Operation completed over 1 objects/10.1 MiB.                                     \n",
            "Copying gs://textasdata/coursework/coursework_subreddit_test.json...\n",
            "/ [1 files][  2.7 MiB/  2.7 MiB]                                                \n",
            "Operation completed over 1 objects/2.7 MiB.                                      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iCEG8t6PC2f7",
        "colab_type": "code",
        "outputId": "3d03b6dc-9618-4247-bc0d-9ebafabf0a0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_threads = pd.read_json(path_or_buf=subreddit_train, lines=True)\n",
        "print(list(train_threads.columns.values))\n",
        "print(train_threads.head())\n",
        "print(train_threads.size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['is_self_post', 'posts', 'subreddit', 'title', 'url']\n",
            "   is_self_post                                              posts  \\\n",
            "0           1.0  [{'body': 'I think everyone has that one frien...   \n",
            "1           1.0  [{'body': 'I not 100% sure this is the right p...   \n",
            "2           1.0  [{'body': '', 'author': 'Leisure321', 'url': '...   \n",
            "3           1.0  [{'body': 'It's called 'forgetting things'.', ...   \n",
            "4           1.0  [{'body': 'How would I do this? I am looking t...   \n",
            "\n",
            "        subreddit                                              title  \\\n",
            "0   relationships  How do I [23F] communicate with my self-center...   \n",
            "1  summonerschool  What Cherry switch do you recommend for League...   \n",
            "2       askreddit                   Where do memes go when they die?   \n",
            "3           trees                     Some weird long term affects??   \n",
            "4        buildapc  Simple question: If I install Windows to a sta...   \n",
            "\n",
            "                                                 url  \n",
            "0  https://www.reddit.com/r/relationships/comment...  \n",
            "1  https://www.reddit.com/r/summonerschool/commen...  \n",
            "2  https://www.reddit.com/r/AskReddit/comments/4d...  \n",
            "3  https://www.reddit.com/r/trees/comments/1h300m...  \n",
            "4  https://www.reddit.com/r/buildapc/comments/jhb...  \n",
            "7280\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "89UU3g27C8SZ",
        "colab_type": "code",
        "outputId": "10e41caa-d1ae-48ab-bf1a-6d68edb0acd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "cell_type": "code",
      "source": [
        "test_threads = pd.read_json(path_or_buf=subreddit_test, lines=True)\n",
        "print(test_threads.head())\n",
        "print(test_threads.size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   is_self_post                                              posts  \\\n",
            "0           1.0  [{'body': 'Was watching a VOD from last years ...   \n",
            "1           1.0  [{'body': 'Basically what the title says.', 'u...   \n",
            "2           1.0  [{'body': '', 'author': 'Daft-Punk', 'url': 'h...   \n",
            "3           1.0  [{'body': 'I start running this year. I do it ...   \n",
            "4           1.0  [{'body': '[deleted]', 'url': 'https://www.red...   \n",
            "\n",
            "       subreddit                                              title  \\\n",
            "0      starcraft  Just a reminder on how much SC2 has evolved th...   \n",
            "1    whowouldwin  Your Favorite Hero Now Has A Healing Factor As...   \n",
            "2      askreddit  If you could live anywhere in the world, where...   \n",
            "3      askreddit                   Do you ever get use to exercise?   \n",
            "4  tipofmytongue         [TOMT] [book] A scary french book for kids   \n",
            "\n",
            "                                                 url  \n",
            "0  https://www.reddit.com/r/starcraft/comments/mq...  \n",
            "1  https://www.reddit.com/r/whowouldwin/comments/...  \n",
            "2  https://www.reddit.com/r/AskReddit/comments/27...  \n",
            "3  https://www.reddit.com/r/AskReddit/comments/x9...  \n",
            "4  https://www.reddit.com/r/tipofmytongue/comment...  \n",
            "1825\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3Nl9qzazDQ_6",
        "colab_type": "code",
        "outputId": "2cb7b685-3523-47bf-d2ee-df3177cdf8a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        }
      },
      "cell_type": "code",
      "source": [
        "subreddit_counts = train_threads['subreddit'].value_counts()\n",
        "print(subreddit_counts.describe())\n",
        "top_subbreddits = subreddit_counts.nlargest(20)\n",
        "top_subbreddits_list = top_subbreddits.index.tolist()\n",
        "print(top_subbreddits)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "count     20.000000\n",
            "mean      72.800000\n",
            "std       73.368285\n",
            "min       28.000000\n",
            "25%       36.250000\n",
            "50%       45.500000\n",
            "75%       63.750000\n",
            "max      334.000000\n",
            "Name: subreddit, dtype: float64\n",
            "askreddit               334\n",
            "leagueoflegends         196\n",
            "buildapc                131\n",
            "explainlikeimfive        82\n",
            "trees                    66\n",
            "techsupport              63\n",
            "gaming                   62\n",
            "pcmasterrace             62\n",
            "electronic_cigarette     59\n",
            "relationships            48\n",
            "tipofmytongue            43\n",
            "hearthstone              38\n",
            "jailbreak                38\n",
            "summonerschool           37\n",
            "atheism                  37\n",
            "reddit.com               34\n",
            "whowouldwin              33\n",
            "movies                   33\n",
            "personalfinance          32\n",
            "starcraft                28\n",
            "Name: subreddit, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "APqToKUNdwa8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Q1"
      ]
    },
    {
      "metadata": {
        "id": "Bvkl0dS6cQ0g",
        "colab_type": "code",
        "outputId": "751390fb-c562-4be8-e2b3-82cd88928a8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2657
        }
      },
      "cell_type": "code",
      "source": [
        "#construct train data dataframe\n",
        "import json\n",
        "train_temp = list()\n",
        "\n",
        "with open(subreddit_train) as jsonfile:\n",
        "  for i, line in enumerate(jsonfile):\n",
        "    thread = json.loads(line)\n",
        "    posts = \"\"\n",
        "    authors = \"\"\n",
        "    # Keep information about the source thread where the post comes from.\n",
        "    for post in thread['posts']:\n",
        "      posts+=post.get('body', \"\") + \" \"\n",
        "      authors+=post.get('author', \"\") + \" \"\n",
        "      len_of_posts = str(len(posts))\n",
        "      len_of_thread = str(len(thread['subreddit']))\n",
        "    \n",
        "    train_temp.append((thread['subreddit'], \n",
        "                    thread['title'], \n",
        "                    authors,\n",
        "                    posts,\n",
        "                    len_of_posts,\n",
        "                    len_of_thread))\n",
        "print(len(train_temp))\n",
        "\n",
        "labels = ['subreddit', 'title',  'author', 'body', 'len_of_posts', 'len_of_thread']\n",
        "train_data = pd.DataFrame(train_temp, columns=labels)\n",
        "train_data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1456\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>subreddit</th>\n",
              "      <th>title</th>\n",
              "      <th>author</th>\n",
              "      <th>body</th>\n",
              "      <th>len_of_posts</th>\n",
              "      <th>len_of_thread</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>relationships</td>\n",
              "      <td>How do I [23F] communicate with my self-center...</td>\n",
              "      <td>Pouritdownmythroat WhyFrankWhy Pouritdownmythr...</td>\n",
              "      <td>I think everyone has that one friend who loves...</td>\n",
              "      <td>2518</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>summonerschool</td>\n",
              "      <td>What Cherry switch do you recommend for League...</td>\n",
              "      <td>ThisGermanGuy shaunrnm Sub_Salac MisterBlack...</td>\n",
              "      <td>I not 100% sure this is the right place to pos...</td>\n",
              "      <td>3358</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>askreddit</td>\n",
              "      <td>Where do memes go when they die?</td>\n",
              "      <td>Leisure321 Zeolance  Buttersgoo23 quzimaa Esca...</td>\n",
              "      <td>Facebook  9gag Memes never truly die Dickbutt...</td>\n",
              "      <td>136</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>trees</td>\n",
              "      <td>Some weird long term affects??</td>\n",
              "      <td>refugee4chan donquixote6179 ThatStonedAsianGuy</td>\n",
              "      <td>It's called 'forgetting things'. Dude i dont t...</td>\n",
              "      <td>291</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>buildapc</td>\n",
              "      <td>Simple question: If I install Windows to a sta...</td>\n",
              "      <td>catalyzeme uses rvabdn greg2709 spydr101</td>\n",
              "      <td>How would I do this? I am looking to wait anot...</td>\n",
              "      <td>1449</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>whowouldwin</td>\n",
              "      <td>Sentry vs. Swamp Thing</td>\n",
              "      <td>gpacman21 jeansplice Ame-no-nobuko vadergeek A...</td>\n",
              "      <td>[Respect Sentry.](http://www.reddit.com/r/resp...</td>\n",
              "      <td>3214</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>summonerschool</td>\n",
              "      <td>Hows zed going this season?</td>\n",
              "      <td>ellosmello fozzix VarusMidOrAfk I_know_where_y...</td>\n",
              "      <td>Is he still a good pick? I played him alot in ...</td>\n",
              "      <td>1025</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>trees</td>\n",
              "      <td>Favorite hip-hop/r&amp;b to listen to while high?</td>\n",
              "      <td>walkngonawire ResonantCascade snicklepants jui...</td>\n",
              "      <td>We all know about stoner rock, stoner metal, s...</td>\n",
              "      <td>1847</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>personalfinance</td>\n",
              "      <td>Opening an IRA as a college student</td>\n",
              "      <td>patrick6h aBoglehead thisismyfirstpost twblalock</td>\n",
              "      <td>Hi, I am looking to open an IRA while I still ...</td>\n",
              "      <td>1025</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>leagueoflegends</td>\n",
              "      <td>LCS spots secured for corporations or players?!</td>\n",
              "      <td>CrazyChunckMaster420 xmodusterz Frogad Waheyy ...</td>\n",
              "      <td>I am just shocked by how Riot decided to handl...</td>\n",
              "      <td>1334</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>askreddit</td>\n",
              "      <td>Should I tip furniture delivery folk?</td>\n",
              "      <td>throwaway51711 Lorimor  Lorimor GCanuck throwa...</td>\n",
              "      <td>I've purchased a large quantity of furniture, ...</td>\n",
              "      <td>2012</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>techsupport</td>\n",
              "      <td>You must restart your computer to apply these ...</td>\n",
              "      <td>jcsix CharlieKillsRats jcsix  jcsix</td>\n",
              "      <td>I noticed this message appeared when I came ba...</td>\n",
              "      <td>1079</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>buildapc</td>\n",
              "      <td>[Build Help]Is this motherboard good? and does...</td>\n",
              "      <td>jamvanderloeff</td>\n",
              "      <td>[deleted] Yes, it will work with your build.  ...</td>\n",
              "      <td>1379</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>pcmasterrace</td>\n",
              "      <td>It's that time of year again</td>\n",
              "      <td>ParadoxAnarchy VeteranCommander 28thsundown Ez...</td>\n",
              "      <td>Reading that on the article made me sick.\\nAn...</td>\n",
              "      <td>932</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>hearthstone</td>\n",
              "      <td>How do I beat the priest?</td>\n",
              "      <td>vanasbry000 Jinjetsu derindel Skyegg Doughy12...</td>\n",
              "      <td>I have lost 11 games in a row now. This is aga...</td>\n",
              "      <td>5569</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>buildapc</td>\n",
              "      <td>So I randomly decided to set up a NAS.</td>\n",
              "      <td>SPL1TT3R gtgoku SPL1TT3R gtgoku SPL1TT3R gtgok...</td>\n",
              "      <td>###Build Help/Ready:\\n\\n**Have you read the si...</td>\n",
              "      <td>6202</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>askreddit</td>\n",
              "      <td>People of Reddit: What Great Lectures or Talks...</td>\n",
              "      <td>CalonTost Winkn cantthinkofaCO0Lname badmak ka...</td>\n",
              "      <td>What are some good/interesting talks or lectur...</td>\n",
              "      <td>853</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>explainlikeimfive</td>\n",
              "      <td>ELI5: Why does my voice sound different in my ...</td>\n",
              "      <td>Tape_measure scienceteacherguy Tape_measure br...</td>\n",
              "      <td>When you hear someone elses voice, you are p...</td>\n",
              "      <td>1624</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>summonerschool</td>\n",
              "      <td>Jungling at Summoner Level 10: Some data about...</td>\n",
              "      <td>KR_shmungler zedisfied_  Osiato</td>\n",
              "      <td>It's probably not a good idea to start junglin...</td>\n",
              "      <td>11021</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>techsupport</td>\n",
              "      <td>Few questions about RAM</td>\n",
              "      <td>Nedrin BecomeWind ColKrismiss rcmaehl ColKrism...</td>\n",
              "      <td>posted this on Askcomputerscience, but it does...</td>\n",
              "      <td>5795</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>trees</td>\n",
              "      <td>How hard would it be to get a medicinal licens...</td>\n",
              "      <td>HilbertSystem frisbeefrank HilbertSystem frisb...</td>\n",
              "      <td>I'm headed over to San Francisco at the end of...</td>\n",
              "      <td>499</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>askreddit</td>\n",
              "      <td>What is a good book to you?</td>\n",
              "      <td>Hlaufenberg Noomsi jeffreyharharwood  sco154</td>\n",
              "      <td>[deleted] Fear and Loathing in Las Vegas Tales...</td>\n",
              "      <td>120</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>explainlikeimfive</td>\n",
              "      <td>ELI5: How do we know what the centre of the ea...</td>\n",
              "      <td>ouissellat ofMindandHeart Teekno riconquer</td>\n",
              "      <td>If we've only drug 12km it seems that we're ju...</td>\n",
              "      <td>1969</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>buildapc</td>\n",
              "      <td>Got a used heatsink on eBay, turned out to be ...</td>\n",
              "      <td>deadgreysn0w froschkonig deadgreysn0w froschko...</td>\n",
              "      <td>I ordered a Hyper 212 Cooler Master on eBay, b...</td>\n",
              "      <td>1862</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>hearthstone</td>\n",
              "      <td>So about this new Murloc Card</td>\n",
              "      <td>Jayavarmen OyleSlyck Finckel dz5b605 Jayavarme...</td>\n",
              "      <td>http://i.imgur.com/076pqZ9.jpg\\nI actually had...</td>\n",
              "      <td>1949</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>buildapc</td>\n",
              "      <td>First timer, building from scratch</td>\n",
              "      <td>arlanTLDR  arlanTLDR omglook joeshlub someawes...</td>\n",
              "      <td>So I've never built a computer before, but I'm...</td>\n",
              "      <td>1797</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>askreddit</td>\n",
              "      <td>If you could only visit 5 websites for the res...</td>\n",
              "      <td>French87 LakeAndTheRiver dumandizzy inclinedto...</td>\n",
              "      <td>No loopholes like \"hurr duurrrr I'd choose goo...</td>\n",
              "      <td>571</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>askreddit</td>\n",
              "      <td>What is your mental landscape while you dream?</td>\n",
              "      <td>thejumpingasian QwertyPoint TheUnheard blah_b...</td>\n",
              "      <td>Mine tends to be either near water or near mou...</td>\n",
              "      <td>2892</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>askreddit</td>\n",
              "      <td>Sociopaths of Reddit, assuming you have no emp...</td>\n",
              "      <td>aminakoydum random_____user elvagabundotonto a...</td>\n",
              "      <td>Myself.  Sociopaths don't necessarily like th...</td>\n",
              "      <td>817</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>explainlikeimfive</td>\n",
              "      <td>ELI5: How do antennas work?</td>\n",
              "      <td>Teillu ToxiClay idetectanerd armena vwlsmssng ...</td>\n",
              "      <td>Simply put, electric power is pumped into a l...</td>\n",
              "      <td>2657</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1426</th>\n",
              "      <td>jailbreak</td>\n",
              "      <td>[Help/Question][iOS 8.1.1] Batterylife caused ...</td>\n",
              "      <td>ReddestDream  ReddestDream  ReddestDream</td>\n",
              "      <td>I updated batterylife and enabled true percent...</td>\n",
              "      <td>947</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1427</th>\n",
              "      <td>leagueoflegends</td>\n",
              "      <td>What programs and hardware do I need to record...</td>\n",
              "      <td>cabbius maximumcrisis Nekture Hatelore</td>\n",
              "      <td>I have fraps (full version) and my computer is...</td>\n",
              "      <td>1311</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1428</th>\n",
              "      <td>askreddit</td>\n",
              "      <td>Why do I feel like such a creep when I watch a...</td>\n",
              "      <td>Im_not_creepy_am_I cdude HastaLasagna cdude  I...</td>\n",
              "      <td>White guy, university student in north america...</td>\n",
              "      <td>2845</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1429</th>\n",
              "      <td>gaming</td>\n",
              "      <td>I love you guys, but this needs to stop.</td>\n",
              "      <td>Amphabian desertREAPER01 NGRoachClip JakeSteam</td>\n",
              "      <td>Surely I'm not the only one who's tired of the...</td>\n",
              "      <td>962</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1430</th>\n",
              "      <td>relationships</td>\n",
              "      <td>I [29 F] have been single for ages and now I d...</td>\n",
              "      <td>singleforever_ Harangue_itang singleforever_ H...</td>\n",
              "      <td>I hope this is the right place to post this. I...</td>\n",
              "      <td>3395</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1431</th>\n",
              "      <td>starcraft</td>\n",
              "      <td>What chairs do the pros use? We always talk ab...</td>\n",
              "      <td>blueboybob brikken AllinIRL SnowJoust bfish510...</td>\n",
              "      <td>You're pretty much talking about anyone with ...</td>\n",
              "      <td>5471</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1432</th>\n",
              "      <td>askreddit</td>\n",
              "      <td>Math is not my subject, so I need help with th...</td>\n",
              "      <td>tatom TRUBored  gumpy5 T400 gumpy5 T400</td>\n",
              "      <td>[Youtube link - Android Unlock](http://www.you...</td>\n",
              "      <td>800</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1433</th>\n",
              "      <td>explainlikeimfive</td>\n",
              "      <td>ELI5: How come so many ancient countries (and ...</td>\n",
              "      <td>Bubminy palcatraz JesusaurusPrime StupidLemonE...</td>\n",
              "      <td>Well, in many ancient civilisations, people w...</td>\n",
              "      <td>2266</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1434</th>\n",
              "      <td>pcmasterrace</td>\n",
              "      <td>Recently ascended. Games are crashing. I'm los...</td>\n",
              "      <td>Rosy_Ps_Boyfriend Goofybud16 Rosy_Ps_Boyfriend...</td>\n",
              "      <td>I recently built my first PC. I was a casual P...</td>\n",
              "      <td>9780</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1435</th>\n",
              "      <td>tipofmytongue</td>\n",
              "      <td>[TOMT]Reddit post about a girl who had a creep...</td>\n",
              "      <td>BraddardStark  BraddardStark  BraddardStark Br...</td>\n",
              "      <td>I remember reading the story a while ago now, ...</td>\n",
              "      <td>649</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1436</th>\n",
              "      <td>askreddit</td>\n",
              "      <td>Who is a popular musician/band in your country...</td>\n",
              "      <td>BlueD_ Munchman5000 BlueD_ Munchman5000 Munchm...</td>\n",
              "      <td>Mainly aiming this at non British/US Redditors...</td>\n",
              "      <td>471</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1437</th>\n",
              "      <td>tipofmytongue</td>\n",
              "      <td>[TOMT] A 2010 folk/bluegrass music video</td>\n",
              "      <td>losemybreath yiotta losemybreath</td>\n",
              "      <td>At least I'm pretty sure it was from last year...</td>\n",
              "      <td>1300</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1438</th>\n",
              "      <td>gaming</td>\n",
              "      <td>My personal question regarding DLC (Long-winde...</td>\n",
              "      <td>bigsol81 nomofica trolli_mctroll nomofica trol...</td>\n",
              "      <td>I've been seeing a lot about the recent debacl...</td>\n",
              "      <td>3788</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1439</th>\n",
              "      <td>leagueoflegends</td>\n",
              "      <td>Am I the only one who thinks that Fiddle's \"Dr...</td>\n",
              "      <td>Velkyria 420donglord TheBeastOnFire Defiantific</td>\n",
              "      <td>he doesn't need a buff he's already a strong ...</td>\n",
              "      <td>251</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1440</th>\n",
              "      <td>reddit.com</td>\n",
              "      <td>She Said Goodbye! I'm getting divorced, and to...</td>\n",
              "      <td>mandysteve mandysteve    peg_leg  Garbagio qqu...</td>\n",
              "      <td>My Reddit bros rock! You can hang with us bro...</td>\n",
              "      <td>373</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1441</th>\n",
              "      <td>askreddit</td>\n",
              "      <td>Reddit, whats the strangest thing you have see...</td>\n",
              "      <td>ClavicleRub twelvedayslate Blue_Stuff pridwyn ...</td>\n",
              "      <td>I *always* find it strange when a couple gets...</td>\n",
              "      <td>3795</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1442</th>\n",
              "      <td>relationships</td>\n",
              "      <td>Update - Boyfriend sent nude pics to ex - is t...</td>\n",
              "      <td>trajicrabbit biteysaur dorthyway SamsIphone tr...</td>\n",
              "      <td>This is an update to the following [post](http...</td>\n",
              "      <td>3199</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1443</th>\n",
              "      <td>askreddit</td>\n",
              "      <td>For Redditors with girlfriends, what was the f...</td>\n",
              "      <td>Viperking5  taichi_duck     Eternal_Princess_...</td>\n",
              "      <td>\"eyy bby wnt sum fuk?\" And she said?  \"Sucky!...</td>\n",
              "      <td>1209</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1444</th>\n",
              "      <td>movies</td>\n",
              "      <td>Watching Fight Club for the first time tonight</td>\n",
              "      <td>Mac1822 highlysober detroyer  FuckCracker  Ajz...</td>\n",
              "      <td>Can't talk about it.  [groan] I still haven't ...</td>\n",
              "      <td>3882</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1445</th>\n",
              "      <td>atheism</td>\n",
              "      <td>A thought that has occurred to me about Christ...</td>\n",
              "      <td>Tthrond76   aflarge  aflarge  aflarge  aflarge</td>\n",
              "      <td>First and foremost I want to make absolutely c...</td>\n",
              "      <td>2291</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1446</th>\n",
              "      <td>askreddit</td>\n",
              "      <td>What makes you want to get out of bed? [serious]</td>\n",
              "      <td>what_the_rock_cooked Nambot what_the_rock_cook...</td>\n",
              "      <td>What drives you? What makes you wanna push for...</td>\n",
              "      <td>3400</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1447</th>\n",
              "      <td>techsupport</td>\n",
              "      <td>2nd Monitor isn't having any output at all, help!</td>\n",
              "      <td>Mars200 Mars200 m1kepro Mars200</td>\n",
              "      <td>With the most recent AMD update my 2nd monitor...</td>\n",
              "      <td>910</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1448</th>\n",
              "      <td>leagueoflegends</td>\n",
              "      <td>A small gp change idea</td>\n",
              "      <td>Unknof SGTQuackers Protonis Unknof Unknof Pr0d...</td>\n",
              "      <td>It´s not a big deal but i hate that you stop m...</td>\n",
              "      <td>833</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1449</th>\n",
              "      <td>trees</td>\n",
              "      <td>Midterm election day is tomorrow, and there is...</td>\n",
              "      <td>EBDoo  EBDoo DDiggler321 MrNewAndImprove EBDoo</td>\n",
              "      <td>I'm not very involved in politics, but I do ex...</td>\n",
              "      <td>1231</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1450</th>\n",
              "      <td>leagueoflegends</td>\n",
              "      <td>Riot, there's still time to make World's quart...</td>\n",
              "      <td>tiaow DeepSpacePandaEUW tiaow dudaseifert Deep...</td>\n",
              "      <td>Come on guys.. it would sort out any \"intentio...</td>\n",
              "      <td>1138</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1451</th>\n",
              "      <td>leagueoflegends</td>\n",
              "      <td>2 Desktop Banner that I made! Thoughts?</td>\n",
              "      <td>IcybossPlays darkaxefire butttcrackjoe 252978104</td>\n",
              "      <td>http://prntscr.com/60tux4 &lt;--- Rengar, Zed, Ri...</td>\n",
              "      <td>297</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1452</th>\n",
              "      <td>pcmasterrace</td>\n",
              "      <td>Got this nice wallpaper through Uplay rewards ...</td>\n",
              "      <td>kswitch5022 psyEDk   Col_Crunch  Arch_0</td>\n",
              "      <td>I like it. not to nit pick .. okay i'm nit pi...</td>\n",
              "      <td>752</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1453</th>\n",
              "      <td>gaming</td>\n",
              "      <td>What are your views on watching someone play t...</td>\n",
              "      <td>GoldLeaderLando aedgar777 Xirii screamingherbe...</td>\n",
              "      <td>I live with two brothers and we buy our own ga...</td>\n",
              "      <td>1742</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1454</th>\n",
              "      <td>relationships</td>\n",
              "      <td>[20 M] When I get into a relationship, should ...</td>\n",
              "      <td>funkieturtle DoneAllWrong hopefulthr0waway fun...</td>\n",
              "      <td>My previous question was removed because I did...</td>\n",
              "      <td>1761</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1455</th>\n",
              "      <td>leagueoflegends</td>\n",
              "      <td>Why is there no mana biscuit? I'd love to have...</td>\n",
              "      <td>Master_Cen DaylightDarkle BadMessiah Master_Ce...</td>\n",
              "      <td>Health biscuit is nice and stuff, but since it...</td>\n",
              "      <td>865</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1456 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              subreddit                                              title  \\\n",
              "0         relationships  How do I [23F] communicate with my self-center...   \n",
              "1        summonerschool  What Cherry switch do you recommend for League...   \n",
              "2             askreddit                   Where do memes go when they die?   \n",
              "3                 trees                     Some weird long term affects??   \n",
              "4              buildapc  Simple question: If I install Windows to a sta...   \n",
              "5           whowouldwin                             Sentry vs. Swamp Thing   \n",
              "6        summonerschool                        Hows zed going this season?   \n",
              "7                 trees      Favorite hip-hop/r&b to listen to while high?   \n",
              "8       personalfinance                Opening an IRA as a college student   \n",
              "9       leagueoflegends    LCS spots secured for corporations or players?!   \n",
              "10            askreddit              Should I tip furniture delivery folk?   \n",
              "11          techsupport  You must restart your computer to apply these ...   \n",
              "12             buildapc  [Build Help]Is this motherboard good? and does...   \n",
              "13         pcmasterrace                       It's that time of year again   \n",
              "14          hearthstone                          How do I beat the priest?   \n",
              "15             buildapc             So I randomly decided to set up a NAS.   \n",
              "16            askreddit  People of Reddit: What Great Lectures or Talks...   \n",
              "17    explainlikeimfive  ELI5: Why does my voice sound different in my ...   \n",
              "18       summonerschool  Jungling at Summoner Level 10: Some data about...   \n",
              "19          techsupport                            Few questions about RAM   \n",
              "20                trees  How hard would it be to get a medicinal licens...   \n",
              "21            askreddit                        What is a good book to you?   \n",
              "22    explainlikeimfive  ELI5: How do we know what the centre of the ea...   \n",
              "23             buildapc  Got a used heatsink on eBay, turned out to be ...   \n",
              "24          hearthstone                      So about this new Murloc Card   \n",
              "25             buildapc                 First timer, building from scratch   \n",
              "26            askreddit  If you could only visit 5 websites for the res...   \n",
              "27            askreddit     What is your mental landscape while you dream?   \n",
              "28            askreddit  Sociopaths of Reddit, assuming you have no emp...   \n",
              "29    explainlikeimfive                        ELI5: How do antennas work?   \n",
              "...                 ...                                                ...   \n",
              "1426          jailbreak  [Help/Question][iOS 8.1.1] Batterylife caused ...   \n",
              "1427    leagueoflegends  What programs and hardware do I need to record...   \n",
              "1428          askreddit  Why do I feel like such a creep when I watch a...   \n",
              "1429             gaming           I love you guys, but this needs to stop.   \n",
              "1430      relationships  I [29 F] have been single for ages and now I d...   \n",
              "1431          starcraft  What chairs do the pros use? We always talk ab...   \n",
              "1432          askreddit  Math is not my subject, so I need help with th...   \n",
              "1433  explainlikeimfive  ELI5: How come so many ancient countries (and ...   \n",
              "1434       pcmasterrace  Recently ascended. Games are crashing. I'm los...   \n",
              "1435      tipofmytongue  [TOMT]Reddit post about a girl who had a creep...   \n",
              "1436          askreddit  Who is a popular musician/band in your country...   \n",
              "1437      tipofmytongue           [TOMT] A 2010 folk/bluegrass music video   \n",
              "1438             gaming  My personal question regarding DLC (Long-winde...   \n",
              "1439    leagueoflegends  Am I the only one who thinks that Fiddle's \"Dr...   \n",
              "1440         reddit.com  She Said Goodbye! I'm getting divorced, and to...   \n",
              "1441          askreddit  Reddit, whats the strangest thing you have see...   \n",
              "1442      relationships  Update - Boyfriend sent nude pics to ex - is t...   \n",
              "1443          askreddit  For Redditors with girlfriends, what was the f...   \n",
              "1444             movies     Watching Fight Club for the first time tonight   \n",
              "1445            atheism  A thought that has occurred to me about Christ...   \n",
              "1446          askreddit   What makes you want to get out of bed? [serious]   \n",
              "1447        techsupport  2nd Monitor isn't having any output at all, help!   \n",
              "1448    leagueoflegends                             A small gp change idea   \n",
              "1449              trees  Midterm election day is tomorrow, and there is...   \n",
              "1450    leagueoflegends  Riot, there's still time to make World's quart...   \n",
              "1451    leagueoflegends            2 Desktop Banner that I made! Thoughts?   \n",
              "1452       pcmasterrace  Got this nice wallpaper through Uplay rewards ...   \n",
              "1453             gaming  What are your views on watching someone play t...   \n",
              "1454      relationships  [20 M] When I get into a relationship, should ...   \n",
              "1455    leagueoflegends  Why is there no mana biscuit? I'd love to have...   \n",
              "\n",
              "                                                 author  \\\n",
              "0     Pouritdownmythroat WhyFrankWhy Pouritdownmythr...   \n",
              "1       ThisGermanGuy shaunrnm Sub_Salac MisterBlack...   \n",
              "2     Leisure321 Zeolance  Buttersgoo23 quzimaa Esca...   \n",
              "3       refugee4chan donquixote6179 ThatStonedAsianGuy    \n",
              "4             catalyzeme uses rvabdn greg2709 spydr101    \n",
              "5     gpacman21 jeansplice Ame-no-nobuko vadergeek A...   \n",
              "6     ellosmello fozzix VarusMidOrAfk I_know_where_y...   \n",
              "7     walkngonawire ResonantCascade snicklepants jui...   \n",
              "8     patrick6h aBoglehead thisismyfirstpost twblalock    \n",
              "9     CrazyChunckMaster420 xmodusterz Frogad Waheyy ...   \n",
              "10    throwaway51711 Lorimor  Lorimor GCanuck throwa...   \n",
              "11                 jcsix CharlieKillsRats jcsix  jcsix    \n",
              "12                               jamvanderloeff           \n",
              "13    ParadoxAnarchy VeteranCommander 28thsundown Ez...   \n",
              "14     vanasbry000 Jinjetsu derindel Skyegg Doughy12...   \n",
              "15    SPL1TT3R gtgoku SPL1TT3R gtgoku SPL1TT3R gtgok...   \n",
              "16    CalonTost Winkn cantthinkofaCO0Lname badmak ka...   \n",
              "17    Tape_measure scienceteacherguy Tape_measure br...   \n",
              "18                     KR_shmungler zedisfied_  Osiato    \n",
              "19    Nedrin BecomeWind ColKrismiss rcmaehl ColKrism...   \n",
              "20    HilbertSystem frisbeefrank HilbertSystem frisb...   \n",
              "21        Hlaufenberg Noomsi jeffreyharharwood  sco154    \n",
              "22          ouissellat ofMindandHeart Teekno riconquer    \n",
              "23    deadgreysn0w froschkonig deadgreysn0w froschko...   \n",
              "24    Jayavarmen OyleSlyck Finckel dz5b605 Jayavarme...   \n",
              "25    arlanTLDR  arlanTLDR omglook joeshlub someawes...   \n",
              "26    French87 LakeAndTheRiver dumandizzy inclinedto...   \n",
              "27     thejumpingasian QwertyPoint TheUnheard blah_b...   \n",
              "28    aminakoydum random_____user elvagabundotonto a...   \n",
              "29    Teillu ToxiClay idetectanerd armena vwlsmssng ...   \n",
              "...                                                 ...   \n",
              "1426          ReddestDream  ReddestDream  ReddestDream    \n",
              "1427           cabbius maximumcrisis Nekture Hatelore     \n",
              "1428  Im_not_creepy_am_I cdude HastaLasagna cdude  I...   \n",
              "1429  Amphabian desertREAPER01 NGRoachClip JakeSteam      \n",
              "1430  singleforever_ Harangue_itang singleforever_ H...   \n",
              "1431  blueboybob brikken AllinIRL SnowJoust bfish510...   \n",
              "1432           tatom TRUBored  gumpy5 T400 gumpy5 T400    \n",
              "1433  Bubminy palcatraz JesusaurusPrime StupidLemonE...   \n",
              "1434  Rosy_Ps_Boyfriend Goofybud16 Rosy_Ps_Boyfriend...   \n",
              "1435  BraddardStark  BraddardStark  BraddardStark Br...   \n",
              "1436  BlueD_ Munchman5000 BlueD_ Munchman5000 Munchm...   \n",
              "1437                  losemybreath yiotta losemybreath    \n",
              "1438  bigsol81 nomofica trolli_mctroll nomofica trol...   \n",
              "1439   Velkyria 420donglord TheBeastOnFire Defiantific    \n",
              "1440  mandysteve mandysteve    peg_leg  Garbagio qqu...   \n",
              "1441  ClavicleRub twelvedayslate Blue_Stuff pridwyn ...   \n",
              "1442  trajicrabbit biteysaur dorthyway SamsIphone tr...   \n",
              "1443   Viperking5  taichi_duck     Eternal_Princess_...   \n",
              "1444  Mac1822 highlysober detroyer  FuckCracker  Ajz...   \n",
              "1445    Tthrond76   aflarge  aflarge  aflarge  aflarge    \n",
              "1446  what_the_rock_cooked Nambot what_the_rock_cook...   \n",
              "1447                   Mars200 Mars200 m1kepro Mars200    \n",
              "1448  Unknof SGTQuackers Protonis Unknof Unknof Pr0d...   \n",
              "1449    EBDoo  EBDoo DDiggler321 MrNewAndImprove EBDoo    \n",
              "1450  tiaow DeepSpacePandaEUW tiaow dudaseifert Deep...   \n",
              "1451  IcybossPlays darkaxefire butttcrackjoe 252978104    \n",
              "1452           kswitch5022 psyEDk   Col_Crunch  Arch_0    \n",
              "1453  GoldLeaderLando aedgar777 Xirii screamingherbe...   \n",
              "1454  funkieturtle DoneAllWrong hopefulthr0waway fun...   \n",
              "1455  Master_Cen DaylightDarkle BadMessiah Master_Ce...   \n",
              "\n",
              "                                                   body len_of_posts  \\\n",
              "0     I think everyone has that one friend who loves...         2518   \n",
              "1     I not 100% sure this is the right place to pos...         3358   \n",
              "2      Facebook  9gag Memes never truly die Dickbutt...          136   \n",
              "3     It's called 'forgetting things'. Dude i dont t...          291   \n",
              "4     How would I do this? I am looking to wait anot...         1449   \n",
              "5     [Respect Sentry.](http://www.reddit.com/r/resp...         3214   \n",
              "6     Is he still a good pick? I played him alot in ...         1025   \n",
              "7     We all know about stoner rock, stoner metal, s...         1847   \n",
              "8     Hi, I am looking to open an IRA while I still ...         1025   \n",
              "9     I am just shocked by how Riot decided to handl...         1334   \n",
              "10    I've purchased a large quantity of furniture, ...         2012   \n",
              "11    I noticed this message appeared when I came ba...         1079   \n",
              "12    [deleted] Yes, it will work with your build.  ...         1379   \n",
              "13     Reading that on the article made me sick.\\nAn...          932   \n",
              "14    I have lost 11 games in a row now. This is aga...         5569   \n",
              "15    ###Build Help/Ready:\\n\\n**Have you read the si...         6202   \n",
              "16    What are some good/interesting talks or lectur...          853   \n",
              "17      When you hear someone elses voice, you are p...         1624   \n",
              "18    It's probably not a good idea to start junglin...        11021   \n",
              "19    posted this on Askcomputerscience, but it does...         5795   \n",
              "20    I'm headed over to San Francisco at the end of...          499   \n",
              "21    [deleted] Fear and Loathing in Las Vegas Tales...          120   \n",
              "22    If we've only drug 12km it seems that we're ju...         1969   \n",
              "23    I ordered a Hyper 212 Cooler Master on eBay, b...         1862   \n",
              "24    http://i.imgur.com/076pqZ9.jpg\\nI actually had...         1949   \n",
              "25    So I've never built a computer before, but I'm...         1797   \n",
              "26    No loopholes like \"hurr duurrrr I'd choose goo...          571   \n",
              "27    Mine tends to be either near water or near mou...         2892   \n",
              "28     Myself.  Sociopaths don't necessarily like th...          817   \n",
              "29     Simply put, electric power is pumped into a l...         2657   \n",
              "...                                                 ...          ...   \n",
              "1426  I updated batterylife and enabled true percent...          947   \n",
              "1427  I have fraps (full version) and my computer is...         1311   \n",
              "1428  White guy, university student in north america...         2845   \n",
              "1429  Surely I'm not the only one who's tired of the...          962   \n",
              "1430  I hope this is the right place to post this. I...         3395   \n",
              "1431   You're pretty much talking about anyone with ...         5471   \n",
              "1432  [Youtube link - Android Unlock](http://www.you...          800   \n",
              "1433   Well, in many ancient civilisations, people w...         2266   \n",
              "1434  I recently built my first PC. I was a casual P...         9780   \n",
              "1435  I remember reading the story a while ago now, ...          649   \n",
              "1436  Mainly aiming this at non British/US Redditors...          471   \n",
              "1437  At least I'm pretty sure it was from last year...         1300   \n",
              "1438  I've been seeing a lot about the recent debacl...         3788   \n",
              "1439   he doesn't need a buff he's already a strong ...          251   \n",
              "1440   My Reddit bros rock! You can hang with us bro...          373   \n",
              "1441   I *always* find it strange when a couple gets...         3795   \n",
              "1442  This is an update to the following [post](http...         3199   \n",
              "1443   \"eyy bby wnt sum fuk?\" And she said?  \"Sucky!...         1209   \n",
              "1444  Can't talk about it.  [groan] I still haven't ...         3882   \n",
              "1445  First and foremost I want to make absolutely c...         2291   \n",
              "1446  What drives you? What makes you wanna push for...         3400   \n",
              "1447  With the most recent AMD update my 2nd monitor...          910   \n",
              "1448  It´s not a big deal but i hate that you stop m...          833   \n",
              "1449  I'm not very involved in politics, but I do ex...         1231   \n",
              "1450  Come on guys.. it would sort out any \"intentio...         1138   \n",
              "1451  http://prntscr.com/60tux4 <--- Rengar, Zed, Ri...          297   \n",
              "1452   I like it. not to nit pick .. okay i'm nit pi...          752   \n",
              "1453  I live with two brothers and we buy our own ga...         1742   \n",
              "1454  My previous question was removed because I did...         1761   \n",
              "1455  Health biscuit is nice and stuff, but since it...          865   \n",
              "\n",
              "     len_of_thread  \n",
              "0               13  \n",
              "1               14  \n",
              "2                9  \n",
              "3                5  \n",
              "4                8  \n",
              "5               11  \n",
              "6               14  \n",
              "7                5  \n",
              "8               15  \n",
              "9               15  \n",
              "10               9  \n",
              "11              11  \n",
              "12               8  \n",
              "13              12  \n",
              "14              11  \n",
              "15               8  \n",
              "16               9  \n",
              "17              17  \n",
              "18              14  \n",
              "19              11  \n",
              "20               5  \n",
              "21               9  \n",
              "22              17  \n",
              "23               8  \n",
              "24              11  \n",
              "25               8  \n",
              "26               9  \n",
              "27               9  \n",
              "28               9  \n",
              "29              17  \n",
              "...            ...  \n",
              "1426             9  \n",
              "1427            15  \n",
              "1428             9  \n",
              "1429             6  \n",
              "1430            13  \n",
              "1431             9  \n",
              "1432             9  \n",
              "1433            17  \n",
              "1434            12  \n",
              "1435            13  \n",
              "1436             9  \n",
              "1437            13  \n",
              "1438             6  \n",
              "1439            15  \n",
              "1440            10  \n",
              "1441             9  \n",
              "1442            13  \n",
              "1443             9  \n",
              "1444             6  \n",
              "1445             7  \n",
              "1446             9  \n",
              "1447            11  \n",
              "1448            15  \n",
              "1449             5  \n",
              "1450            15  \n",
              "1451            15  \n",
              "1452            12  \n",
              "1453             6  \n",
              "1454            13  \n",
              "1455            15  \n",
              "\n",
              "[1456 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "metadata": {
        "id": "IC_I01aXGyvU",
        "colab_type": "code",
        "outputId": "b84fdcd4-451a-4eb0-d9af-f5516505dbfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2691
        }
      },
      "cell_type": "code",
      "source": [
        "#construct train data dataframe\n",
        "import json\n",
        "test_temp = list()\n",
        "\n",
        "with open(subreddit_test) as jsonfile:\n",
        "  for i, line in enumerate(jsonfile):\n",
        "    thread = json.loads(line)\n",
        "    posts = \"\"\n",
        "    authors = \"\"\n",
        "    # Keep information about the source thread where the post comes from.\n",
        "    for post in thread['posts']:\n",
        "      posts+=post.get('body', \"\") + \" \"\n",
        "      authors+=post.get('author', \"\") + \" \"\n",
        "      len_of_posts = str(len(posts))\n",
        "      len_of_thread = str(len(thread['subreddit']))\n",
        "    \n",
        "    test_temp.append((thread['subreddit'], \n",
        "                    thread['title'], \n",
        "                    authors,\n",
        "                    posts,\n",
        "                    len_of_posts,\n",
        "                    len_of_thread))\n",
        "print(len(test_temp))\n",
        "\n",
        "labels = ['subreddit', 'title',  'author', 'body', 'len_of_posts', 'len_of_thread']\n",
        "test_data = pd.DataFrame(test_temp, columns=labels)\n",
        "test_data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "365\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>subreddit</th>\n",
              "      <th>title</th>\n",
              "      <th>author</th>\n",
              "      <th>body</th>\n",
              "      <th>len_of_posts</th>\n",
              "      <th>len_of_thread</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>starcraft</td>\n",
              "      <td>Just a reminder on how much SC2 has evolved th...</td>\n",
              "      <td>MeisterKarl smsy vehemus NruJaC</td>\n",
              "      <td>Was watching a VOD from last years DreamHack W...</td>\n",
              "      <td>764</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>whowouldwin</td>\n",
              "      <td>Your Favorite Hero Now Has A Healing Factor As...</td>\n",
              "      <td>TheAnti-Monitor iamcatch22 blames_irrationall...</td>\n",
              "      <td>Basically what the title says. Danny Phantom i...</td>\n",
              "      <td>2846</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>askreddit</td>\n",
              "      <td>If you could live anywhere in the world, where...</td>\n",
              "      <td>Daft-Punk  AutoModerator Memithezombiekiller I...</td>\n",
              "      <td>On a beach someplace warm and far away from a...</td>\n",
              "      <td>1609</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>askreddit</td>\n",
              "      <td>Do you ever get use to exercise?</td>\n",
              "      <td>theawe_some DrWallyHayes IGrammarGood theawe_s...</td>\n",
              "      <td>I start running this year. I do it frequently ...</td>\n",
              "      <td>1884</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>tipofmytongue</td>\n",
              "      <td>[TOMT] [book] A scary french book for kids</td>\n",
              "      <td>grapesandmilk</td>\n",
              "      <td>[deleted] Could you possibly provide us a clue...</td>\n",
              "      <td>481</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>trees</td>\n",
              "      <td>I Don't Understand Why People Hate the War On ...</td>\n",
              "      <td>duel_dude Knotfloyd KuronekoKawaii vishnukg ra...</td>\n",
              "      <td>I think they make pretty good music: https://w...</td>\n",
              "      <td>847</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>askreddit</td>\n",
              "      <td>What do you miss the most from childhood?</td>\n",
              "      <td>woodstock01 pernoe Coutinho_Fan logallama beck...</td>\n",
              "      <td>Being carefree and having someone else worry ...</td>\n",
              "      <td>700</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>tipofmytongue</td>\n",
              "      <td>[TOMT] [Song] I heard this on the radio</td>\n",
              "      <td>gunthug  gunthug cpaige88</td>\n",
              "      <td>I recorded my voice http://www.speakpipe.com/v...</td>\n",
              "      <td>283</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>explainlikeimfive</td>\n",
              "      <td>ELI5: Why is pee warm?</td>\n",
              "      <td>throwawaymayblol pnotar throwawaymayblol paulk...</td>\n",
              "      <td>It should come out at the same temperature as...</td>\n",
              "      <td>311</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>hearthstone</td>\n",
              "      <td>Undocumented change: Stall prevention</td>\n",
              "      <td>Orconem  Hjelpen wpScraps</td>\n",
              "      <td>So I meant to post this yesterday, but in one ...</td>\n",
              "      <td>461</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>buildapc</td>\n",
              "      <td>[£] What is the cheapest z97 motherboard I can...</td>\n",
              "      <td>jakedebest  jakedebest  jakedebest jakedebest</td>\n",
              "      <td>Also to be able to use it in the future with I...</td>\n",
              "      <td>1050</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>askreddit</td>\n",
              "      <td>Reddit, what makes a good teacher?</td>\n",
              "      <td>marcusesses  FreeTheAnimals otterplay  daderad...</td>\n",
              "      <td>Loving deeply his or her material.  Enthusias...</td>\n",
              "      <td>1705</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>atheism</td>\n",
              "      <td>Fear No Evil [wallpaper]</td>\n",
              "      <td>Character_T Viin etbob623 shazang zzorga Pseud...</td>\n",
              "      <td>Inspirational. I must not fear.\\n\\nFear is th...</td>\n",
              "      <td>3366</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>trees</td>\n",
              "      <td>My first time</td>\n",
              "      <td>baconeater613 MrPerez baconeater613 Clund12</td>\n",
              "      <td>So i smoked for the first time tonight and it ...</td>\n",
              "      <td>281</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>electronic_cigarette</td>\n",
              "      <td>Whit E. Octopus Vapors Announces a Raise in Pr...</td>\n",
              "      <td>wittyoctopus genthecamel wittyoctopus thelastl...</td>\n",
              "      <td>Dear fellow vapers, regretfully I must announc...</td>\n",
              "      <td>6827</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>personalfinance</td>\n",
              "      <td>I've been contributing to my HSA while still u...</td>\n",
              "      <td>FinanceGuy6 AutoModerator JJJJust FinanceGuy6 ...</td>\n",
              "      <td>I just started my job about a year ago and the...</td>\n",
              "      <td>3896</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>buildapc</td>\n",
              "      <td>Router kicking people off internet.</td>\n",
              "      <td>twoscoop BuffaloLP twoscoop  JaxXx_oL20  twosc...</td>\n",
              "      <td>Is it possible that my dual band network card ...</td>\n",
              "      <td>874</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>reddit.com</td>\n",
              "      <td>Ask Reddit: i am somewhat overweight, i want t...</td>\n",
              "      <td>playplayplay Dagon AngledLuffa</td>\n",
              "      <td>\"take the cake\"?\\n\\n\"take too long\"?\\n\\nI hop...</td>\n",
              "      <td>1024</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>buildapc</td>\n",
              "      <td>[Troubleshooting] Powered up my new build, and...</td>\n",
              "      <td>japaul32  japaul32 azn_dude1 bowzer1919</td>\n",
              "      <td>I've never experienced this before, but I got ...</td>\n",
              "      <td>1186</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>leagueoflegends</td>\n",
              "      <td>Currently leveling up a smurf account!</td>\n",
              "      <td>worstneilna Douglasgreyv worstneilna krihan wo...</td>\n",
              "      <td>I'm not leveling up my smurf for the intention...</td>\n",
              "      <td>687</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>leagueoflegends</td>\n",
              "      <td>Match History Not Updating?</td>\n",
              "      <td>RedEyedSoul Crashco17 RedEyedSoul IRQ-Horu</td>\n",
              "      <td>Is anyone else suffering this issue? I played ...</td>\n",
              "      <td>705</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>askreddit</td>\n",
              "      <td>What movie title best describes your most rece...</td>\n",
              "      <td>some_guy_on_Earth Narwheagle MikeTheMastodon18...</td>\n",
              "      <td>Alone in the Dark Contagion Home Alone  Borde...</td>\n",
              "      <td>469</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>buildapc</td>\n",
              "      <td>700ish build help.</td>\n",
              "      <td>mguerra809 khodbros Nasjan77 mguerra809 Nasjan...</td>\n",
              "      <td>###Build Help/Ready:\\n\\n**Have you read the si...</td>\n",
              "      <td>5211</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>askreddit</td>\n",
              "      <td>poor people of reddit..whats the best way you ...</td>\n",
              "      <td>resa5xd Swiftdaggers canvassy Macular_Patdown ...</td>\n",
              "      <td>Don't eat fast food. It's so much cheaper and...</td>\n",
              "      <td>3528</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>relationships</td>\n",
              "      <td>Do I (26F) tell my ex (M30) from 4 years that ...</td>\n",
              "      <td>throwingthisawayya Seshaia throwingthisawayya ...</td>\n",
              "      <td>As you can tell from the title, this situation...</td>\n",
              "      <td>5368</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>askreddit</td>\n",
              "      <td>What was your favorite and least favorite thin...</td>\n",
              "      <td>LonglivetheNorth ash13ys LonglivetheNorth ash1...</td>\n",
              "      <td>When Katniss ended up with Peta.\\n\\nSorry, I'...</td>\n",
              "      <td>734</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>askreddit</td>\n",
              "      <td>You just created a new world changing product....</td>\n",
              "      <td>DGD_poorRobot Forestman88 RealShame DGD_poorRo...</td>\n",
              "      <td>May cause anal bleeding, nausea, night sweats...</td>\n",
              "      <td>854</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>atheism</td>\n",
              "      <td>Death?</td>\n",
              "      <td>TheDownvoteFreak DEATH__HIMSELF ajose001 thatg...</td>\n",
              "      <td>As an agnostic, I am wondering what do you bel...</td>\n",
              "      <td>2307</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>trees</td>\n",
              "      <td>MD/DC/VA residents... any clues as to when DC ...</td>\n",
              "      <td>eh_fuck_it1980 AnalogHumanSentient eh_fuck_it1...</td>\n",
              "      <td>It's cool it's legal and all since February. G...</td>\n",
              "      <td>732</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>techsupport</td>\n",
              "      <td>broken external hard drive, how can i get the ...</td>\n",
              "      <td>xxoczukxx i010011010 xxoczukxx i010011010 xxoc...</td>\n",
              "      <td>So my sister broke her external hard drive som...</td>\n",
              "      <td>3012</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>335</th>\n",
              "      <td>askreddit</td>\n",
              "      <td>If you had a count down timer for anyone in th...</td>\n",
              "      <td>Quhzey 8livesdown Quhzey 8livesdown Updoc_RS3 ...</td>\n",
              "      <td>The timer would beep. What if it was one that...</td>\n",
              "      <td>314</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>336</th>\n",
              "      <td>askreddit</td>\n",
              "      <td>What do you want out of life?</td>\n",
              "      <td>wearedoctors landsharkxx IUsePretzelLogic Obes...</td>\n",
              "      <td>What motivates you, reddit? life motivates me ...</td>\n",
              "      <td>224</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>337</th>\n",
              "      <td>pcmasterrace</td>\n",
              "      <td>Hard Boot From USB...now New Key Won't Work?</td>\n",
              "      <td>RiffyDivine2  faissaloo</td>\n",
              "      <td>Help me PC Master Race...You're My Only Hope (...</td>\n",
              "      <td>1641</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>338</th>\n",
              "      <td>askreddit</td>\n",
              "      <td>Self-defense experts and/or people who have be...</td>\n",
              "      <td>ihavetopee32 super_leet_hacker ajos2 ihavetope...</td>\n",
              "      <td>Stories are ok and what they have taught you. ...</td>\n",
              "      <td>722</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>339</th>\n",
              "      <td>askreddit</td>\n",
              "      <td>What circlejerk makes you SO ANGRY when you se...</td>\n",
              "      <td>MWozz    remog</td>\n",
              "      <td>For me it has to be the fucking \"The punchline...</td>\n",
              "      <td>761</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>340</th>\n",
              "      <td>trees</td>\n",
              "      <td>Anyone else fucking hate grinding up weed?</td>\n",
              "      <td>HaydenHank y2kcasualty HaydenHank Murkee420 fr...</td>\n",
              "      <td>Its such a fucking chore, its the only thing I...</td>\n",
              "      <td>728</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>341</th>\n",
              "      <td>hearthstone</td>\n",
              "      <td>I can't feel my legs!</td>\n",
              "      <td>Zalindor Ninjaspar10 Zalindor Ninjaspar10 Knei...</td>\n",
              "      <td>Hearthstone on iPad has kept me in this bathro...</td>\n",
              "      <td>454</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>342</th>\n",
              "      <td>trees</td>\n",
              "      <td>What kind of High are you?</td>\n",
              "      <td>Rivdog98 AverageWhiteJesus Rivdog98 AverageWhi...</td>\n",
              "      <td>What kind of high are you? Are you loud, quiet...</td>\n",
              "      <td>3624</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>343</th>\n",
              "      <td>askreddit</td>\n",
              "      <td>What is the appeal of the original Tron movie?</td>\n",
              "      <td>deathmouse  subtonix thisusernametakentoo Ram...</td>\n",
              "      <td>I understand that it was quite advanced for it...</td>\n",
              "      <td>2436</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>344</th>\n",
              "      <td>electronic_cigarette</td>\n",
              "      <td>Just bought ejuice with bitcoin...</td>\n",
              "      <td>night_chemist   dogetipbot 3tern1ty dogetipbot...</td>\n",
              "      <td>The future is now. \\n\\nAnyways this post is pr...</td>\n",
              "      <td>5022</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>345</th>\n",
              "      <td>hearthstone</td>\n",
              "      <td>This game is amazing</td>\n",
              "      <td>Montgolian gunch Montgolian SH4D0W0733 Ezekiel...</td>\n",
              "      <td>I noticed the game always at the top of the tw...</td>\n",
              "      <td>3658</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>346</th>\n",
              "      <td>buildapc</td>\n",
              "      <td>Sapphire or XFX - Radeon HD 6870</td>\n",
              "      <td>totalBS codywarmbo ACDCGAMER</td>\n",
              "      <td>Help Please\\n\\nhttp://www.newegg.com/Product/P...</td>\n",
              "      <td>649</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>347</th>\n",
              "      <td>personalfinance</td>\n",
              "      <td>Ideas to rebuild credit?</td>\n",
              "      <td>Caito9  Caito9  dequeued</td>\n",
              "      <td>Hey guys, as the title states, I'm looking to ...</td>\n",
              "      <td>2150</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>348</th>\n",
              "      <td>pcmasterrace</td>\n",
              "      <td>The ASUS MG279Q (FreeSync 1440p monitor) has b...</td>\n",
              "      <td>Teslatic warfie27 jattyrr Teslatic</td>\n",
              "      <td>The ASUS MG279Q (FreeSync 1440p monitor) has b...</td>\n",
              "      <td>920</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>349</th>\n",
              "      <td>askreddit</td>\n",
              "      <td>What is the strangest or most amusing reason y...</td>\n",
              "      <td>yellowtuxedo VanessaSoIll Quizznor VanessaSoIl...</td>\n",
              "      <td>\"Abandoning my job\"\\n\\nI had paid vacation ap...</td>\n",
              "      <td>4766</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>350</th>\n",
              "      <td>tipofmytongue</td>\n",
              "      <td>[TOMT] Help me find this fun catchy song</td>\n",
              "      <td>Ande_Ka_Funda ScarletTanager ButterNuttz charl...</td>\n",
              "      <td>I heard it at my gym but unfortunately don't r...</td>\n",
              "      <td>760</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>351</th>\n",
              "      <td>leagueoflegends</td>\n",
              "      <td>Need help with mouse Binding</td>\n",
              "      <td>scopic72 shozaku scopic72 shozaku</td>\n",
              "      <td>Hi i bought a RedDragon Perdition Mouse but i ...</td>\n",
              "      <td>1094</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>352</th>\n",
              "      <td>atheism</td>\n",
              "      <td>Atheists thoughts on the State.</td>\n",
              "      <td>philosorapper Beetle559 chenobble zoidberg82 B...</td>\n",
              "      <td>A Christian can accept a world where 9,999 god...</td>\n",
              "      <td>18338</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>353</th>\n",
              "      <td>askreddit</td>\n",
              "      <td>Anyone had experience with doubting reality en...</td>\n",
              "      <td>tooexistential73 BigCrappola tooexistential73 ...</td>\n",
              "      <td>So I think I'm going through an existential cr...</td>\n",
              "      <td>4321</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>354</th>\n",
              "      <td>personalfinance</td>\n",
              "      <td>Couple on Minimum Wage Moving Out (UK)</td>\n",
              "      <td>jimfixeditforme GeordanUK jimfixeditforme sta...</td>\n",
              "      <td>[deleted] to be honest while you're living at ...</td>\n",
              "      <td>8299</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>355</th>\n",
              "      <td>tipofmytongue</td>\n",
              "      <td>TOMT: This gif of what appears to be an SU-27 ...</td>\n",
              "      <td>Spam4119 mantarayj Spam4119 Spam4119 mantarayj</td>\n",
              "      <td>http://i.minus.com/i8o3yB3Lm7A1f.gif\\n\\nEDIT: ...</td>\n",
              "      <td>659</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>356</th>\n",
              "      <td>trees</td>\n",
              "      <td>What does r/trees think about my piece?</td>\n",
              "      <td>HerbyMcBluntsmokerr adeadhead Rjsexton458 adea...</td>\n",
              "      <td>I'm not the biggest fan of the colors. So on ...</td>\n",
              "      <td>1819</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>357</th>\n",
              "      <td>explainlikeimfive</td>\n",
              "      <td>ELI5: Why is it that I have trim my dogs nails...</td>\n",
              "      <td>JohnnySorrow ___dreadnought LordDongler</td>\n",
              "      <td>Wolves, wild dogs, and many other feral anima...</td>\n",
              "      <td>260</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>358</th>\n",
              "      <td>starcraft</td>\n",
              "      <td>Byun vs SuperNova - Mass TvT incoming with eve...</td>\n",
              "      <td>mequals1m1w mequals1m1w PrakPrak lO_O mequals1...</td>\n",
              "      <td>Edit: Unfortunately on GOM, match 6 &amp; 7 are th...</td>\n",
              "      <td>564</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>359</th>\n",
              "      <td>buildapc</td>\n",
              "      <td>[Build Help] What graphics card should I top t...</td>\n",
              "      <td>TuGator harleysmoke step1makeart TuGator</td>\n",
              "      <td>So I put this build together. \\n\\nPCPartPicker...</td>\n",
              "      <td>2490</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>360</th>\n",
              "      <td>reddit.com</td>\n",
              "      <td>Reddit Feature Suggestion: Cross posting suppo...</td>\n",
              "      <td>magelisk magelisk bageloid squig JohnoTheFooli...</td>\n",
              "      <td>Perhaps I'm blind but I've never seen this rec...</td>\n",
              "      <td>6678</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>361</th>\n",
              "      <td>atheism</td>\n",
              "      <td>Interesting conversation with my wife's grandp...</td>\n",
              "      <td>jim85541 UnclePutin</td>\n",
              "      <td>congrats, it doesn't always go that way. Dam s...</td>\n",
              "      <td>684</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>362</th>\n",
              "      <td>pcmasterrace</td>\n",
              "      <td>Brethren, how many games do you have in your g...</td>\n",
              "      <td>Taanz FrostyTheHippo Lizzardspawn ImDeadInside...</td>\n",
              "      <td>39 on steam and roughly 20 on origin. \\n\\nComp...</td>\n",
              "      <td>904</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>363</th>\n",
              "      <td>askreddit</td>\n",
              "      <td>[Serious] What are your thoughts on pirating?</td>\n",
              "      <td>PotatoStalker AsshatVik mojave_moon  PotatoSta...</td>\n",
              "      <td>Helps the industry rather than destroying it....</td>\n",
              "      <td>4399</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>364</th>\n",
              "      <td>askreddit</td>\n",
              "      <td>Your favorite subreddit is a superhero. What a...</td>\n",
              "      <td>AlexanderDivine Evolve_or_Bye AlexanderDivine ...</td>\n",
              "      <td>Are you high? Not for another 35 minutes. He ...</td>\n",
              "      <td>369</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>365 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                subreddit                                              title  \\\n",
              "0               starcraft  Just a reminder on how much SC2 has evolved th...   \n",
              "1             whowouldwin  Your Favorite Hero Now Has A Healing Factor As...   \n",
              "2               askreddit  If you could live anywhere in the world, where...   \n",
              "3               askreddit                   Do you ever get use to exercise?   \n",
              "4           tipofmytongue         [TOMT] [book] A scary french book for kids   \n",
              "5                   trees  I Don't Understand Why People Hate the War On ...   \n",
              "6               askreddit          What do you miss the most from childhood?   \n",
              "7           tipofmytongue            [TOMT] [Song] I heard this on the radio   \n",
              "8       explainlikeimfive                             ELI5: Why is pee warm?   \n",
              "9             hearthstone              Undocumented change: Stall prevention   \n",
              "10               buildapc  [£] What is the cheapest z97 motherboard I can...   \n",
              "11              askreddit                 Reddit, what makes a good teacher?   \n",
              "12                atheism                           Fear No Evil [wallpaper]   \n",
              "13                  trees                                      My first time   \n",
              "14   electronic_cigarette  Whit E. Octopus Vapors Announces a Raise in Pr...   \n",
              "15        personalfinance  I've been contributing to my HSA while still u...   \n",
              "16               buildapc                Router kicking people off internet.   \n",
              "17             reddit.com  Ask Reddit: i am somewhat overweight, i want t...   \n",
              "18               buildapc  [Troubleshooting] Powered up my new build, and...   \n",
              "19        leagueoflegends             Currently leveling up a smurf account!   \n",
              "20        leagueoflegends                        Match History Not Updating?   \n",
              "21              askreddit  What movie title best describes your most rece...   \n",
              "22               buildapc                                 700ish build help.   \n",
              "23              askreddit  poor people of reddit..whats the best way you ...   \n",
              "24          relationships  Do I (26F) tell my ex (M30) from 4 years that ...   \n",
              "25              askreddit  What was your favorite and least favorite thin...   \n",
              "26              askreddit  You just created a new world changing product....   \n",
              "27                atheism                                             Death?   \n",
              "28                  trees  MD/DC/VA residents... any clues as to when DC ...   \n",
              "29            techsupport  broken external hard drive, how can i get the ...   \n",
              "..                    ...                                                ...   \n",
              "335             askreddit  If you had a count down timer for anyone in th...   \n",
              "336             askreddit                      What do you want out of life?   \n",
              "337          pcmasterrace       Hard Boot From USB...now New Key Won't Work?   \n",
              "338             askreddit  Self-defense experts and/or people who have be...   \n",
              "339             askreddit  What circlejerk makes you SO ANGRY when you se...   \n",
              "340                 trees         Anyone else fucking hate grinding up weed?   \n",
              "341           hearthstone                              I can't feel my legs!   \n",
              "342                 trees                         What kind of High are you?   \n",
              "343             askreddit     What is the appeal of the original Tron movie?   \n",
              "344  electronic_cigarette                 Just bought ejuice with bitcoin...   \n",
              "345           hearthstone                               This game is amazing   \n",
              "346              buildapc                   Sapphire or XFX - Radeon HD 6870   \n",
              "347       personalfinance                           Ideas to rebuild credit?   \n",
              "348          pcmasterrace  The ASUS MG279Q (FreeSync 1440p monitor) has b...   \n",
              "349             askreddit  What is the strangest or most amusing reason y...   \n",
              "350         tipofmytongue           [TOMT] Help me find this fun catchy song   \n",
              "351       leagueoflegends                       Need help with mouse Binding   \n",
              "352               atheism                    Atheists thoughts on the State.   \n",
              "353             askreddit  Anyone had experience with doubting reality en...   \n",
              "354       personalfinance             Couple on Minimum Wage Moving Out (UK)   \n",
              "355         tipofmytongue  TOMT: This gif of what appears to be an SU-27 ...   \n",
              "356                 trees            What does r/trees think about my piece?   \n",
              "357     explainlikeimfive  ELI5: Why is it that I have trim my dogs nails...   \n",
              "358             starcraft  Byun vs SuperNova - Mass TvT incoming with eve...   \n",
              "359              buildapc  [Build Help] What graphics card should I top t...   \n",
              "360            reddit.com  Reddit Feature Suggestion: Cross posting suppo...   \n",
              "361               atheism  Interesting conversation with my wife's grandp...   \n",
              "362          pcmasterrace  Brethren, how many games do you have in your g...   \n",
              "363             askreddit      [Serious] What are your thoughts on pirating?   \n",
              "364             askreddit  Your favorite subreddit is a superhero. What a...   \n",
              "\n",
              "                                                author  \\\n",
              "0                     MeisterKarl smsy vehemus NruJaC    \n",
              "1     TheAnti-Monitor iamcatch22 blames_irrationall...   \n",
              "2    Daft-Punk  AutoModerator Memithezombiekiller I...   \n",
              "3    theawe_some DrWallyHayes IGrammarGood theawe_s...   \n",
              "4                                      grapesandmilk     \n",
              "5    duel_dude Knotfloyd KuronekoKawaii vishnukg ra...   \n",
              "6    woodstock01 pernoe Coutinho_Fan logallama beck...   \n",
              "7                          gunthug  gunthug cpaige88     \n",
              "8    throwawaymayblol pnotar throwawaymayblol paulk...   \n",
              "9                           Orconem  Hjelpen wpScraps    \n",
              "10     jakedebest  jakedebest  jakedebest jakedebest     \n",
              "11   marcusesses  FreeTheAnimals otterplay  daderad...   \n",
              "12   Character_T Viin etbob623 shazang zzorga Pseud...   \n",
              "13        baconeater613 MrPerez baconeater613 Clund12    \n",
              "14   wittyoctopus genthecamel wittyoctopus thelastl...   \n",
              "15   FinanceGuy6 AutoModerator JJJJust FinanceGuy6 ...   \n",
              "16   twoscoop BuffaloLP twoscoop  JaxXx_oL20  twosc...   \n",
              "17                     playplayplay Dagon AngledLuffa    \n",
              "18            japaul32  japaul32 azn_dude1 bowzer1919    \n",
              "19   worstneilna Douglasgreyv worstneilna krihan wo...   \n",
              "20         RedEyedSoul Crashco17 RedEyedSoul IRQ-Horu    \n",
              "21   some_guy_on_Earth Narwheagle MikeTheMastodon18...   \n",
              "22   mguerra809 khodbros Nasjan77 mguerra809 Nasjan...   \n",
              "23   resa5xd Swiftdaggers canvassy Macular_Patdown ...   \n",
              "24   throwingthisawayya Seshaia throwingthisawayya ...   \n",
              "25   LonglivetheNorth ash13ys LonglivetheNorth ash1...   \n",
              "26   DGD_poorRobot Forestman88 RealShame DGD_poorRo...   \n",
              "27   TheDownvoteFreak DEATH__HIMSELF ajose001 thatg...   \n",
              "28   eh_fuck_it1980 AnalogHumanSentient eh_fuck_it1...   \n",
              "29   xxoczukxx i010011010 xxoczukxx i010011010 xxoc...   \n",
              "..                                                 ...   \n",
              "335  Quhzey 8livesdown Quhzey 8livesdown Updoc_RS3 ...   \n",
              "336  wearedoctors landsharkxx IUsePretzelLogic Obes...   \n",
              "337                           RiffyDivine2  faissaloo    \n",
              "338  ihavetopee32 super_leet_hacker ajos2 ihavetope...   \n",
              "339                                    MWozz    remog    \n",
              "340  HaydenHank y2kcasualty HaydenHank Murkee420 fr...   \n",
              "341  Zalindor Ninjaspar10 Zalindor Ninjaspar10 Knei...   \n",
              "342  Rivdog98 AverageWhiteJesus Rivdog98 AverageWhi...   \n",
              "343   deathmouse  subtonix thisusernametakentoo Ram...   \n",
              "344  night_chemist   dogetipbot 3tern1ty dogetipbot...   \n",
              "345  Montgolian gunch Montgolian SH4D0W0733 Ezekiel...   \n",
              "346                      totalBS codywarmbo ACDCGAMER    \n",
              "347                          Caito9  Caito9  dequeued    \n",
              "348                Teslatic warfie27 jattyrr Teslatic    \n",
              "349  yellowtuxedo VanessaSoIll Quizznor VanessaSoIl...   \n",
              "350  Ande_Ka_Funda ScarletTanager ButterNuttz charl...   \n",
              "351                 scopic72 shozaku scopic72 shozaku    \n",
              "352  philosorapper Beetle559 chenobble zoidberg82 B...   \n",
              "353  tooexistential73 BigCrappola tooexistential73 ...   \n",
              "354   jimfixeditforme GeordanUK jimfixeditforme sta...   \n",
              "355    Spam4119 mantarayj Spam4119 Spam4119 mantarayj    \n",
              "356  HerbyMcBluntsmokerr adeadhead Rjsexton458 adea...   \n",
              "357           JohnnySorrow ___dreadnought LordDongler    \n",
              "358  mequals1m1w mequals1m1w PrakPrak lO_O mequals1...   \n",
              "359          TuGator harleysmoke step1makeart TuGator    \n",
              "360  magelisk magelisk bageloid squig JohnoTheFooli...   \n",
              "361                               jim85541 UnclePutin    \n",
              "362  Taanz FrostyTheHippo Lizzardspawn ImDeadInside...   \n",
              "363  PotatoStalker AsshatVik mojave_moon  PotatoSta...   \n",
              "364  AlexanderDivine Evolve_or_Bye AlexanderDivine ...   \n",
              "\n",
              "                                                  body len_of_posts  \\\n",
              "0    Was watching a VOD from last years DreamHack W...          764   \n",
              "1    Basically what the title says. Danny Phantom i...         2846   \n",
              "2     On a beach someplace warm and far away from a...         1609   \n",
              "3    I start running this year. I do it frequently ...         1884   \n",
              "4    [deleted] Could you possibly provide us a clue...          481   \n",
              "5    I think they make pretty good music: https://w...          847   \n",
              "6     Being carefree and having someone else worry ...          700   \n",
              "7    I recorded my voice http://www.speakpipe.com/v...          283   \n",
              "8     It should come out at the same temperature as...          311   \n",
              "9    So I meant to post this yesterday, but in one ...          461   \n",
              "10   Also to be able to use it in the future with I...         1050   \n",
              "11    Loving deeply his or her material.  Enthusias...         1705   \n",
              "12    Inspirational. I must not fear.\\n\\nFear is th...         3366   \n",
              "13   So i smoked for the first time tonight and it ...          281   \n",
              "14   Dear fellow vapers, regretfully I must announc...         6827   \n",
              "15   I just started my job about a year ago and the...         3896   \n",
              "16   Is it possible that my dual band network card ...          874   \n",
              "17    \"take the cake\"?\\n\\n\"take too long\"?\\n\\nI hop...         1024   \n",
              "18   I've never experienced this before, but I got ...         1186   \n",
              "19   I'm not leveling up my smurf for the intention...          687   \n",
              "20   Is anyone else suffering this issue? I played ...          705   \n",
              "21    Alone in the Dark Contagion Home Alone  Borde...          469   \n",
              "22   ###Build Help/Ready:\\n\\n**Have you read the si...         5211   \n",
              "23    Don't eat fast food. It's so much cheaper and...         3528   \n",
              "24   As you can tell from the title, this situation...         5368   \n",
              "25    When Katniss ended up with Peta.\\n\\nSorry, I'...          734   \n",
              "26    May cause anal bleeding, nausea, night sweats...          854   \n",
              "27   As an agnostic, I am wondering what do you bel...         2307   \n",
              "28   It's cool it's legal and all since February. G...          732   \n",
              "29   So my sister broke her external hard drive som...         3012   \n",
              "..                                                 ...          ...   \n",
              "335   The timer would beep. What if it was one that...          314   \n",
              "336  What motivates you, reddit? life motivates me ...          224   \n",
              "337  Help me PC Master Race...You're My Only Hope (...         1641   \n",
              "338  Stories are ok and what they have taught you. ...          722   \n",
              "339  For me it has to be the fucking \"The punchline...          761   \n",
              "340  Its such a fucking chore, its the only thing I...          728   \n",
              "341  Hearthstone on iPad has kept me in this bathro...          454   \n",
              "342  What kind of high are you? Are you loud, quiet...         3624   \n",
              "343  I understand that it was quite advanced for it...         2436   \n",
              "344  The future is now. \\n\\nAnyways this post is pr...         5022   \n",
              "345  I noticed the game always at the top of the tw...         3658   \n",
              "346  Help Please\\n\\nhttp://www.newegg.com/Product/P...          649   \n",
              "347  Hey guys, as the title states, I'm looking to ...         2150   \n",
              "348  The ASUS MG279Q (FreeSync 1440p monitor) has b...          920   \n",
              "349   \"Abandoning my job\"\\n\\nI had paid vacation ap...         4766   \n",
              "350  I heard it at my gym but unfortunately don't r...          760   \n",
              "351  Hi i bought a RedDragon Perdition Mouse but i ...         1094   \n",
              "352  A Christian can accept a world where 9,999 god...        18338   \n",
              "353  So I think I'm going through an existential cr...         4321   \n",
              "354  [deleted] to be honest while you're living at ...         8299   \n",
              "355  http://i.minus.com/i8o3yB3Lm7A1f.gif\\n\\nEDIT: ...          659   \n",
              "356   I'm not the biggest fan of the colors. So on ...         1819   \n",
              "357   Wolves, wild dogs, and many other feral anima...          260   \n",
              "358  Edit: Unfortunately on GOM, match 6 & 7 are th...          564   \n",
              "359  So I put this build together. \\n\\nPCPartPicker...         2490   \n",
              "360  Perhaps I'm blind but I've never seen this rec...         6678   \n",
              "361  congrats, it doesn't always go that way. Dam s...          684   \n",
              "362  39 on steam and roughly 20 on origin. \\n\\nComp...          904   \n",
              "363   Helps the industry rather than destroying it....         4399   \n",
              "364   Are you high? Not for another 35 minutes. He ...          369   \n",
              "\n",
              "    len_of_thread  \n",
              "0               9  \n",
              "1              11  \n",
              "2               9  \n",
              "3               9  \n",
              "4              13  \n",
              "5               5  \n",
              "6               9  \n",
              "7              13  \n",
              "8              17  \n",
              "9              11  \n",
              "10              8  \n",
              "11              9  \n",
              "12              7  \n",
              "13              5  \n",
              "14             20  \n",
              "15             15  \n",
              "16              8  \n",
              "17             10  \n",
              "18              8  \n",
              "19             15  \n",
              "20             15  \n",
              "21              9  \n",
              "22              8  \n",
              "23              9  \n",
              "24             13  \n",
              "25              9  \n",
              "26              9  \n",
              "27              7  \n",
              "28              5  \n",
              "29             11  \n",
              "..            ...  \n",
              "335             9  \n",
              "336             9  \n",
              "337            12  \n",
              "338             9  \n",
              "339             9  \n",
              "340             5  \n",
              "341            11  \n",
              "342             5  \n",
              "343             9  \n",
              "344            20  \n",
              "345            11  \n",
              "346             8  \n",
              "347            15  \n",
              "348            12  \n",
              "349             9  \n",
              "350            13  \n",
              "351            15  \n",
              "352             7  \n",
              "353             9  \n",
              "354            15  \n",
              "355            13  \n",
              "356             5  \n",
              "357            17  \n",
              "358             9  \n",
              "359             8  \n",
              "360            10  \n",
              "361             7  \n",
              "362            12  \n",
              "363             9  \n",
              "364             9  \n",
              "\n",
              "[365 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "metadata": {
        "id": "9a3QjrE-MrGo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#construct labels\n",
        "train_labels = train_data['subreddit']\n",
        "test_labels = test_data['subreddit']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l8QCyIl2i__H",
        "colab_type": "code",
        "outputId": "60b3f97c-c10d-4a2d-c85f-74791c3bb94e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the medium english model. \n",
        "!python -m spacy download en_core_web_md\n",
        "\n",
        "nlp = spacy.load('en_core_web_md', disable=['ner'])\n",
        "nlp.remove_pipe('tagger')\n",
        "nlp.remove_pipe('parser')\n",
        "\n",
        "# Download a stopword list\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_md==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.0.0/en_core_web_md-2.0.0.tar.gz#egg=en_core_web_md==2.0.0 in /usr/local/lib/python3.6/dist-packages (2.0.0)\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/en_core_web_md -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/en_core_web_md\n",
            "\n",
            "    You can now load the model via spacy.load('en_core_web_md')\n",
            "\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "metadata": {
        "id": "uuej9Cllff0W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Tokenize\n",
        "def spacy_tokenize(string):\n",
        "  tokens = list()\n",
        "  doc = nlp(string)\n",
        "  for token in doc:\n",
        "    tokens.append(token)\n",
        "  return tokens\n",
        "\n",
        "#Normalize\n",
        "def normalize(tokens):\n",
        "  normalized_tokens = list()\n",
        "  for token in tokens:\n",
        "    normalized = token.text.lower().strip()\n",
        "    if ((token.is_alpha or token.is_digit)):\n",
        "      normalized_tokens.append(normalized)\n",
        "  return normalized_tokens\n",
        "  return normalized_tokens\n",
        "\n",
        "#Tokenize and normalize\n",
        "def tokenize_normalize(string):\n",
        "  return normalize(spacy_tokenize(string))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1hjTY_q2ktLl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Evaluation: accuracy, macro-averaged precision, recall, and F1 measures\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import fbeta_score\n",
        "\n",
        "def evaluation_summary(description, predictions, true_labels):\n",
        "  print(\"Evaluation for: \" + description)\n",
        "  precision = precision_score(predictions, true_labels,average='macro')\n",
        "  recall = recall_score(predictions, true_labels, average='macro')\n",
        "  accuracy = accuracy_score(predictions, true_labels)\n",
        "  f1 = fbeta_score(predictions, true_labels, 1, average='macro') \n",
        "  print(\"Classifier '%s' has Acc=%0.3f P=%0.3f R=%0.3f F1=%0.3f\" % (description,accuracy,precision,recall,f1))\n",
        "  print(classification_report(predictions, true_labels, digits=3))\n",
        "  print('\\nConfusion matrix:\\n',confusion_matrix(true_labels, predictions))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TM4nvkjiTIPk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Select feature\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class ItemSelector(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"For data grouped by feature, select subset of data at a provided key.    \"\"\"\n",
        "\n",
        "    def __init__(self, key):\n",
        "        self.key = key\n",
        "\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, data_dict):\n",
        "        return data_dict[self.key]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bLoPOeZQaqfQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### vectorization"
      ]
    },
    {
      "metadata": {
        "id": "FX3EurrSTMYc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# one-hot encoding vectorizor\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Use FeatureUnion to combine the features from post body, thread title and post author id\n",
        "prediction_pipeline_1 = Pipeline([\n",
        "        ('union', FeatureUnion(\n",
        "          transformer_list=[\n",
        "            ('body', Pipeline([\n",
        "              ('selector', ItemSelector(key='body')),\n",
        "              ('one-hot', CountVectorizer(tokenizer=tokenize_normalize, binary=True)), \n",
        "              ])),\n",
        "            ('title', Pipeline([\n",
        "              ('selector', ItemSelector(key='title')),\n",
        "              ('one-hot', CountVectorizer(tokenizer=tokenize_normalize, binary=True)), \n",
        "              ])),\n",
        "            ('author', Pipeline([\n",
        "              ('selector', ItemSelector(key='author')),\n",
        "              ('one-hot', CountVectorizer(tokenizer=spacy_tokenize, binary=True)), \n",
        "              ])),\n",
        "        ])\n",
        "        )\n",
        "    ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HmcKi8VHa-qI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TF-IDF vectorizor\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Use FeatureUnion to combine the features from post body, thread title and post author id\n",
        "prediction_pipeline_2 = Pipeline([\n",
        "        ('union', FeatureUnion(\n",
        "          transformer_list=[\n",
        "            ('body', Pipeline([\n",
        "              ('selector', ItemSelector(key='body')),\n",
        "              ('tf-idf', TfidfVectorizer(tokenizer=tokenize_normalize)), \n",
        "              ])),\n",
        "            ('title', Pipeline([\n",
        "              ('selector', ItemSelector(key='title')),\n",
        "              ('tf-idf', TfidfVectorizer(tokenizer=tokenize_normalize)), \n",
        "              ])),\n",
        "            ('author', Pipeline([\n",
        "              ('selector', ItemSelector(key='author')),\n",
        "              ('tf-idf', TfidfVectorizer(tokenizer=spacy_tokenize)), \n",
        "              ])),\n",
        "        ])\n",
        "        )\n",
        "    ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U5cAQqVcTP3q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Construct train/test features using onr-hot encoding vectorizor\n",
        "one_hot_train_features = prediction_pipeline_1.fit_transform(train_data)\n",
        "one_hot_test_features = prediction_pipeline_1.transform(test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fYNDCnQUbbwj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Construct train/test features using TF-IDF vectorizor\n",
        "tf_idf_train_features = prediction_pipeline_2.fit_transform(train_data)\n",
        "tf_idf_test_features = prediction_pipeline_2.transform(test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ztn89s8eawXd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### DummyClassifier"
      ]
    },
    {
      "metadata": {
        "id": "Mef96IIHlU07",
        "colab_type": "code",
        "outputId": "32e7d21f-62c7-4377-a11b-688b3d3f9a3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1992
        }
      },
      "cell_type": "code",
      "source": [
        "#DummyClassifier, one-hot encoding\n",
        "from sklearn.dummy import DummyClassifier\n",
        "\n",
        "dummy_prior = DummyClassifier(strategy='stratified')\n",
        "dummy_prior.fit(one_hot_train_features, train_labels)\n",
        "print(dummy_prior.score(one_hot_test_features, test_labels))\n",
        "evaluation_summary(\"Dummy Prior with one-hot\", dummy_prior.predict(one_hot_test_features), test_labels)\n",
        "\n",
        "dummy_mf = DummyClassifier(strategy='most_frequent')\n",
        "dummy_mf.fit(one_hot_train_features, train_labels)\n",
        "print(dummy_mf.score(one_hot_test_features, test_labels))\n",
        "evaluation_summary(\"Dummy Majority with one-hot\", dummy_mf.predict(one_hot_test_features), test_labels)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DummyClassifier(constant=None, random_state=None, strategy='stratified')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        },
        {
          "output_type": "stream",
          "text": [
            "0.1232876712328767\n",
            "Evaluation for: Dummy Prior with one-hot\n",
            "Classifier 'Dummy Prior with one-hot' has Acc=0.107 P=0.051 R=0.055 F1=0.052\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "           askreddit      0.250     0.263     0.256        80\n",
            "             atheism      0.000     0.000     0.000         5\n",
            "            buildapc      0.108     0.108     0.108        37\n",
            "electronic_cigarette      0.000     0.000     0.000        17\n",
            "   explainlikeimfive      0.071     0.053     0.061        19\n",
            "              gaming      0.059     0.091     0.071        11\n",
            "         hearthstone      0.133     0.200     0.160        10\n",
            "           jailbreak      0.000     0.000     0.000        10\n",
            "     leagueoflegends      0.125     0.098     0.110        61\n",
            "              movies      0.000     0.000     0.000         7\n",
            "        pcmasterrace      0.087     0.118     0.100        17\n",
            "     personalfinance      0.000     0.000     0.000         3\n",
            "          reddit.com      0.000     0.000     0.000        10\n",
            "       relationships      0.000     0.000     0.000         6\n",
            "           starcraft      0.100     0.111     0.105         9\n",
            "      summonerschool      0.000     0.000     0.000        15\n",
            "         techsupport      0.077     0.059     0.067        17\n",
            "       tipofmytongue      0.000     0.000     0.000         7\n",
            "               trees      0.000     0.000     0.000        16\n",
            "         whowouldwin      0.000     0.000     0.000         8\n",
            "\n",
            "           micro avg      0.107     0.107     0.107       365\n",
            "           macro avg      0.051     0.055     0.052       365\n",
            "        weighted avg      0.106     0.107     0.106       365\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[21  0 13  0  6  3  3  3  7  2  4  0  3  1  1  3  4  4  3  3]\n",
            " [ 3  0  0  0  0  1  0  1  2  0  0  0  2  0  0  0  0  0  3  0]\n",
            " [10  0  4  4  1  1  1  1  7  0  2  1  0  0  1  0  1  1  1  1]\n",
            " [ 2  1  0  0  0  0  0  0  3  0  0  0  0  0  0  0  1  1  1  0]\n",
            " [ 5  0  0  1  1  1  0  0  4  0  1  0  1  0  0  0  0  0  0  0]\n",
            " [ 1  1  0  0  0  1  2  1  8  0  0  0  0  0  0  0  1  0  1  1]\n",
            " [ 3  0  1  3  0  0  2  0  1  1  0  0  0  0  0  1  2  0  0  1]\n",
            " [ 6  0  1  1  1  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0]\n",
            " [ 6  0  5  2  3  2  1  1  6  2  3  1  1  3  2  6  2  0  1  1]\n",
            " [ 1  1  0  0  0  0  0  0  1  0  0  0  0  0  1  0  0  1  0  0]\n",
            " [ 3  1  1  0  2  0  0  1  6  0  2  0  1  2  0  0  3  0  1  0]\n",
            " [ 1  0  2  1  0  1  0  1  1  0  1  0  0  0  2  0  0  0  0  0]\n",
            " [ 2  0  0  0  0  0  0  1  1  0  0  0  0  0  0  0  0  0  2  0]\n",
            " [ 1  0  0  1  0  1  1  0  1  0  0  0  0  0  0  0  0  0  1  0]\n",
            " [ 2  0  1  1  0  0  0  0  1  1  1  0  1  0  1  1  0  0  0  0]\n",
            " [ 0  0  0  0  1  0  0  0  4  0  1  0  0  0  0  0  0  0  0  0]\n",
            " [ 3  0  2  1  1  0  0  0  3  0  0  1  1  0  0  0  1  0  0  0]\n",
            " [ 3  0  2  1  0  0  0  0  2  0  0  0  0  0  0  2  1  0  0  0]\n",
            " [ 3  0  4  1  2  0  0  0  3  1  2  0  0  0  1  1  1  0  0  1]\n",
            " [ 4  1  1  0  1  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DummyClassifier(constant=None, random_state=None, strategy='most_frequent')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        },
        {
          "output_type": "stream",
          "text": [
            "0.23013698630136986\n",
            "Evaluation for: Dummy Majority with one-hot\n",
            "Classifier 'Dummy Majority with one-hot' has Acc=0.230 P=0.050 R=0.012 F1=0.019\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "           askreddit      1.000     0.230     0.374       365\n",
            "             atheism      0.000     0.000     0.000         0\n",
            "            buildapc      0.000     0.000     0.000         0\n",
            "electronic_cigarette      0.000     0.000     0.000         0\n",
            "   explainlikeimfive      0.000     0.000     0.000         0\n",
            "              gaming      0.000     0.000     0.000         0\n",
            "         hearthstone      0.000     0.000     0.000         0\n",
            "           jailbreak      0.000     0.000     0.000         0\n",
            "     leagueoflegends      0.000     0.000     0.000         0\n",
            "              movies      0.000     0.000     0.000         0\n",
            "        pcmasterrace      0.000     0.000     0.000         0\n",
            "     personalfinance      0.000     0.000     0.000         0\n",
            "          reddit.com      0.000     0.000     0.000         0\n",
            "       relationships      0.000     0.000     0.000         0\n",
            "           starcraft      0.000     0.000     0.000         0\n",
            "      summonerschool      0.000     0.000     0.000         0\n",
            "         techsupport      0.000     0.000     0.000         0\n",
            "       tipofmytongue      0.000     0.000     0.000         0\n",
            "               trees      0.000     0.000     0.000         0\n",
            "         whowouldwin      0.000     0.000     0.000         0\n",
            "\n",
            "           micro avg      0.230     0.230     0.230       365\n",
            "           macro avg      0.050     0.012     0.019       365\n",
            "        weighted avg      1.000     0.230     0.374       365\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[84  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [12  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [37  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 9  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [14  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [17  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [15  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [11  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [48  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [23  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 6  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 6  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 6  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [13  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [11  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 8  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "qmT-zH5lcFnd",
        "colab_type": "code",
        "outputId": "206b2ef6-f636-4c78-c46e-91f60978c01c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1992
        }
      },
      "cell_type": "code",
      "source": [
        "#DummyClassifier, TF-IDF\n",
        "from sklearn.dummy import DummyClassifier\n",
        "\n",
        "dummy_prior = DummyClassifier(strategy='stratified')\n",
        "dummy_prior.fit(tf_idf_train_features, train_labels)\n",
        "print(dummy_prior.score(tf_idf_test_features, test_labels))\n",
        "evaluation_summary(\"Dummy Prior with TF-IDF\", dummy_prior.predict(tf_idf_test_features), test_labels)\n",
        "\n",
        "dummy_mf = DummyClassifier(strategy='most_frequent')\n",
        "dummy_mf.fit(tf_idf_train_features, train_labels)\n",
        "print(dummy_mf.score(tf_idf_test_features, test_labels))\n",
        "evaluation_summary(\"Dummy Majority with TF-IDF\", dummy_mf.predict(tf_idf_test_features), test_labels)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DummyClassifier(constant=None, random_state=None, strategy='stratified')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        },
        {
          "output_type": "stream",
          "text": [
            "0.10136986301369863\n",
            "Evaluation for: Dummy Prior with TF-IDF\n",
            "Classifier 'Dummy Prior with TF-IDF' has Acc=0.107 P=0.039 R=0.048 F1=0.043\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "           askreddit      0.310     0.317     0.313        82\n",
            "             atheism      0.000     0.000     0.000         9\n",
            "            buildapc      0.081     0.091     0.086        33\n",
            "electronic_cigarette      0.000     0.000     0.000        12\n",
            "   explainlikeimfive      0.000     0.000     0.000        27\n",
            "              gaming      0.000     0.000     0.000        23\n",
            "         hearthstone      0.067     0.125     0.087         8\n",
            "           jailbreak      0.091     0.100     0.095        10\n",
            "     leagueoflegends      0.104     0.111     0.108        45\n",
            "              movies      0.000     0.000     0.000        11\n",
            "        pcmasterrace      0.087     0.167     0.114        12\n",
            "     personalfinance      0.000     0.000     0.000         8\n",
            "          reddit.com      0.000     0.000     0.000        10\n",
            "       relationships      0.000     0.000     0.000        11\n",
            "           starcraft      0.000     0.000     0.000         3\n",
            "      summonerschool      0.000     0.000     0.000         9\n",
            "         techsupport      0.000     0.000     0.000        13\n",
            "       tipofmytongue      0.000     0.000     0.000        12\n",
            "               trees      0.050     0.059     0.054        17\n",
            "         whowouldwin      0.000     0.000     0.000        10\n",
            "\n",
            "           micro avg      0.107     0.107     0.107       365\n",
            "           macro avg      0.039     0.048     0.043       365\n",
            "        weighted avg      0.099     0.107     0.102       365\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[26  1  6  4  6  6  1  3 13  1  0  1  2  1  0  2  3  2  3  3]\n",
            " [ 4  0  0  1  0  1  0  0  0  0  1  1  0  0  0  0  1  1  2  0]\n",
            " [ 9  0  3  0  3  2  0  0  6  2  1  1  1  1  1  1  1  1  2  2]\n",
            " [ 0  0  0  0  1  1  1  1  1  0  1  0  0  2  0  0  0  0  1  0]\n",
            " [ 5  0  1  1  0  0  0  0  2  2  0  0  0  1  0  2  0  0  0  0]\n",
            " [ 2  0  3  0  3  0  0  1  4  0  1  0  1  0  0  0  1  1  0  0]\n",
            " [ 2  0  1  2  2  1  1  0  3  0  0  1  0  0  0  0  0  0  0  2]\n",
            " [ 1  0  0  0  1  1  1  1  0  1  0  0  1  0  1  0  1  0  2  0]\n",
            " [ 9  4  5  2  2  3  0  1  5  0  3  1  2  3  0  1  1  4  1  1]\n",
            " [ 1  0  0  1  0  0  0  0  0  0  0  1  1  0  0  1  0  0  0  0]\n",
            " [ 4  0  2  0  2  1  2  1  1  2  2  1  0  1  0  1  2  0  1  0]\n",
            " [ 4  1  0  0  1  1  0  0  1  0  0  0  1  0  0  0  0  0  1  0]\n",
            " [ 1  1  1  0  0  0  0  0  2  0  0  0  0  1  0  0  0  0  0  0]\n",
            " [ 1  0  1  0  0  0  0  1  0  1  0  0  0  0  0  0  0  1  0  1]\n",
            " [ 3  0  1  0  0  0  0  0  1  0  1  0  0  0  0  0  1  1  1  1]\n",
            " [ 1  0  0  0  0  0  2  0  0  1  0  1  0  0  0  0  0  0  1  0]\n",
            " [ 5  0  0  0  1  3  0  0  2  0  0  0  0  1  0  1  0  0  0  0]\n",
            " [ 0  1  4  0  1  0  0  0  2  1  0  0  1  0  0  0  0  0  1  0]\n",
            " [ 2  0  3  1  4  2  0  1  2  0  2  0  0  0  1  0  1  0  1  0]\n",
            " [ 2  1  2  0  0  1  0  0  0  0  0  0  0  0  0  0  1  1  0  0]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DummyClassifier(constant=None, random_state=None, strategy='most_frequent')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        },
        {
          "output_type": "stream",
          "text": [
            "0.23013698630136986\n",
            "Evaluation for: Dummy Majority with TF-IDF\n",
            "Classifier 'Dummy Majority with TF-IDF' has Acc=0.230 P=0.050 R=0.012 F1=0.019\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "           askreddit      1.000     0.230     0.374       365\n",
            "             atheism      0.000     0.000     0.000         0\n",
            "            buildapc      0.000     0.000     0.000         0\n",
            "electronic_cigarette      0.000     0.000     0.000         0\n",
            "   explainlikeimfive      0.000     0.000     0.000         0\n",
            "              gaming      0.000     0.000     0.000         0\n",
            "         hearthstone      0.000     0.000     0.000         0\n",
            "           jailbreak      0.000     0.000     0.000         0\n",
            "     leagueoflegends      0.000     0.000     0.000         0\n",
            "              movies      0.000     0.000     0.000         0\n",
            "        pcmasterrace      0.000     0.000     0.000         0\n",
            "     personalfinance      0.000     0.000     0.000         0\n",
            "          reddit.com      0.000     0.000     0.000         0\n",
            "       relationships      0.000     0.000     0.000         0\n",
            "           starcraft      0.000     0.000     0.000         0\n",
            "      summonerschool      0.000     0.000     0.000         0\n",
            "         techsupport      0.000     0.000     0.000         0\n",
            "       tipofmytongue      0.000     0.000     0.000         0\n",
            "               trees      0.000     0.000     0.000         0\n",
            "         whowouldwin      0.000     0.000     0.000         0\n",
            "\n",
            "           micro avg      0.230     0.230     0.230       365\n",
            "           macro avg      0.050     0.012     0.019       365\n",
            "        weighted avg      1.000     0.230     0.374       365\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[84  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [12  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [37  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 9  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [14  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [17  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [15  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [11  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [48  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [23  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 6  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 6  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 6  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [13  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [11  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 8  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "I6m2mdpua4Xb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### LogisticRegression"
      ]
    },
    {
      "metadata": {
        "id": "g6py27-wfijw",
        "colab_type": "code",
        "outputId": "a752f286-fb1b-49eb-e931-262d017902bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1138
        }
      },
      "cell_type": "code",
      "source": [
        "#Classifier 1: LogisticRegression, one-hot encoding\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "lr = LogisticRegression()\n",
        "lr_model = lr.fit(one_hot_train_features, train_labels)\n",
        "print(lr_model.score(one_hot_test_features, test_labels))\n",
        "evaluation_summary(\"LogisticRegression with one-hot\", lr_model.predict(one_hot_test_features), test_labels)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.6931506849315069\n",
            "Evaluation for: LogisticRegression with one-hot\n",
            "Classifier 'LogisticRegression with one-hot' has Acc=0.693 P=0.589 R=0.775 F1=0.623\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "           askreddit      0.929     0.678     0.784       115\n",
            "             atheism      0.500     0.857     0.632         7\n",
            "            buildapc      0.757     0.800     0.778        35\n",
            "electronic_cigarette      0.889     0.800     0.842        10\n",
            "   explainlikeimfive      0.786     0.611     0.688        18\n",
            "              gaming      0.471     0.421     0.444        19\n",
            "         hearthstone      0.467     1.000     0.636         7\n",
            "           jailbreak      0.545     0.857     0.667         7\n",
            "     leagueoflegends      0.896     0.566     0.694        76\n",
            "              movies      0.400     1.000     0.571         2\n",
            "        pcmasterrace      0.217     0.625     0.323         8\n",
            "     personalfinance      0.900     1.000     0.947         9\n",
            "          reddit.com      0.000     0.000     0.000         0\n",
            "       relationships      1.000     0.857     0.923         7\n",
            "           starcraft      0.100     1.000     0.182         1\n",
            "      summonerschool      0.333     1.000     0.500         2\n",
            "         techsupport      0.769     0.588     0.667        17\n",
            "       tipofmytongue      0.818     1.000     0.900         9\n",
            "               trees      0.500     0.833     0.625        12\n",
            "         whowouldwin      0.500     1.000     0.667         4\n",
            "\n",
            "           micro avg      0.693     0.693     0.693       365\n",
            "           macro avg      0.589     0.775     0.623       365\n",
            "        weighted avg      0.797     0.693     0.719       365\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[78  0  0  0  3  3  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 4  6  0  0  1  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 1  0 28  1  1  1  0  0  2  0  0  0  0  0  0  0  3  0  0  0]\n",
            " [ 1  0  0  8  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 2  0  0  0 11  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 3  0  0  0  0  8  0  0  5  0  1  0  0  0  0  0  0  0  0  0]\n",
            " [ 1  0  0  0  0  1  7  1  2  0  1  0  0  1  0  0  0  0  1  0]\n",
            " [ 1  0  0  0  0  1  0  6  3  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 3  0  0  0  1  1  0  0 43  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 3  0  0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 2  0  6  0  0  2  0  0  4  0  5  0  0  0  0  0  4  0  0  0]\n",
            " [ 0  0  0  0  1  0  0  0  0  0  0  9  0  0  0  0  0  0  0  0]\n",
            " [ 5  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  6  0  0  0  0  0  0]\n",
            " [ 2  0  0  1  0  2  0  0  3  0  0  0  0  0  1  0  0  0  1  0]\n",
            " [ 0  0  0  0  0  0  0  0  4  0  0  0  0  0  0  2  0  0  0  0]\n",
            " [ 0  0  1  0  0  0  0  0  1  0  1  0  0  0  0  0 10  0  0  0]\n",
            " [ 2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  9  0  0]\n",
            " [ 5  0  0  0  0  0  0  0  5  0  0  0  0  0  0  0  0  0 10  0]\n",
            " [ 2  1  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  4]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "vG8JcL3YcnJ6",
        "colab_type": "code",
        "outputId": "7ae6cff5-a15e-4cae-fc1e-8256063fc7e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1138
        }
      },
      "cell_type": "code",
      "source": [
        "#Classifier 1: LogisticRegression,TF-IDF\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "lr = LogisticRegression()\n",
        "lr_model = lr.fit(tf_idf_train_features, train_labels)\n",
        "print(lr_model.score(tf_idf_test_features, test_labels))\n",
        "evaluation_summary(\"LogisticRegression  with TF-IDF\", lr_model.predict(tf_idf_test_features), test_labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.547945205479452\n",
            "Evaluation for: LogisticRegression  with TF-IDF\n",
            "Classifier 'LogisticRegression  with TF-IDF' has Acc=0.548 P=0.354 R=0.637 F1=0.380\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "           askreddit      0.952     0.440     0.602       182\n",
            "             atheism      0.083     1.000     0.154         1\n",
            "            buildapc      0.838     0.775     0.805        40\n",
            "electronic_cigarette      0.667     0.750     0.706         8\n",
            "   explainlikeimfive      0.357     0.714     0.476         7\n",
            "              gaming      0.059     0.250     0.095         4\n",
            "         hearthstone      0.067     1.000     0.125         1\n",
            "           jailbreak      0.182     1.000     0.308         2\n",
            "     leagueoflegends      0.854     0.488     0.621        84\n",
            "              movies      0.000     0.000     0.000         0\n",
            "        pcmasterrace      0.087     0.667     0.154         3\n",
            "     personalfinance      0.500     1.000     0.667         5\n",
            "          reddit.com      0.000     0.000     0.000         0\n",
            "       relationships      1.000     1.000     1.000         6\n",
            "           starcraft      0.000     0.000     0.000         0\n",
            "      summonerschool      0.000     0.000     0.000         0\n",
            "         techsupport      0.692     0.818     0.750        11\n",
            "       tipofmytongue      0.364     1.000     0.533         4\n",
            "               trees      0.250     0.833     0.385         6\n",
            "         whowouldwin      0.125     1.000     0.222         1\n",
            "\n",
            "           micro avg      0.548     0.548     0.548       365\n",
            "           macro avg      0.354     0.637     0.380       365\n",
            "        weighted avg      0.840     0.548     0.621       365\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[80  0  0  0  1  2  0  0  1  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [10  1  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 5  0 31  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0]\n",
            " [ 3  0  0  6  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 9  0  0  0  5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 7  0  0  0  0  1  0  0  8  0  1  0  0  0  0  0  0  0  0  0]\n",
            " [ 5  0  0  0  0  1  1  0  7  0  0  0  0  0  0  0  0  0  1  0]\n",
            " [ 6  0  0  0  0  0  0  2  3  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 6  0  1  0  0  0  0  0 41  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 7  0  6  0  0  0  0  0  7  0  2  0  0  0  0  0  1  0  0  0]\n",
            " [ 4  0  1  0  0  0  0  0  0  0  0  5  0  0  0  0  0  0  0  0]\n",
            " [ 5  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  6  0  0  0  0  0  0]\n",
            " [ 4  0  0  1  0  0  0  0  5  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  6  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 2  0  1  1  0  0  0  0  0  0  0  0  0  0  0  0  9  0  0  0]\n",
            " [ 7  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  4  0  0]\n",
            " [12  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0  0  0  5  0]\n",
            " [ 5  0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0  1]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "aDtJDDSla9cQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###  SVC Classifier"
      ]
    },
    {
      "metadata": {
        "id": "dITuQPcsRssv",
        "colab_type": "code",
        "outputId": "2e64af8e-1bb0-4d47-80ad-5046206c1b7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1103
        }
      },
      "cell_type": "code",
      "source": [
        "#Classifier 2: SVC Classifier, one-hot encoding\n",
        "from sklearn.svm import SVC\n",
        "svc_calssifier = SVC()\n",
        "svc_model = svc_calssifier.fit(one_hot_train_features, train_labels) \n",
        "print(svc_model.score(one_hot_test_features, test_labels))\n",
        "evaluation_summary(\"SVC Classifier with one-hot\", svc_model.predict(one_hot_test_features), test_labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.23013698630136986\n",
            "Evaluation for: SVC Classifier with one-hot\n",
            "Classifier 'SVC Classifier with one-hot' has Acc=0.230 P=0.050 R=0.012 F1=0.019\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "           askreddit      1.000     0.230     0.374       365\n",
            "             atheism      0.000     0.000     0.000         0\n",
            "            buildapc      0.000     0.000     0.000         0\n",
            "electronic_cigarette      0.000     0.000     0.000         0\n",
            "   explainlikeimfive      0.000     0.000     0.000         0\n",
            "              gaming      0.000     0.000     0.000         0\n",
            "         hearthstone      0.000     0.000     0.000         0\n",
            "           jailbreak      0.000     0.000     0.000         0\n",
            "     leagueoflegends      0.000     0.000     0.000         0\n",
            "              movies      0.000     0.000     0.000         0\n",
            "        pcmasterrace      0.000     0.000     0.000         0\n",
            "     personalfinance      0.000     0.000     0.000         0\n",
            "          reddit.com      0.000     0.000     0.000         0\n",
            "       relationships      0.000     0.000     0.000         0\n",
            "           starcraft      0.000     0.000     0.000         0\n",
            "      summonerschool      0.000     0.000     0.000         0\n",
            "         techsupport      0.000     0.000     0.000         0\n",
            "       tipofmytongue      0.000     0.000     0.000         0\n",
            "               trees      0.000     0.000     0.000         0\n",
            "         whowouldwin      0.000     0.000     0.000         0\n",
            "\n",
            "           micro avg      0.230     0.230     0.230       365\n",
            "           macro avg      0.050     0.012     0.019       365\n",
            "        weighted avg      1.000     0.230     0.374       365\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[84  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [12  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [37  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 9  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [14  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [17  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [15  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [11  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [48  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [23  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 6  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 6  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 6  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [13  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [11  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 8  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "vgEycUfvcvjP",
        "colab_type": "code",
        "outputId": "170c4097-3693-4767-938f-aace3957fc08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1103
        }
      },
      "cell_type": "code",
      "source": [
        "#Classifier 2: SVC Classifier, TF-IDF\n",
        "from sklearn.svm import SVC\n",
        "svc_calssifier = SVC()\n",
        "svc_model = svc_calssifier.fit(tf_idf_train_features, train_labels) \n",
        "print(svc_model.score(tf_idf_test_features, test_labels))\n",
        "evaluation_summary(\"SVC Classifier  with TF-IDF\", svc_model.predict(tf_idf_test_features), test_labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.23013698630136986\n",
            "Evaluation for: SVC Classifier  with TF-IDF\n",
            "Classifier 'SVC Classifier  with TF-IDF' has Acc=0.230 P=0.050 R=0.012 F1=0.019\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "           askreddit      1.000     0.230     0.374       365\n",
            "             atheism      0.000     0.000     0.000         0\n",
            "            buildapc      0.000     0.000     0.000         0\n",
            "electronic_cigarette      0.000     0.000     0.000         0\n",
            "   explainlikeimfive      0.000     0.000     0.000         0\n",
            "              gaming      0.000     0.000     0.000         0\n",
            "         hearthstone      0.000     0.000     0.000         0\n",
            "           jailbreak      0.000     0.000     0.000         0\n",
            "     leagueoflegends      0.000     0.000     0.000         0\n",
            "              movies      0.000     0.000     0.000         0\n",
            "        pcmasterrace      0.000     0.000     0.000         0\n",
            "     personalfinance      0.000     0.000     0.000         0\n",
            "          reddit.com      0.000     0.000     0.000         0\n",
            "       relationships      0.000     0.000     0.000         0\n",
            "           starcraft      0.000     0.000     0.000         0\n",
            "      summonerschool      0.000     0.000     0.000         0\n",
            "         techsupport      0.000     0.000     0.000         0\n",
            "       tipofmytongue      0.000     0.000     0.000         0\n",
            "               trees      0.000     0.000     0.000         0\n",
            "         whowouldwin      0.000     0.000     0.000         0\n",
            "\n",
            "           micro avg      0.230     0.230     0.230       365\n",
            "           macro avg      0.050     0.012     0.019       365\n",
            "        weighted avg      1.000     0.230     0.374       365\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[84  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [12  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [37  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 9  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [14  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [17  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [15  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [11  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [48  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [23  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 6  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 6  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 6  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [13  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [11  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 8  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "rPlpOkaobAkb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### BernoulliNB"
      ]
    },
    {
      "metadata": {
        "id": "B2xfqCYuQejG",
        "colab_type": "code",
        "outputId": "ecc122bd-d9c9-41fc-b645-33fb1ecbf79e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1067
        }
      },
      "cell_type": "code",
      "source": [
        "#Classifier 3: BernoulliNB, one-hot encoding\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "classifier = BernoulliNB()\n",
        "nb_model = classifier.fit(one_hot_train_features, train_labels)\n",
        "print(nb_model.score(one_hot_test_features, test_labels))\n",
        "evaluation_summary(\"BernoulliNB with one-hot\", nb_model.predict(one_hot_test_features), test_labels)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.3013698630136986\n",
            "Evaluation for: BernoulliNB with one-hot\n",
            "Classifier 'BernoulliNB with one-hot' has Acc=0.301 P=0.083 R=0.072 F1=0.062\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "           askreddit      1.000     0.262     0.415       321\n",
            "             atheism      0.000     0.000     0.000         0\n",
            "            buildapc      0.541     0.800     0.645        25\n",
            "electronic_cigarette      0.000     0.000     0.000         0\n",
            "   explainlikeimfive      0.000     0.000     0.000         2\n",
            "              gaming      0.000     0.000     0.000         0\n",
            "         hearthstone      0.000     0.000     0.000         0\n",
            "           jailbreak      0.000     0.000     0.000         0\n",
            "     leagueoflegends      0.125     0.375     0.188        16\n",
            "              movies      0.000     0.000     0.000         0\n",
            "        pcmasterrace      0.000     0.000     0.000         0\n",
            "     personalfinance      0.000     0.000     0.000         0\n",
            "          reddit.com      0.000     0.000     0.000         0\n",
            "       relationships      0.000     0.000     0.000         1\n",
            "           starcraft      0.000     0.000     0.000         0\n",
            "      summonerschool      0.000     0.000     0.000         0\n",
            "         techsupport      0.000     0.000     0.000         0\n",
            "       tipofmytongue      0.000     0.000     0.000         0\n",
            "               trees      0.000     0.000     0.000         0\n",
            "         whowouldwin      0.000     0.000     0.000         0\n",
            "\n",
            "           micro avg      0.301     0.301     0.301       365\n",
            "           macro avg      0.083     0.072     0.062       365\n",
            "        weighted avg      0.922     0.301     0.417       365\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[84  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [10  0  0  0  1  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0]\n",
            " [17  0 20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 9  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [14  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [17  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [12  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [11  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [42  0  0  0  0  0  0  0  6  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [20  0  3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 9  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 6  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 6  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 9  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  6  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [11  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [11  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 8  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "itwZgkYOdYwq",
        "colab_type": "code",
        "outputId": "35010dc3-e212-45f7-f66a-371f28259817",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1067
        }
      },
      "cell_type": "code",
      "source": [
        "#Classifier 3: BernoulliNB, TF-IDF\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "classifier = BernoulliNB()\n",
        "nb_model = classifier.fit(tf_idf_train_features, train_labels)\n",
        "print(nb_model.score(tf_idf_test_features, test_labels))\n",
        "evaluation_summary(\"BernoulliNB  with TF-IDF\", nb_model.predict(tf_idf_test_features), test_labels)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.3013698630136986\n",
            "Evaluation for: BernoulliNB  with TF-IDF\n",
            "Classifier 'BernoulliNB  with TF-IDF' has Acc=0.301 P=0.083 R=0.072 F1=0.062\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "           askreddit      1.000     0.262     0.415       321\n",
            "             atheism      0.000     0.000     0.000         0\n",
            "            buildapc      0.541     0.800     0.645        25\n",
            "electronic_cigarette      0.000     0.000     0.000         0\n",
            "   explainlikeimfive      0.000     0.000     0.000         2\n",
            "              gaming      0.000     0.000     0.000         0\n",
            "         hearthstone      0.000     0.000     0.000         0\n",
            "           jailbreak      0.000     0.000     0.000         0\n",
            "     leagueoflegends      0.125     0.375     0.188        16\n",
            "              movies      0.000     0.000     0.000         0\n",
            "        pcmasterrace      0.000     0.000     0.000         0\n",
            "     personalfinance      0.000     0.000     0.000         0\n",
            "          reddit.com      0.000     0.000     0.000         0\n",
            "       relationships      0.000     0.000     0.000         1\n",
            "           starcraft      0.000     0.000     0.000         0\n",
            "      summonerschool      0.000     0.000     0.000         0\n",
            "         techsupport      0.000     0.000     0.000         0\n",
            "       tipofmytongue      0.000     0.000     0.000         0\n",
            "               trees      0.000     0.000     0.000         0\n",
            "         whowouldwin      0.000     0.000     0.000         0\n",
            "\n",
            "           micro avg      0.301     0.301     0.301       365\n",
            "           macro avg      0.083     0.072     0.062       365\n",
            "        weighted avg      0.922     0.301     0.417       365\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[84  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [10  0  0  0  1  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0]\n",
            " [17  0 20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 9  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [14  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [17  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [12  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [11  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [42  0  0  0  0  0  0  0  6  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [20  0  3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 9  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 6  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 6  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 9  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  6  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [11  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [11  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 8  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "-N2vEdi_eAtK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Q2"
      ]
    },
    {
      "metadata": {
        "id": "CQ9CBzMipZxC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# LogisticRegression with TF-IDF vectorizor \n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class ItemSelector(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"For data grouped by feature, select subset of data at a provided key.    \"\"\"\n",
        "\n",
        "    def __init__(self, key):\n",
        "        self.key = key\n",
        "\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, data_dict):\n",
        "        return data_dict[self.key]\n",
        "      \n",
        "from sklearn.pipeline import FeatureUnion\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# Use FeatureUnion to combine the features from post body, thread title and post author id\n",
        "prediction_pipeline_3 = Pipeline([\n",
        "        ('union', FeatureUnion(\n",
        "          transformer_list=[\n",
        "            ('body', Pipeline([\n",
        "              ('selector', ItemSelector(key='body')),\n",
        "              ('tf-idf', TfidfVectorizer(tokenizer=tokenize_normalize)),            \n",
        "              ])),\n",
        "            ('title', Pipeline([\n",
        "              ('selector', ItemSelector(key='title')),\n",
        "              ('tf-idf', TfidfVectorizer(tokenizer=tokenize_normalize)), \n",
        "              ])),\n",
        "            ('author', Pipeline([\n",
        "              ('selector', ItemSelector(key='author')),\n",
        "              ('tf-idf', TfidfVectorizer(tokenizer=spacy_tokenize)), \n",
        "              ])),\n",
        "          ])),\n",
        "         ('logreg', LogisticRegression())\n",
        "    ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LNHeEaxa3-5q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### parameters tuning"
      ]
    },
    {
      "metadata": {
        "id": "pBNwbnEmrcTb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "parameter_sublinear_tf = {'union__body__tf-idf__sublinear_tf':(True, False)}\n",
        "parameter_ngram_range = {'union__body__tf-idf__ngram_range':((1,1), (1,2), (1,3))}\n",
        "parameter_max_features = {'union__body__tf-idf__max_features':(None, 5000, 10000, 50000)}\n",
        "\n",
        "parameter_C = {'logreg__C':[1000, 10000]}\n",
        "parameter_multi_class = {'logreg__multi_class':('ovr', 'multinomial', 'auto')}\n",
        "parameter_solver =  {'logreg__solver':('newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga')}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OKo8_1mARs0P",
        "colab_type": "code",
        "outputId": "7c558c80-62da-47e7-b5b7-5ba4d3210077",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        }
      },
      "cell_type": "code",
      "source": [
        "# sublinear_tf\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "grid_search = GridSearchCV(prediction_pipeline_3, param_grid=parameter_sublinear_tf, n_jobs=1, verbose=1, scoring='f1_macro', cv=2)\n",
        "print(\"Performing grid search...\")\n",
        "print(\"pipeline:\", [name for name, _ in prediction_pipeline_3.steps])\n",
        "print(\"parameters:\")\n",
        "print(parameter_sublinear_tf)\n",
        "# grid_search.get_params().keys()\n",
        "grid_search.fit(train_data, train_labels)\n",
        "\n",
        "# best choice: False\n",
        "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
        "print(\"Best parameters set:\")\n",
        "best_parameters = grid_search.best_estimator_.get_params()\n",
        "for param_name in sorted(parameter_sublinear_tf.keys()):\n",
        "  print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Performing grid search...\n",
            "pipeline: ['union', 'logreg']\n",
            "parameters:\n",
            "{'union__body__tf-idf__sublinear_tf': (True, False)}\n",
            "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:   43.1s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best score: 0.353\n",
            "Best parameters set:\n",
            "\tunion__body__tf-idf__sublinear_tf: False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "l_F_n_as4g9E",
        "colab_type": "code",
        "outputId": "37f94051-8f85-4ff3-df48-543b4acf41a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        }
      },
      "cell_type": "code",
      "source": [
        "# ngram_range\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "grid_search = GridSearchCV(prediction_pipeline_3, param_grid=parameter_ngram_range, n_jobs=1, verbose=1, scoring='f1_macro', cv=2)\n",
        "print(\"Performing grid search...\")\n",
        "print(\"pipeline:\", [name for name, _ in prediction_pipeline_3.steps])\n",
        "print(\"parameters:\")\n",
        "print(parameter_ngram_range)\n",
        "#grid_search.get_params().keys()\n",
        "grid_search.fit(train_data, train_labels)\n",
        "# best choice: (1, 1)\n",
        "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
        "print(\"Best parameters set:\")\n",
        "best_parameters = grid_search.best_estimator_.get_params()\n",
        "for param_name in sorted(parameter_ngram_range.keys()):\n",
        "  print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Performing grid search...\n",
            "pipeline: ['union', 'logreg']\n",
            "parameters:\n",
            "{'union__body__tf-idf__ngram_range': ((1, 1), (1, 2), (1, 3))}\n",
            "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:  1.8min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best score: 0.353\n",
            "Best parameters set:\n",
            "\tunion__body__tf-idf__ngram_range: (1, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lovHzRy74mbU",
        "colab_type": "code",
        "outputId": "2c038280-cf21-4773-8382-c333f0f014e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        }
      },
      "cell_type": "code",
      "source": [
        "# max_features \n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "grid_search = GridSearchCV(prediction_pipeline_3, param_grid=parameter_max_features, n_jobs=1, verbose=1, scoring='f1_macro', cv=2)\n",
        "print(\"Performing grid search...\")\n",
        "print(\"pipeline:\", [name for name, _ in prediction_pipeline_3.steps])\n",
        "print(\"parameters:\")\n",
        "print(parameter_max_features)\n",
        "grid_search.fit(train_data, train_labels)\n",
        "# best choice: 5000\n",
        "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
        "print(\"Best parameters set:\")\n",
        "best_parameters = grid_search.best_estimator_.get_params()\n",
        "for param_name in sorted(parameter_max_features.keys()):\n",
        "  print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Performing grid search...\n",
            "pipeline: ['union', 'logreg']\n",
            "parameters:\n",
            "{'union__body__tf-idf__max_features': (None, 5000, 10000, 50000)}\n",
            "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:  1.4min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best score: 0.355\n",
            "Best parameters set:\n",
            "\tunion__body__tf-idf__max_features: 5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MpYYjzwZ4s2T",
        "colab_type": "code",
        "outputId": "a3e85170-159c-4d6a-d16f-1dae00231133",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        }
      },
      "cell_type": "code",
      "source": [
        "# parameter_C\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "grid_search = GridSearchCV(prediction_pipeline_3, param_grid=parameter_C, n_jobs=1, verbose=1, scoring='f1_macro', cv=2)\n",
        "print(\"Performing grid search...\")\n",
        "print(\"pipeline:\", [name for name, _ in prediction_pipeline_3.steps])\n",
        "print(\"parameters:\")\n",
        "print(parameter_C)\n",
        "grid_search.fit(train_data, train_labels)\n",
        "# best choice: 10000\n",
        "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
        "print(\"Best parameters set:\")\n",
        "best_parameters = grid_search.best_estimator_.get_params()\n",
        "for param_name in sorted(parameter_C.keys()):\n",
        "  print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Performing grid search...\n",
            "pipeline: ['selector', 'one-hot', 'logreg']\n",
            "parameters:\n",
            "{'logreg__C': [1000, 10000]}\n",
            "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:   46.5s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best score: 0.395\n",
            "Best parameters set:\n",
            "\tlogreg__C: 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "yFCfXiN55CqS",
        "outputId": "6eccea66-994c-4a3c-c350-e44951d587d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        }
      },
      "cell_type": "code",
      "source": [
        "# parameter_solver\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "grid_search = GridSearchCV(prediction_pipeline_3, param_grid=parameter_solver, n_jobs=1, verbose=1, scoring='f1_macro', cv=2)\n",
        "print(\"Performing grid search...\")\n",
        "print(\"pipeline:\", [name for name, _ in prediction_pipeline_3.steps])\n",
        "print(\"parameters:\")\n",
        "print(parameter_solver)\n",
        "grid_search.fit(train_data, train_labels)\n",
        "# best choice: liblinear\n",
        "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
        "print(\"Best parameters set:\")\n",
        "best_parameters = grid_search.best_estimator_.get_params()\n",
        "for param_name in sorted(parameter_solver.keys()):\n",
        "  print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Performing grid search...\n",
            "pipeline: ['union', 'logreg']\n",
            "parameters:\n",
            "{'logreg__solver': ('newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga')}\n",
            "Fitting 2 folds for each of 5 candidates, totalling 10 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:  2.6min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best score: 0.353\n",
            "Best parameters set:\n",
            "\tlogreg__solver: 'liblinear'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QtJFdbi344sm",
        "colab_type": "code",
        "outputId": "c45acebd-731a-4a64-c482-35decc37a87f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "cell_type": "code",
      "source": [
        "# parameter_multi_class\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "grid_search = GridSearchCV(prediction_pipeline_3, param_grid=parameter_multi_class, n_jobs=1, verbose=1, scoring='f1_macro', cv=2)\n",
        "print(\"Performing grid search...\")\n",
        "print(\"pipeline:\", [name for name, _ in prediction_pipeline_3.steps])\n",
        "print(\"parameters:\")\n",
        "print(parameter_multi_class)\n",
        "grid_search.fit(train_data, train_labels)\n",
        "# best choice: auto\n",
        "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
        "print(\"Best parameters set:\")\n",
        "best_parameters = grid_search.best_estimator_.get_params()\n",
        "for param_name in sorted(parameter_multi_class.keys()):\n",
        "  print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Performing grid search...\n",
            "pipeline: ['union', 'logreg']\n",
            "parameters:\n",
            "{'logreg__multi_class': ('ovr', 'multinomial', 'auto')}\n",
            "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:  1.5min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best score: 0.353\n",
            "Best parameters set:\n",
            "\tlogreg__multi_class: 'auto'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4u_zdh0JPVl1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### error analysis"
      ]
    },
    {
      "metadata": {
        "id": "vKGbqIbeDH4N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import FeatureUnion\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# Use FeatureUnion to combine the features from post body, thread title and post author id\n",
        "prediction_pipeline_4 = Pipeline([\n",
        "        ('union', FeatureUnion(\n",
        "          transformer_list=[\n",
        "            ('body', Pipeline([\n",
        "              ('selector', ItemSelector(key='body')),\n",
        "              ('tf-idf', TfidfVectorizer(tokenizer=tokenize_normalize, max_features = 5000)),            \n",
        "              ])),\n",
        "            ('title', Pipeline([\n",
        "              ('selector', ItemSelector(key='title')),\n",
        "              ('tf-idf', TfidfVectorizer(tokenizer=tokenize_normalize, max_features = 5000)), \n",
        "              ])),\n",
        "            ('author', Pipeline([\n",
        "              ('selector', ItemSelector(key='author')),\n",
        "              ('tf-idf', TfidfVectorizer(tokenizer=spacy_tokenize, max_features = 5000)), \n",
        "              ])),\n",
        "          ])),\n",
        "         ('logreg', LogisticRegression(C=10000, solver='saga', multi_class='auto'))\n",
        "    ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z-aN2W1vpghv",
        "colab_type": "code",
        "outputId": "063a2a5b-ea0e-4a78-fed7-9490cb7cc290",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 958
        }
      },
      "cell_type": "code",
      "source": [
        "# LogisticRegression,TF-IDF, with tuned parameters\n",
        "prediction_pipeline_4.fit(train_data, train_labels)\n",
        "evaluation_summary(\"LogisticRegression,TF-IDF, with tuned parameters\", prediction_pipeline_4.predict(test_data), test_labels)                   "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Evaluation for: LogisticRegression,TF-IDF, with tuned parameters\n",
            "Classifier 'LogisticRegression,TF-IDF, with tuned parameters' has Acc=0.710 P=0.608 R=0.738 F1=0.623\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "           askreddit      0.869     0.652     0.745       112\n",
            "             atheism      0.417     0.833     0.556         6\n",
            "            buildapc      0.865     0.842     0.853        38\n",
            "electronic_cigarette      0.889     0.727     0.800        11\n",
            "   explainlikeimfive      1.000     0.636     0.778        22\n",
            "              gaming      0.471     0.471     0.471        17\n",
            "         hearthstone      0.667     1.000     0.800        10\n",
            "           jailbreak      0.727     1.000     0.842         8\n",
            "     leagueoflegends      0.854     0.719     0.781        57\n",
            "              movies      0.200     0.500     0.286         2\n",
            "        pcmasterrace      0.304     0.778     0.438         9\n",
            "     personalfinance      0.800     0.889     0.842         9\n",
            "          reddit.com      0.000     0.000     0.000         2\n",
            "       relationships      1.000     0.857     0.923         7\n",
            "           starcraft      0.200     1.000     0.333         2\n",
            "      summonerschool      0.167     1.000     0.286         1\n",
            "         techsupport      0.923     0.571     0.706        21\n",
            "       tipofmytongue      0.636     0.875     0.737         8\n",
            "               trees      0.550     0.688     0.611        16\n",
            "         whowouldwin      0.625     0.714     0.667         7\n",
            "\n",
            "           micro avg      0.710     0.710     0.710       365\n",
            "           macro avg      0.608     0.738     0.623       365\n",
            "        weighted avg      0.792     0.710     0.732       365\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[73  0  0  0  3  3  0  0  1  0  0  1  2  0  0  0  0  0  1  0]\n",
            " [ 3  5  0  0  3  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0]\n",
            " [ 1  0 32  0  0  0  0  0  0  0  0  0  0  0  0  0  4  0  0  0]\n",
            " [ 1  0  0  8  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0 14  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 3  1  0  0  0  8  0  0  4  0  0  0  0  0  0  0  0  1  0  0]\n",
            " [ 1  0  0  0  0  2 10  0  1  0  0  0  0  0  0  0  0  0  1  0]\n",
            " [ 2  0  0  0  0  0  0  8  0  0  0  0  0  0  0  0  1  0  0  0]\n",
            " [ 4  0  0  0  1  1  0  0 41  0  1  0  0  0  0  0  0  0  0  0]\n",
            " [ 4  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 4  0  5  0  0  1  0  0  1  0  7  0  0  0  0  0  4  0  0  1]\n",
            " [ 0  0  0  0  1  0  0  0  0  0  0  8  0  0  0  0  0  0  1  0]\n",
            " [ 3  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  2  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  6  0  0  0  0  0  0]\n",
            " [ 2  0  0  2  0  1  0  0  2  0  0  0  0  0  2  0  0  0  0  1]\n",
            " [ 0  0  0  0  0  0  0  0  5  0  0  0  0  0  0  1  0  0  0  0]\n",
            " [ 0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0 12  0  0  0]\n",
            " [ 3  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  7  0  0]\n",
            " [ 5  0  1  0  0  1  0  0  2  0  0  0  0  0  0  0  0  0 11  0]\n",
            " [ 3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  5]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0u8oU97NPSY6",
        "colab_type": "code",
        "outputId": "b3db4925-7353-424a-9984-9fbf0a18f9f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1906
        }
      },
      "cell_type": "code",
      "source": [
        "predictions = pd.Series(prediction_pipeline_4.predict(test_data)) \n",
        "comp = test_data\n",
        "comp = comp.assign(predict=predictions.values)\n",
        "comp[['subreddit', 'predict']]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>subreddit</th>\n",
              "      <th>predict</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>starcraft</td>\n",
              "      <td>gaming</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>whowouldwin</td>\n",
              "      <td>whowouldwin</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>askreddit</td>\n",
              "      <td>askreddit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>askreddit</td>\n",
              "      <td>askreddit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>tipofmytongue</td>\n",
              "      <td>tipofmytongue</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>trees</td>\n",
              "      <td>askreddit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>askreddit</td>\n",
              "      <td>askreddit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>tipofmytongue</td>\n",
              "      <td>tipofmytongue</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>explainlikeimfive</td>\n",
              "      <td>explainlikeimfive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>hearthstone</td>\n",
              "      <td>leagueoflegends</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>buildapc</td>\n",
              "      <td>askreddit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>askreddit</td>\n",
              "      <td>askreddit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>atheism</td>\n",
              "      <td>askreddit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>trees</td>\n",
              "      <td>trees</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>electronic_cigarette</td>\n",
              "      <td>electronic_cigarette</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>personalfinance</td>\n",
              "      <td>personalfinance</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>buildapc</td>\n",
              "      <td>techsupport</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>reddit.com</td>\n",
              "      <td>trees</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>buildapc</td>\n",
              "      <td>buildapc</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>leagueoflegends</td>\n",
              "      <td>leagueoflegends</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>leagueoflegends</td>\n",
              "      <td>leagueoflegends</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>askreddit</td>\n",
              "      <td>askreddit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>buildapc</td>\n",
              "      <td>buildapc</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>askreddit</td>\n",
              "      <td>askreddit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>relationships</td>\n",
              "      <td>relationships</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>askreddit</td>\n",
              "      <td>askreddit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>askreddit</td>\n",
              "      <td>askreddit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>atheism</td>\n",
              "      <td>explainlikeimfive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>trees</td>\n",
              "      <td>trees</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>techsupport</td>\n",
              "      <td>techsupport</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>335</th>\n",
              "      <td>askreddit</td>\n",
              "      <td>askreddit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>336</th>\n",
              "      <td>askreddit</td>\n",
              "      <td>askreddit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>337</th>\n",
              "      <td>pcmasterrace</td>\n",
              "      <td>techsupport</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>338</th>\n",
              "      <td>askreddit</td>\n",
              "      <td>askreddit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>339</th>\n",
              "      <td>askreddit</td>\n",
              "      <td>askreddit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>340</th>\n",
              "      <td>trees</td>\n",
              "      <td>trees</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>341</th>\n",
              "      <td>hearthstone</td>\n",
              "      <td>trees</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>342</th>\n",
              "      <td>trees</td>\n",
              "      <td>askreddit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>343</th>\n",
              "      <td>askreddit</td>\n",
              "      <td>askreddit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>344</th>\n",
              "      <td>electronic_cigarette</td>\n",
              "      <td>electronic_cigarette</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>345</th>\n",
              "      <td>hearthstone</td>\n",
              "      <td>gaming</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>346</th>\n",
              "      <td>buildapc</td>\n",
              "      <td>buildapc</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>347</th>\n",
              "      <td>personalfinance</td>\n",
              "      <td>personalfinance</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>348</th>\n",
              "      <td>pcmasterrace</td>\n",
              "      <td>askreddit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>349</th>\n",
              "      <td>askreddit</td>\n",
              "      <td>askreddit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>350</th>\n",
              "      <td>tipofmytongue</td>\n",
              "      <td>tipofmytongue</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>351</th>\n",
              "      <td>leagueoflegends</td>\n",
              "      <td>pcmasterrace</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>352</th>\n",
              "      <td>atheism</td>\n",
              "      <td>explainlikeimfive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>353</th>\n",
              "      <td>askreddit</td>\n",
              "      <td>askreddit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>354</th>\n",
              "      <td>personalfinance</td>\n",
              "      <td>personalfinance</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>355</th>\n",
              "      <td>tipofmytongue</td>\n",
              "      <td>askreddit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>356</th>\n",
              "      <td>trees</td>\n",
              "      <td>gaming</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>357</th>\n",
              "      <td>explainlikeimfive</td>\n",
              "      <td>explainlikeimfive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>358</th>\n",
              "      <td>starcraft</td>\n",
              "      <td>whowouldwin</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>359</th>\n",
              "      <td>buildapc</td>\n",
              "      <td>buildapc</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>360</th>\n",
              "      <td>reddit.com</td>\n",
              "      <td>askreddit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>361</th>\n",
              "      <td>atheism</td>\n",
              "      <td>relationships</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>362</th>\n",
              "      <td>pcmasterrace</td>\n",
              "      <td>pcmasterrace</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>363</th>\n",
              "      <td>askreddit</td>\n",
              "      <td>askreddit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>364</th>\n",
              "      <td>askreddit</td>\n",
              "      <td>askreddit</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>365 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                subreddit               predict\n",
              "0               starcraft                gaming\n",
              "1             whowouldwin           whowouldwin\n",
              "2               askreddit             askreddit\n",
              "3               askreddit             askreddit\n",
              "4           tipofmytongue         tipofmytongue\n",
              "5                   trees             askreddit\n",
              "6               askreddit             askreddit\n",
              "7           tipofmytongue         tipofmytongue\n",
              "8       explainlikeimfive     explainlikeimfive\n",
              "9             hearthstone       leagueoflegends\n",
              "10               buildapc             askreddit\n",
              "11              askreddit             askreddit\n",
              "12                atheism             askreddit\n",
              "13                  trees                 trees\n",
              "14   electronic_cigarette  electronic_cigarette\n",
              "15        personalfinance       personalfinance\n",
              "16               buildapc           techsupport\n",
              "17             reddit.com                 trees\n",
              "18               buildapc              buildapc\n",
              "19        leagueoflegends       leagueoflegends\n",
              "20        leagueoflegends       leagueoflegends\n",
              "21              askreddit             askreddit\n",
              "22               buildapc              buildapc\n",
              "23              askreddit             askreddit\n",
              "24          relationships         relationships\n",
              "25              askreddit             askreddit\n",
              "26              askreddit             askreddit\n",
              "27                atheism     explainlikeimfive\n",
              "28                  trees                 trees\n",
              "29            techsupport           techsupport\n",
              "..                    ...                   ...\n",
              "335             askreddit             askreddit\n",
              "336             askreddit             askreddit\n",
              "337          pcmasterrace           techsupport\n",
              "338             askreddit             askreddit\n",
              "339             askreddit             askreddit\n",
              "340                 trees                 trees\n",
              "341           hearthstone                 trees\n",
              "342                 trees             askreddit\n",
              "343             askreddit             askreddit\n",
              "344  electronic_cigarette  electronic_cigarette\n",
              "345           hearthstone                gaming\n",
              "346              buildapc              buildapc\n",
              "347       personalfinance       personalfinance\n",
              "348          pcmasterrace             askreddit\n",
              "349             askreddit             askreddit\n",
              "350         tipofmytongue         tipofmytongue\n",
              "351       leagueoflegends          pcmasterrace\n",
              "352               atheism     explainlikeimfive\n",
              "353             askreddit             askreddit\n",
              "354       personalfinance       personalfinance\n",
              "355         tipofmytongue             askreddit\n",
              "356                 trees                gaming\n",
              "357     explainlikeimfive     explainlikeimfive\n",
              "358             starcraft           whowouldwin\n",
              "359              buildapc              buildapc\n",
              "360            reddit.com             askreddit\n",
              "361               atheism         relationships\n",
              "362          pcmasterrace          pcmasterrace\n",
              "363             askreddit             askreddit\n",
              "364             askreddit             askreddit\n",
              "\n",
              "[365 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "metadata": {
        "id": "eVW8Em2rY3_u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### feature development"
      ]
    },
    {
      "metadata": {
        "id": "vGhHG7UubRiv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Tokenize\n",
        "def spacy_tokenize(string):\n",
        "  tokens = list()\n",
        "  doc = nlp(string)\n",
        "  for token in doc:\n",
        "    tokens.append(token)\n",
        "  return tokens\n",
        "\n",
        "#Normalize\n",
        "def normalize(tokens):\n",
        "  normalized_tokens = list()\n",
        "  for token in tokens:\n",
        "    normalized = token.text.lower().strip()\n",
        "    if ((token.is_alpha or token.is_digit or !token.is_stop)): #new feature 1: stopwords handling\n",
        "      normalized_tokens.append(normalized)\n",
        "  return normalized_tokens\n",
        "  return normalized_tokens\n",
        "\n",
        "#Tokenize and normalize\n",
        "def tokenize_normalize(string):\n",
        "  return normalize(spacy_tokenize(string))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E3kClTx-WhiT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import FeatureUnion\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# Use FeatureUnion to combine the features from post body, thread title and post author id\n",
        "prediction_pipeline_5 = Pipeline([\n",
        "        ('union', FeatureUnion(\n",
        "          transformer_list=[\n",
        "            ('body', Pipeline([\n",
        "              ('selector', ItemSelector(key='body')),\n",
        "              ('tf-idf', TfidfVectorizer(tokenizer=tokenize_normalize, max_features = 5000)),            \n",
        "              ])),\n",
        "            ('title', Pipeline([\n",
        "              ('selector', ItemSelector(key='title')),\n",
        "              ('tf-idf', TfidfVectorizer(tokenizer=tokenize_normalize, max_features = 5000)), \n",
        "              ])),\n",
        "            ('author', Pipeline([\n",
        "              ('selector', ItemSelector(key='author')),\n",
        "              ('tf-idf', TfidfVectorizer(tokenizer=spacy_tokenize, max_features = 5000)), \n",
        "              ])),\n",
        "            ('len_of_posts', Pipeline([\n",
        "              ('selector', ItemSelector(key='len_of_posts')),\n",
        "              ('tf-idf', TfidfVectorizer(tokenizer=spacy_tokenize, max_features = 5000)),     #new feature 2: length of posts       \n",
        "              ])),\n",
        "#             ('len_of_thread', Pipeline([\n",
        "#               ('selector', ItemSelector(key='len_of_posts')),\n",
        "#               ('tf-idf', TfidfVectorizer(tokenizer=spacy_tokenize, max_features = 5000)),            \n",
        "#               ])),\n",
        "          ])),\n",
        "         ('logreg', LogisticRegression(C=10000, solver='saga', multi_class='auto'))\n",
        "    ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xty6tC_sZZOQ",
        "colab_type": "code",
        "outputId": "f3d887b5-84b6-4a4b-bf14-9ee89f7ea9e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1174
        }
      },
      "cell_type": "code",
      "source": [
        "# LogisticRegression,TF-IDF, with added features\n",
        "prediction_pipeline_5.fit(train_data, train_labels)\n",
        "evaluation_summary(\"LogisticRegression,TF-IDF, with added features\", prediction_pipeline_5.predict(test_data), test_labels)         "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "     steps=[('union', FeatureUnion(n_jobs=None,\n",
              "       transformer_list=[('body', Pipeline(memory=None,\n",
              "     steps=[('selector', ItemSelector(key='body')), ('tf-idf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='cont...penalty='l2', random_state=None, solver='saga',\n",
              "          tol=0.0001, verbose=0, warm_start=False))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 141
        },
        {
          "output_type": "stream",
          "text": [
            "Evaluation for: LogisticRegression,TF-IDF, with added features\n",
            "Classifier 'LogisticRegression,TF-IDF, with added features' has Acc=0.658 P=0.523 R=0.676 F1=0.544\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "           askreddit      0.917     0.570     0.703       135\n",
            "             atheism      0.333     1.000     0.500         4\n",
            "            buildapc      0.865     0.744     0.800        43\n",
            "electronic_cigarette      0.778     0.700     0.737        10\n",
            "   explainlikeimfive      0.786     0.611     0.688        18\n",
            "              gaming      0.235     0.400     0.296        10\n",
            "         hearthstone      0.467     1.000     0.636         7\n",
            "           jailbreak      0.727     1.000     0.842         8\n",
            "     leagueoflegends      0.875     0.646     0.743        65\n",
            "              movies      0.200     0.500     0.286         2\n",
            "        pcmasterrace      0.174     0.667     0.276         6\n",
            "     personalfinance      0.700     0.875     0.778         8\n",
            "          reddit.com      0.000     0.000     0.000         0\n",
            "       relationships      1.000     0.750     0.857         8\n",
            "           starcraft      0.100     1.000     0.182         1\n",
            "      summonerschool      0.000     0.000     0.000         0\n",
            "         techsupport      0.769     0.588     0.667        17\n",
            "       tipofmytongue      0.636     0.875     0.737         8\n",
            "               trees      0.400     0.800     0.533        10\n",
            "         whowouldwin      0.500     0.800     0.615         5\n",
            "\n",
            "           micro avg      0.658     0.658     0.658       365\n",
            "           macro avg      0.523     0.676     0.544       365\n",
            "        weighted avg      0.801     0.658     0.698       365\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[77  0  0  0  3  3  0  0  0  0  0  1  0  0  0  0  0  0  0  0]\n",
            " [ 5  4  0  0  2  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0]\n",
            " [ 2  0 32  0  0  0  0  0  0  0  0  0  0  0  0  0  3  0  0  0]\n",
            " [ 2  0  0  7  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 3  0  0  0 11  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 6  0  0  0  0  4  0  0  6  0  0  0  0  0  0  0  0  1  0  0]\n",
            " [ 1  0  1  0  0  1  7  0  4  0  0  0  0  1  0  0  0  0  0  0]\n",
            " [ 2  0  0  0  0  0  0  8  0  0  0  0  0  0  0  0  1  0  0  0]\n",
            " [ 4  0  0  0  1  0  0  0 42  0  1  0  0  0  0  0  0  0  0  0]\n",
            " [ 4  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 5  0  7  0  0  1  0  0  3  0  4  0  0  0  0  0  3  0  0  0]\n",
            " [ 2  0  0  0  1  0  0  0  0  0  0  7  0  0  0  0  0  0  0  0]\n",
            " [ 3  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  2  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  6  0  0  0  0  0  0]\n",
            " [ 2  0  0  2  0  1  0  0  3  0  0  0  0  0  1  0  0  0  0  1]\n",
            " [ 0  0  0  0  0  0  0  0  6  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  2  1  0  0  0  0  0  0  0  0  0  0  0  0 10  0  0  0]\n",
            " [ 3  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  7  0  0]\n",
            " [11  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  8  0]\n",
            " [ 3  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  4]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "D-G938e8D5qn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Part B: Discourse prediction ##"
      ]
    },
    {
      "metadata": {
        "id": "TcMhasTVB94F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Q3"
      ]
    },
    {
      "metadata": {
        "id": "9KM6aJtSETPY",
        "colab_type": "code",
        "outputId": "474654b2-b149-4094-a8b9-e952fcb7f505",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "cell_type": "code",
      "source": [
        "discourse_train = \"coursework_discourse_train.json\"\n",
        "discourse_test = \"coursework_discourse_test.json\"\n",
        "  \n",
        "!gsutil cp gs://textasdata/coursework/coursework_discourse_train.json $discourse_train  \n",
        "!gsutil cp gs://textasdata/coursework/coursework_discourse_test.json  $discourse_test"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://textasdata/coursework/coursework_discourse_train.json...\n",
            "- [1 files][ 60.2 MiB/ 60.2 MiB]                                                \n",
            "Operation completed over 1 objects/60.2 MiB.                                     \n",
            "Copying gs://textasdata/coursework/coursework_discourse_test.json...\n",
            "/ [1 files][ 15.1 MiB/ 15.1 MiB]                                                \n",
            "Operation completed over 1 objects/15.1 MiB.                                     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pwOaf_6aD-Vh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# The reddit thread structure is nested with posts in a new content.\n",
        "# This block reads the file as json and creates a new data frame.\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "def load_posts(file):\n",
        "  # A temporary variable to store the list of post content.\n",
        "  posts_tmp = list()\n",
        "\n",
        "  with open(file) as jsonfile:\n",
        "    for i, line in enumerate(jsonfile):\n",
        "     # if (i > 2): break\n",
        "      thread = json.loads(line)\n",
        "      for post in thread['posts']:\n",
        "        # NOTE: This could be changed to use additional features from the post or thread.\n",
        "        # DO NOT change the labels for the test set.\n",
        "        posts_tmp.append((thread['subreddit'], thread['title'], thread['url'],\n",
        "                        post['id'], post.get('author', \"\"), post.get('body', \"\"), post.get(\"majority_link\", \"\"), \n",
        "                        post.get('post_depth', 0), post.get('majority_type', \"\"), # discourse type label \n",
        "                        post.get('in_reply_to', \"\") ))\n",
        "\n",
        "# Create the posts data frame.  \n",
        "  labels = ['subreddit', 'title', 'url', 'id', 'author', 'body', 'majority_link', \n",
        "          'post_depth', 'discourse_type', 'in_reply_to']\n",
        "  return pd.DataFrame(posts_tmp, columns=labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PDzHTDcmEQ11",
        "colab_type": "code",
        "outputId": "2af349d7-8b3b-4d36-a728-de4691f7eec6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        }
      },
      "cell_type": "code",
      "source": [
        "train_posts = load_posts(discourse_train)\n",
        "# Filter out empty labels\n",
        "train_posts = train_posts[train_posts['discourse_type'] != \"\"]\n",
        "print(train_posts.head())\n",
        "print(\"Num posts: \", train_posts.size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    subreddit                           title  \\\n",
            "0  worldofpvp  Help me decide my new PvP main   \n",
            "1  worldofpvp  Help me decide my new PvP main   \n",
            "2  worldofpvp  Help me decide my new PvP main   \n",
            "3  worldofpvp  Help me decide my new PvP main   \n",
            "4  worldofpvp  Help me decide my new PvP main   \n",
            "\n",
            "                                                 url          id  \\\n",
            "0  https://www.reddit.com/r/worldofpvp/comments/2...   t3_2v0anq   \n",
            "1  https://www.reddit.com/r/worldofpvp/comments/2...  t1_codb2p9   \n",
            "2  https://www.reddit.com/r/worldofpvp/comments/2...  t1_codg0we   \n",
            "3  https://www.reddit.com/r/worldofpvp/comments/2...  t1_coeatsq   \n",
            "4  https://www.reddit.com/r/worldofpvp/comments/2...  t1_codbyit   \n",
            "\n",
            "         author                                               body  \\\n",
            "0      TyrickEU  Hi. \\nAs a raider previously, i had no problem...   \n",
            "1          vurt  [deleted]  \\n ^^^^^^^^^^^^^^^^0.5422 \\n > [Wha...   \n",
            "2   OptimusNice  This goes mostly for 3v3 since that seems to b...   \n",
            "3                Rets are in a good position right now, althoug...   \n",
            "4  Rageinjector  Druid are the best pvp healer atm and are grea...   \n",
            "\n",
            "  majority_link  post_depth discourse_type in_reply_to  \n",
            "0          none           0       question              \n",
            "1     t3_2v0anq           1         answer   t3_2v0anq  \n",
            "2     t3_2v0anq           1         answer   t3_2v0anq  \n",
            "3     t3_2v0anq           1         answer   t3_2v0anq  \n",
            "4     t3_2v0anq           1         answer   t3_2v0anq  \n",
            "Num posts:  792670\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dqGLzyTOGadY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The label for the post we will be predicting is in the discourse_type column."
      ]
    },
    {
      "metadata": {
        "id": "7vvo7hfCEmvj",
        "colab_type": "code",
        "outputId": "2399a74a-9bfd-4b8f-da6a-e45d2e73e637",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "test_posts = load_posts(discourse_test)\n",
        "# Filter out empty labels\n",
        "test_posts = test_posts[test_posts['discourse_type'] != \"\"]\n",
        "print(\"Num posts: \", test_posts.size)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num posts:  198120\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Jat55HhNHGBp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_labels = train_posts['discourse_type']\n",
        "test_labels = test_posts['discourse_type']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rFl6sM58HNFp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Examine the distribution over labels on the training data."
      ]
    },
    {
      "metadata": {
        "id": "n3NbLPBhFOkp",
        "colab_type": "code",
        "outputId": "8afb87c3-2e05-4d36-e111-24a3817f8d02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "cell_type": "code",
      "source": [
        "discourse_counts = train_labels.value_counts()\n",
        "print(discourse_counts.describe())\n",
        "\n",
        "top_discourse = discourse_counts.nlargest(200)\n",
        "print(top_discourse)\n",
        "top_discourse = top_discourse.index.tolist()\n",
        "print(top_discourse)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "count       10.000000\n",
            "mean      7926.700000\n",
            "std       9664.321866\n",
            "min       1266.000000\n",
            "25%       1671.500000\n",
            "50%       3235.500000\n",
            "75%      11919.750000\n",
            "max      31419.000000\n",
            "Name: discourse_type, dtype: float64\n",
            "answer              31419\n",
            "elaboration         14775\n",
            "question            13610\n",
            "appreciation         6849\n",
            "agreement            3868\n",
            "disagreement         2603\n",
            "humor                1787\n",
            "other                1633\n",
            "announcement         1457\n",
            "negativereaction     1266\n",
            "Name: discourse_type, dtype: int64\n",
            "['answer', 'elaboration', 'question', 'appreciation', 'agreement', 'disagreement', 'humor', 'other', 'announcement', 'negativereaction']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-OHHdXnH8lM1",
        "colab_type": "code",
        "outputId": "ba8d0f6e-dca8-4497-f5e9-838966df1f70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        }
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the medium english model. \n",
        "!python -m spacy download en_core_web_md\n",
        "\n",
        "nlp = spacy.load('en_core_web_md', disable=['ner'])\n",
        "nlp.remove_pipe('tagger')\n",
        "nlp.remove_pipe('parser')\n",
        "\n",
        "#Tokenize\n",
        "def spacy_tokenize(string):\n",
        "  tokens = list()\n",
        "  doc = nlp(string)\n",
        "  for token in doc:\n",
        "    tokens.append(token)\n",
        "  return tokens\n",
        "\n",
        "#Normalize\n",
        "def normalize(tokens):\n",
        "  normalized_tokens = list()\n",
        "  for token in tokens:\n",
        "    normalized = token.text.lower().strip()\n",
        "    if ((token.is_alpha or token.is_digit)):\n",
        "      normalized_tokens.append(normalized)\n",
        "  return normalized_tokens\n",
        "  return normalized_tokens\n",
        "\n",
        "#Tokenize and normalize\n",
        "def tokenize_normalize(string):\n",
        "  return normalize(spacy_tokenize(string))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_md==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.0.0/en_core_web_md-2.0.0.tar.gz#egg=en_core_web_md==2.0.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.0.0/en_core_web_md-2.0.0.tar.gz (120.8MB)\n",
            "\u001b[K    100% |████████████████████████████████| 120.9MB 96.7MB/s \n",
            "\u001b[?25hInstalling collected packages: en-core-web-md\n",
            "  Running setup.py install for en-core-web-md ... \u001b[?25ldone\n",
            "\u001b[?25hSuccessfully installed en-core-web-md-2.0.0\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/en_core_web_md -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/en_core_web_md\n",
            "\n",
            "    You can now load the model via spacy.load('en_core_web_md')\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QWZOpkca_lgC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Evaluation: accuracy, macro-averaged precision, recall, and F1 measures\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import fbeta_score\n",
        "\n",
        "def evaluation_summary(description, predictions, true_labels):\n",
        "  print(\"Evaluation for: \" + description)\n",
        "  precision = precision_score(predictions, true_labels,average='macro')\n",
        "  recall = recall_score(predictions, true_labels, average='macro')\n",
        "  accuracy = accuracy_score(predictions, true_labels)\n",
        "  f1 = fbeta_score(predictions, true_labels, 1, average='macro') \n",
        "  print(\"Classifier '%s' has Acc=%0.3f P=%0.3f R=%0.3f F1=%0.3f\" % (description,accuracy,precision,recall,f1))\n",
        "  print(classification_report(predictions, true_labels, digits=3))\n",
        "  print('\\nConfusion matrix:\\n',confusion_matrix(true_labels, predictions))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pbVa1fkZGHVQ",
        "colab_type": "code",
        "outputId": "bf07f21c-70bb-4304-a8f3-4e6fe623ff22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        }
      },
      "cell_type": "code",
      "source": [
        "# Select feature\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class ItemSelector(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"For data grouped by feature, select subset of data at a provided key.    \"\"\"\n",
        "\n",
        "    def __init__(self, key):\n",
        "        self.key = key\n",
        "\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, data_dict):\n",
        "        return data_dict[self.key]\n",
        "      \n",
        "# TF-IDF vectorizor\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Use FeatureUnion to combine the features from post body, thread title and post author id\n",
        "prediction_pipeline_a = Pipeline([\n",
        "        ('union', FeatureUnion(\n",
        "          transformer_list=[\n",
        "            ('body', Pipeline([\n",
        "              ('selector', ItemSelector(key='body')),\n",
        "              ('tf-idf', TfidfVectorizer(tokenizer=tokenize_normalize, max_features = 5000)), \n",
        "              ])),\n",
        "            ('title', Pipeline([\n",
        "              ('selector', ItemSelector(key='title')),\n",
        "              ('tf-idf', TfidfVectorizer(tokenizer=tokenize_normalize, max_features = 5000)), \n",
        "              ])),\n",
        "        ])\n",
        "        )\n",
        "    ])\n",
        "\n",
        "# Define the features\n",
        "X_train = prediction_pipeline_a.fit_transform(train_posts)\n",
        "X_test = prediction_pipeline_a.transform(test_posts)\n",
        "\n",
        "from sklearn.dummy import DummyClassifier\n",
        "clf = DummyClassifier(strategy='stratified',random_state=0)\n",
        "clf.fit(X_train, train_posts['discourse_type'])\n",
        "predictions = clf.predict(X_test)  \n",
        "evaluation_summary(\"DummyClassifier with TF-IDF\", predictions, test_posts['discourse_type'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation for: DummyClassifier with TF-IDF\n",
            "Classifier 'DummyClassifier with TF-IDF' has Acc=0.233 P=0.101 R=0.101 F1=0.101\n",
            "                  precision    recall  f1-score   support\n",
            "\n",
            "       agreement      0.055     0.056     0.055       934\n",
            "    announcement      0.022     0.021     0.021       382\n",
            "          answer      0.393     0.395     0.394      7910\n",
            "    appreciation      0.079     0.085     0.082      1603\n",
            "    disagreement      0.037     0.035     0.036       686\n",
            "     elaboration      0.187     0.183     0.185      3711\n",
            "           humor      0.024     0.024     0.024       449\n",
            "negativereaction      0.026     0.027     0.026       297\n",
            "           other      0.021     0.020     0.021       395\n",
            "        question      0.167     0.166     0.167      3445\n",
            "\n",
            "       micro avg      0.233     0.233     0.233     19812\n",
            "       macro avg      0.101     0.101     0.101     19812\n",
            "    weighted avg      0.233     0.233     0.233     19812\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[  52   20  379   66   38  165   28   19   14  170]\n",
            " [  22    8  129   35   10   64   12    5    6   74]\n",
            " [ 359  166 3123  633  291 1520  179  130  155 1384]\n",
            " [  76   25  688  136   54  321   37   29   41  313]\n",
            " [  20    8  266   62   24  110   25    6   14  106]\n",
            " [ 169   61 1466  305  110  680   76   40   79  644]\n",
            " [  19    6  193   41   13   88   11    7    7   72]\n",
            " [  15    5  126   25    8   54    5    8   10   51]\n",
            " [  23    7  149   41   11   61    7   11    8   58]\n",
            " [ 179   76 1391  259  127  648   69   42   61  573]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "e0Gv-M7043AA",
        "colab_type": "code",
        "outputId": "9bf6e728-fcd2-4de2-cd7d-c7a7a6521048",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 603
        }
      },
      "cell_type": "code",
      "source": [
        "# LogisticRegression,TF-IDF\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "logreg = LogisticRegression(C=10000, solver='saga', multi_class='auto')\n",
        "logreg_model = logreg.fit(X_train, train_posts['discourse_type'])\n",
        "predictions = logreg_model.predict(X_test)  \n",
        "evaluation_summary(\"LogisticRegression  with TF-IDF\", predictions, test_posts['discourse_type'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Evaluation for: LogisticRegression  with TF-IDF\n",
            "Classifier 'LogisticRegression  with TF-IDF' has Acc=0.399 P=0.243 R=0.239 F1=0.240\n",
            "                  precision    recall  f1-score   support\n",
            "\n",
            "       agreement      0.232     0.201     0.215      1102\n",
            "    announcement      0.115     0.066     0.084       639\n",
            "          answer      0.544     0.528     0.536      8173\n",
            "    appreciation      0.525     0.517     0.521      1745\n",
            "    disagreement      0.090     0.092     0.091       630\n",
            "     elaboration      0.226     0.266     0.244      3074\n",
            "           humor      0.081     0.097     0.088       383\n",
            "negativereaction      0.091     0.103     0.097       271\n",
            "           other      0.106     0.091     0.098       438\n",
            "        question      0.419     0.427     0.423      3357\n",
            "\n",
            "       micro avg      0.399     0.399     0.399     19812\n",
            "       macro avg      0.243     0.239     0.240     19812\n",
            "    weighted avg      0.401     0.399     0.399     19812\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[ 221   30  313   73   43  141   11    8   18   93]\n",
            " [  11   42  129   13    4   87    7    3   17   52]\n",
            " [ 364  220 4316  332  241 1168  119   93  101  986]\n",
            " [  91   28  287  903   23  126   27   20   78  137]\n",
            " [  39   25  272   14   58  125   11    9    9   79]\n",
            " [ 216  155 1453  194  145  819   72   46   93  437]\n",
            " [  14   18  172   34   21   69   37   15   22   55]\n",
            " [  10    5   91   26   19   53   13   28   16   46]\n",
            " [  14   16  133   38    5   65   16   12   40   37]\n",
            " [ 122  100 1007  118   71  421   70   37   44 1435]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0aokyrMKCTDG",
        "colab_type": "code",
        "outputId": "627354d6-963e-4e7b-bb54-68f3e7b178d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5476
        }
      },
      "cell_type": "code",
      "source": [
        "predict = pd.Series(predictions)\n",
        "test_tmp = test_posts\n",
        "test_tmp = test_tmp.assign(predict=predict.values)\n",
        "test_tmp"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>subreddit</th>\n",
              "      <th>title</th>\n",
              "      <th>url</th>\n",
              "      <th>id</th>\n",
              "      <th>author</th>\n",
              "      <th>body</th>\n",
              "      <th>majority_link</th>\n",
              "      <th>post_depth</th>\n",
              "      <th>discourse_type</th>\n",
              "      <th>in_reply_to</th>\n",
              "      <th>predict</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>photography</td>\n",
              "      <td>Is it against the law to stop on the side of a...</td>\n",
              "      <td>https://www.reddit.com/r/photography/comments/...</td>\n",
              "      <td>t3_1ds5ds</td>\n",
              "      <td>sobeisforlovers</td>\n",
              "      <td>Edit: I'm in the Oklahoma City and Tulsa Oklah...</td>\n",
              "      <td>none</td>\n",
              "      <td>0</td>\n",
              "      <td>question</td>\n",
              "      <td></td>\n",
              "      <td>announcement</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>photography</td>\n",
              "      <td>Is it against the law to stop on the side of a...</td>\n",
              "      <td>https://www.reddit.com/r/photography/comments/...</td>\n",
              "      <td>t1_c9tbz9b</td>\n",
              "      <td>KevinAndEarth</td>\n",
              "      <td>are you in the USA?  i would say that unless t...</td>\n",
              "      <td>t3_1ds5ds</td>\n",
              "      <td>1</td>\n",
              "      <td>question</td>\n",
              "      <td>t3_1ds5ds</td>\n",
              "      <td>disagreement</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>photography</td>\n",
              "      <td>Is it against the law to stop on the side of a...</td>\n",
              "      <td>https://www.reddit.com/r/photography/comments/...</td>\n",
              "      <td>t1_c9tcqh8</td>\n",
              "      <td></td>\n",
              "      <td>[deleted]</td>\n",
              "      <td>t1_c9tbz9b</td>\n",
              "      <td>2</td>\n",
              "      <td>answer</td>\n",
              "      <td>t1_c9tbz9b</td>\n",
              "      <td>announcement</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>photography</td>\n",
              "      <td>Is it against the law to stop on the side of a...</td>\n",
              "      <td>https://www.reddit.com/r/photography/comments/...</td>\n",
              "      <td>t1_c9thky3</td>\n",
              "      <td>EnglishTraitor</td>\n",
              "      <td>Great info! It shows that stopping on a highwa...</td>\n",
              "      <td>t1_c9tcqh8</td>\n",
              "      <td>3</td>\n",
              "      <td>appreciation</td>\n",
              "      <td>t1_c9tcqh8</td>\n",
              "      <td>answer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>photography</td>\n",
              "      <td>Is it against the law to stop on the side of a...</td>\n",
              "      <td>https://www.reddit.com/r/photography/comments/...</td>\n",
              "      <td>t1_c9twaqh</td>\n",
              "      <td>pixelmonger</td>\n",
              "      <td>Lately, it's been my experience that taking ne...</td>\n",
              "      <td></td>\n",
              "      <td>2</td>\n",
              "      <td>elaboration</td>\n",
              "      <td>t1_c9tbz9b</td>\n",
              "      <td>question</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>photography</td>\n",
              "      <td>Is it against the law to stop on the side of a...</td>\n",
              "      <td>https://www.reddit.com/r/photography/comments/...</td>\n",
              "      <td>t1_c9tg275</td>\n",
              "      <td>RKIvey</td>\n",
              "      <td>Speaking as a LEO, if you're not blocking traf...</td>\n",
              "      <td>t3_1ds5ds</td>\n",
              "      <td>1</td>\n",
              "      <td>answer</td>\n",
              "      <td>t3_1ds5ds</td>\n",
              "      <td>announcement</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>photography</td>\n",
              "      <td>Is it against the law to stop on the side of a...</td>\n",
              "      <td>https://www.reddit.com/r/photography/comments/...</td>\n",
              "      <td>t1_c9tbwts</td>\n",
              "      <td>m1ss1ontomars2k4</td>\n",
              "      <td>I'd assume it is, given that it's supposed to ...</td>\n",
              "      <td>t3_1ds5ds</td>\n",
              "      <td>1</td>\n",
              "      <td>answer</td>\n",
              "      <td>t3_1ds5ds</td>\n",
              "      <td>answer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>photography</td>\n",
              "      <td>Is it against the law to stop on the side of a...</td>\n",
              "      <td>https://www.reddit.com/r/photography/comments/...</td>\n",
              "      <td>t1_c9tgn0x</td>\n",
              "      <td>projecthouse</td>\n",
              "      <td>At least in the US, highway is a very broad te...</td>\n",
              "      <td>t3_1ds5ds</td>\n",
              "      <td>1</td>\n",
              "      <td>answer</td>\n",
              "      <td>t3_1ds5ds</td>\n",
              "      <td>answer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>photography</td>\n",
              "      <td>Is it against the law to stop on the side of a...</td>\n",
              "      <td>https://www.reddit.com/r/photography/comments/...</td>\n",
              "      <td>t1_c9thtlv</td>\n",
              "      <td>texasphotog</td>\n",
              "      <td>I would be worried more about the danger assoc...</td>\n",
              "      <td>t3_1ds5ds</td>\n",
              "      <td>1</td>\n",
              "      <td>answer</td>\n",
              "      <td>t3_1ds5ds</td>\n",
              "      <td>answer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>photography</td>\n",
              "      <td>Is it against the law to stop on the side of a...</td>\n",
              "      <td>https://www.reddit.com/r/photography/comments/...</td>\n",
              "      <td>t1_c9tboi2</td>\n",
              "      <td>Maxion</td>\n",
              "      <td>Yes, and no and in some places there probably ...</td>\n",
              "      <td>t3_1ds5ds</td>\n",
              "      <td>1</td>\n",
              "      <td>answer</td>\n",
              "      <td>t3_1ds5ds</td>\n",
              "      <td>answer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>photography</td>\n",
              "      <td>Is it against the law to stop on the side of a...</td>\n",
              "      <td>https://www.reddit.com/r/photography/comments/...</td>\n",
              "      <td>t1_c9tdysp</td>\n",
              "      <td>Maxion</td>\n",
              "      <td>*woosh*\\n\\nWith the OP not saying where he is ...</td>\n",
              "      <td>t1_c9tdwa9</td>\n",
              "      <td>3</td>\n",
              "      <td>elaboration</td>\n",
              "      <td>t1_c9tdwa9</td>\n",
              "      <td>question</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>photography</td>\n",
              "      <td>Is it against the law to stop on the side of a...</td>\n",
              "      <td>https://www.reddit.com/r/photography/comments/...</td>\n",
              "      <td>t1_c9tefwu</td>\n",
              "      <td>kickstand</td>\n",
              "      <td>Where you live, it is.</td>\n",
              "      <td>t3_1ds5ds</td>\n",
              "      <td>1</td>\n",
              "      <td>answer</td>\n",
              "      <td>t3_1ds5ds</td>\n",
              "      <td>question</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>twitch</td>\n",
              "      <td>Help! Can't have a stable stream on Twitch!</td>\n",
              "      <td>https://www.reddit.com/r/Twitch/comments/3zqn2...</td>\n",
              "      <td>t3_3zqn2h</td>\n",
              "      <td>CypherRS</td>\n",
              "      <td>I have 20mb upload speed but when I try to str...</td>\n",
              "      <td>none</td>\n",
              "      <td>0</td>\n",
              "      <td>question</td>\n",
              "      <td></td>\n",
              "      <td>answer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>twitch</td>\n",
              "      <td>Help! Can't have a stable stream on Twitch!</td>\n",
              "      <td>https://www.reddit.com/r/Twitch/comments/3zqn2...</td>\n",
              "      <td>t1_cyo9jnz</td>\n",
              "      <td>erusch18</td>\n",
              "      <td>What program are you using to broadcast? Try a...</td>\n",
              "      <td>t3_3zqn2h</td>\n",
              "      <td>1</td>\n",
              "      <td>question</td>\n",
              "      <td>t3_3zqn2h</td>\n",
              "      <td>answer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>twitch</td>\n",
              "      <td>Help! Can't have a stable stream on Twitch!</td>\n",
              "      <td>https://www.reddit.com/r/Twitch/comments/3zqn2...</td>\n",
              "      <td>t1_cyoec0s</td>\n",
              "      <td>CypherRS</td>\n",
              "      <td>I've used OBS, OBS MP and XSplit (Free). All w...</td>\n",
              "      <td>t1_cyo9jnz</td>\n",
              "      <td>2</td>\n",
              "      <td>answer</td>\n",
              "      <td>t1_cyo9jnz</td>\n",
              "      <td>answer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>twitch</td>\n",
              "      <td>Help! Can't have a stable stream on Twitch!</td>\n",
              "      <td>https://www.reddit.com/r/Twitch/comments/3zqn2...</td>\n",
              "      <td>t1_cyrgl47</td>\n",
              "      <td>CypherRS</td>\n",
              "      <td>Lol. Thanks a bunch. I've been contacting them...</td>\n",
              "      <td>t1_cypkz1k</td>\n",
              "      <td>4</td>\n",
              "      <td>appreciation</td>\n",
              "      <td>t1_cypkz1k</td>\n",
              "      <td>answer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>whatsthisplant</td>\n",
              "      <td>We got this for my grandma for Mother's Day, a...</td>\n",
              "      <td>https://www.reddit.com/r/whatsthisplant/commen...</td>\n",
              "      <td>t3_29iuc0</td>\n",
              "      <td>nattylog</td>\n",
              "      <td></td>\n",
              "      <td>none</td>\n",
              "      <td>0</td>\n",
              "      <td>question</td>\n",
              "      <td></td>\n",
              "      <td>elaboration</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>whatsthisplant</td>\n",
              "      <td>We got this for my grandma for Mother's Day, a...</td>\n",
              "      <td>https://www.reddit.com/r/whatsthisplant/commen...</td>\n",
              "      <td>t1_cilcnto</td>\n",
              "      <td>I_Hate_Armageddon</td>\n",
              "      <td>Mandevilla Vine http://en.wikipedia.org/wiki/M...</td>\n",
              "      <td>t3_29iuc0</td>\n",
              "      <td>1</td>\n",
              "      <td>answer</td>\n",
              "      <td>t3_29iuc0</td>\n",
              "      <td>elaboration</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>whatsthisplant</td>\n",
              "      <td>We got this for my grandma for Mother's Day, a...</td>\n",
              "      <td>https://www.reddit.com/r/whatsthisplant/commen...</td>\n",
              "      <td>t1_cilco1s</td>\n",
              "      <td>autowikibot</td>\n",
              "      <td>#####&amp;#009;\\n\\n######&amp;#009;\\n\\n####&amp;#009;\\n [*...</td>\n",
              "      <td>t1_cilcnto</td>\n",
              "      <td>2</td>\n",
              "      <td>elaboration</td>\n",
              "      <td>t1_cilcnto</td>\n",
              "      <td>elaboration</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>whatsthisplant</td>\n",
              "      <td>We got this for my grandma for Mother's Day, a...</td>\n",
              "      <td>https://www.reddit.com/r/whatsthisplant/commen...</td>\n",
              "      <td>t1_cilfeh3</td>\n",
              "      <td>nattylog</td>\n",
              "      <td>thanks so much!</td>\n",
              "      <td>t1_cilco1s</td>\n",
              "      <td>2</td>\n",
              "      <td>appreciation</td>\n",
              "      <td>t1_cilcnto</td>\n",
              "      <td>appreciation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>whatsthisplant</td>\n",
              "      <td>We got this for my grandma for Mother's Day, a...</td>\n",
              "      <td>https://www.reddit.com/r/whatsthisplant/commen...</td>\n",
              "      <td>t1_cilm80g</td>\n",
              "      <td>Orichalcon</td>\n",
              "      <td>Don't let it get sopping wet. Mandevillas have...</td>\n",
              "      <td>t3_29iuc0</td>\n",
              "      <td>1</td>\n",
              "      <td>answer</td>\n",
              "      <td>t3_29iuc0</td>\n",
              "      <td>elaboration</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>whatsthisplant</td>\n",
              "      <td>We got this for my grandma for Mother's Day, a...</td>\n",
              "      <td>https://www.reddit.com/r/whatsthisplant/commen...</td>\n",
              "      <td>t1_ciowx0a</td>\n",
              "      <td>nattylog</td>\n",
              "      <td>thanks for all the tips! I'll let my grandma know</td>\n",
              "      <td>t1_cilm80g</td>\n",
              "      <td>2</td>\n",
              "      <td>appreciation</td>\n",
              "      <td>t1_cilm80g</td>\n",
              "      <td>appreciation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>smashbros</td>\n",
              "      <td>Doof is the slickest</td>\n",
              "      <td>https://www.reddit.com/r/smashbros/comments/2n...</td>\n",
              "      <td>t3_2n8y2m</td>\n",
              "      <td>ivyburst</td>\n",
              "      <td>http://gfycat.com/SeparateDescriptiveFrenchbul...</td>\n",
              "      <td>none</td>\n",
              "      <td>0</td>\n",
              "      <td>humor</td>\n",
              "      <td></td>\n",
              "      <td>answer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>smashbros</td>\n",
              "      <td>Doof is the slickest</td>\n",
              "      <td>https://www.reddit.com/r/smashbros/comments/2n...</td>\n",
              "      <td>t1_cmbj3jn</td>\n",
              "      <td>arcticfire1</td>\n",
              "      <td>BASED DOOF</td>\n",
              "      <td>t3_2n8y2m</td>\n",
              "      <td>1</td>\n",
              "      <td>humor</td>\n",
              "      <td>t3_2n8y2m</td>\n",
              "      <td>answer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>nexus6p</td>\n",
              "      <td>PSA: Verizon Travel Pass not compatible with N...</td>\n",
              "      <td>https://www.reddit.com/r/Nexus6P/comments/3xrg...</td>\n",
              "      <td>t3_3xrgxo</td>\n",
              "      <td></td>\n",
              "      <td>So yeah. I was going to go with Project Fi. Th...</td>\n",
              "      <td>none</td>\n",
              "      <td>0</td>\n",
              "      <td>question</td>\n",
              "      <td></td>\n",
              "      <td>elaboration</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>nexus6p</td>\n",
              "      <td>PSA: Verizon Travel Pass not compatible with N...</td>\n",
              "      <td>https://www.reddit.com/r/Nexus6P/comments/3xrg...</td>\n",
              "      <td>t1_cy7cln5</td>\n",
              "      <td>hogBelly</td>\n",
              "      <td>If you call customer service they can add at. ...</td>\n",
              "      <td>t3_3xrgxo</td>\n",
              "      <td>1</td>\n",
              "      <td>answer</td>\n",
              "      <td>t3_3xrgxo</td>\n",
              "      <td>elaboration</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>nexus6p</td>\n",
              "      <td>PSA: Verizon Travel Pass not compatible with N...</td>\n",
              "      <td>https://www.reddit.com/r/Nexus6P/comments/3xrg...</td>\n",
              "      <td>t1_cy7cqud</td>\n",
              "      <td></td>\n",
              "      <td>Maybe I need to call again but I called  and s...</td>\n",
              "      <td>t1_cy7cln5</td>\n",
              "      <td>2</td>\n",
              "      <td>elaboration</td>\n",
              "      <td>t1_cy7cln5</td>\n",
              "      <td>elaboration</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>nexus6p</td>\n",
              "      <td>PSA: Verizon Travel Pass not compatible with N...</td>\n",
              "      <td>https://www.reddit.com/r/Nexus6P/comments/3xrg...</td>\n",
              "      <td>t1_cy7gf54</td>\n",
              "      <td>WinDroidGuy</td>\n",
              "      <td>Which plan are you guys on?</td>\n",
              "      <td>t1_cy7cln5</td>\n",
              "      <td>2</td>\n",
              "      <td>question</td>\n",
              "      <td>t1_cy7cln5</td>\n",
              "      <td>question</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>nexus6p</td>\n",
              "      <td>PSA: Verizon Travel Pass not compatible with N...</td>\n",
              "      <td>https://www.reddit.com/r/Nexus6P/comments/3xrg...</td>\n",
              "      <td>t1_cy7rz1p</td>\n",
              "      <td>hogBelly</td>\n",
              "      <td>I just joined vz, so I an on the 6gb Verizon p...</td>\n",
              "      <td>t1_cy7gf54</td>\n",
              "      <td>3</td>\n",
              "      <td>answer</td>\n",
              "      <td>t1_cy7gf54</td>\n",
              "      <td>appreciation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>nexus6p</td>\n",
              "      <td>PSA: Verizon Travel Pass not compatible with N...</td>\n",
              "      <td>https://www.reddit.com/r/Nexus6P/comments/3xrg...</td>\n",
              "      <td>t1_cy74tm2</td>\n",
              "      <td>WinDroidGuy</td>\n",
              "      <td>What's the reason it's not compatible?!?</td>\n",
              "      <td>t3_3xrgxo</td>\n",
              "      <td>1</td>\n",
              "      <td>question</td>\n",
              "      <td>t3_3xrgxo</td>\n",
              "      <td>elaboration</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22066</th>\n",
              "      <td>childfree</td>\n",
              "      <td>Children dislike = racism?</td>\n",
              "      <td>https://www.reddit.com/r/childfree/comments/49...</td>\n",
              "      <td>t1_d0u3mx5</td>\n",
              "      <td>SecondHandToy</td>\n",
              "      <td>&gt; Children... are not a belief\\n\\nBut Parentho...</td>\n",
              "      <td>t1_d0tvdmh</td>\n",
              "      <td>2</td>\n",
              "      <td>elaboration</td>\n",
              "      <td>t1_d0tvdmh</td>\n",
              "      <td>disagreement</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22067</th>\n",
              "      <td>childfree</td>\n",
              "      <td>Children dislike = racism?</td>\n",
              "      <td>https://www.reddit.com/r/childfree/comments/49...</td>\n",
              "      <td>t1_d0u00n9</td>\n",
              "      <td>Mia_la</td>\n",
              "      <td>Childhood is TEMPORARY. If anything he's the r...</td>\n",
              "      <td>t3_49q6tf</td>\n",
              "      <td>1</td>\n",
              "      <td>answer</td>\n",
              "      <td>t3_49q6tf</td>\n",
              "      <td>announcement</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22068</th>\n",
              "      <td>childfree</td>\n",
              "      <td>Children dislike = racism?</td>\n",
              "      <td>https://www.reddit.com/r/childfree/comments/49...</td>\n",
              "      <td>t1_d0tve8s</td>\n",
              "      <td>shitlady-gamer</td>\n",
              "      <td>No, disliking children is not similar to being...</td>\n",
              "      <td>t3_49q6tf</td>\n",
              "      <td>1</td>\n",
              "      <td>answer</td>\n",
              "      <td>t3_49q6tf</td>\n",
              "      <td>elaboration</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22069</th>\n",
              "      <td>childfree</td>\n",
              "      <td>Children dislike = racism?</td>\n",
              "      <td>https://www.reddit.com/r/childfree/comments/49...</td>\n",
              "      <td>t1_d0tvio1</td>\n",
              "      <td>astorwyn</td>\n",
              "      <td>I agree with you on all counts, maybe I should...</td>\n",
              "      <td>t1_d0tve8s</td>\n",
              "      <td>2</td>\n",
              "      <td>agreement</td>\n",
              "      <td>t1_d0tve8s</td>\n",
              "      <td>agreement</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22070</th>\n",
              "      <td>childfree</td>\n",
              "      <td>Children dislike = racism?</td>\n",
              "      <td>https://www.reddit.com/r/childfree/comments/49...</td>\n",
              "      <td>t1_d0twvyq</td>\n",
              "      <td>shitlady-gamer</td>\n",
              "      <td>Well that makes this transgression worse - in ...</td>\n",
              "      <td>t1_d0tvio1</td>\n",
              "      <td>3</td>\n",
              "      <td>elaboration</td>\n",
              "      <td>t1_d0tvio1</td>\n",
              "      <td>disagreement</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22071</th>\n",
              "      <td>childfree</td>\n",
              "      <td>Children dislike = racism?</td>\n",
              "      <td>https://www.reddit.com/r/childfree/comments/49...</td>\n",
              "      <td>t1_d0txehy</td>\n",
              "      <td>astorwyn</td>\n",
              "      <td>&gt; Friends don't manipulate other friends. Frie...</td>\n",
              "      <td>t1_d0twvyq</td>\n",
              "      <td>4</td>\n",
              "      <td>agreement</td>\n",
              "      <td>t1_d0twvyq</td>\n",
              "      <td>agreement</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22072</th>\n",
              "      <td>childfree</td>\n",
              "      <td>Children dislike = racism?</td>\n",
              "      <td>https://www.reddit.com/r/childfree/comments/49...</td>\n",
              "      <td>t1_d0tzcli</td>\n",
              "      <td>cailian13</td>\n",
              "      <td>Nope. Racist implies you only like one type of...</td>\n",
              "      <td>t3_49q6tf</td>\n",
              "      <td>1</td>\n",
              "      <td>answer</td>\n",
              "      <td>t3_49q6tf</td>\n",
              "      <td>elaboration</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22073</th>\n",
              "      <td>childfree</td>\n",
              "      <td>Children dislike = racism?</td>\n",
              "      <td>https://www.reddit.com/r/childfree/comments/49...</td>\n",
              "      <td>t1_d0tvoup</td>\n",
              "      <td>nicalyssa</td>\n",
              "      <td>Racism is definied as holding one race as more...</td>\n",
              "      <td>t3_49q6tf</td>\n",
              "      <td>1</td>\n",
              "      <td>answer</td>\n",
              "      <td>t3_49q6tf</td>\n",
              "      <td>disagreement</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22074</th>\n",
              "      <td>childfree</td>\n",
              "      <td>Children dislike = racism?</td>\n",
              "      <td>https://www.reddit.com/r/childfree/comments/49...</td>\n",
              "      <td>t1_d0tyis2</td>\n",
              "      <td>PrincessPeach817</td>\n",
              "      <td>George is dead fucking wrong.\\n\\nIt would be b...</td>\n",
              "      <td>t3_49q6tf</td>\n",
              "      <td>1</td>\n",
              "      <td>answer</td>\n",
              "      <td>t3_49q6tf</td>\n",
              "      <td>disagreement</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22075</th>\n",
              "      <td>childfree</td>\n",
              "      <td>Children dislike = racism?</td>\n",
              "      <td>https://www.reddit.com/r/childfree/comments/49...</td>\n",
              "      <td>t1_d0tv5sk</td>\n",
              "      <td>casualLogic</td>\n",
              "      <td>That would be like me calling OP a racist beca...</td>\n",
              "      <td>t3_49q6tf</td>\n",
              "      <td>1</td>\n",
              "      <td>answer</td>\n",
              "      <td>t3_49q6tf</td>\n",
              "      <td>elaboration</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22077</th>\n",
              "      <td>childfree</td>\n",
              "      <td>Children dislike = racism?</td>\n",
              "      <td>https://www.reddit.com/r/childfree/comments/49...</td>\n",
              "      <td>t1_d0u4yyg</td>\n",
              "      <td></td>\n",
              "      <td>nah, but way to go george for belittling real ...</td>\n",
              "      <td>t3_49q6tf</td>\n",
              "      <td>1</td>\n",
              "      <td>answer</td>\n",
              "      <td>t3_49q6tf</td>\n",
              "      <td>disagreement</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22078</th>\n",
              "      <td>childfree</td>\n",
              "      <td>Children dislike = racism?</td>\n",
              "      <td>https://www.reddit.com/r/childfree/comments/49...</td>\n",
              "      <td>t1_d0uc0ne</td>\n",
              "      <td>rpmbear</td>\n",
              "      <td>While it isn't racism (and neither is dislikin...</td>\n",
              "      <td>t3_49q6tf</td>\n",
              "      <td>1</td>\n",
              "      <td>answer</td>\n",
              "      <td>t3_49q6tf</td>\n",
              "      <td>elaboration</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22079</th>\n",
              "      <td>childfree</td>\n",
              "      <td>Children dislike = racism?</td>\n",
              "      <td>https://www.reddit.com/r/childfree/comments/49...</td>\n",
              "      <td>t1_d0udytk</td>\n",
              "      <td>Cmrade_Dorian</td>\n",
              "      <td>Well I mean I can see how he thinks that. Call...</td>\n",
              "      <td>t3_49q6tf</td>\n",
              "      <td>1</td>\n",
              "      <td>answer</td>\n",
              "      <td>t3_49q6tf</td>\n",
              "      <td>elaboration</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22080</th>\n",
              "      <td>childfree</td>\n",
              "      <td>Children dislike = racism?</td>\n",
              "      <td>https://www.reddit.com/r/childfree/comments/49...</td>\n",
              "      <td>t1_d0u5cwf</td>\n",
              "      <td>Amblonyx</td>\n",
              "      <td>This guy sounds \"way over the top\" himself. Th...</td>\n",
              "      <td>t3_49q6tf</td>\n",
              "      <td>1</td>\n",
              "      <td>answer</td>\n",
              "      <td>t3_49q6tf</td>\n",
              "      <td>elaboration</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22081</th>\n",
              "      <td>childfree</td>\n",
              "      <td>Children dislike = racism?</td>\n",
              "      <td>https://www.reddit.com/r/childfree/comments/49...</td>\n",
              "      <td>t1_d0u1na3</td>\n",
              "      <td>apocalippo</td>\n",
              "      <td>I don't think it is racist, no, but technicall...</td>\n",
              "      <td>t3_49q6tf</td>\n",
              "      <td>1</td>\n",
              "      <td>answer</td>\n",
              "      <td>t3_49q6tf</td>\n",
              "      <td>disagreement</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22082</th>\n",
              "      <td>childfree</td>\n",
              "      <td>Children dislike = racism?</td>\n",
              "      <td>https://www.reddit.com/r/childfree/comments/49...</td>\n",
              "      <td>t1_d0u599t</td>\n",
              "      <td>will999909</td>\n",
              "      <td>I mean, there was really no point of you sayin...</td>\n",
              "      <td>t3_49q6tf</td>\n",
              "      <td>1</td>\n",
              "      <td>answer</td>\n",
              "      <td>t3_49q6tf</td>\n",
              "      <td>elaboration</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22083</th>\n",
              "      <td>childfree</td>\n",
              "      <td>Children dislike = racism?</td>\n",
              "      <td>https://www.reddit.com/r/childfree/comments/49...</td>\n",
              "      <td>t1_d0uo6dl</td>\n",
              "      <td>Luminaria19</td>\n",
              "      <td>At *worst,* it's mildly ageist.</td>\n",
              "      <td>t3_49q6tf</td>\n",
              "      <td>1</td>\n",
              "      <td>answer</td>\n",
              "      <td>t3_49q6tf</td>\n",
              "      <td>agreement</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22084</th>\n",
              "      <td>picrequests</td>\n",
              "      <td>Photo Restoration request complete-cannot find...</td>\n",
              "      <td>https://www.reddit.com/r/picrequests/comments/...</td>\n",
              "      <td>t3_163amt</td>\n",
              "      <td>Getusom32</td>\n",
              "      <td>Someone posted this image for a sister I belie...</td>\n",
              "      <td>none</td>\n",
              "      <td>0</td>\n",
              "      <td>question</td>\n",
              "      <td></td>\n",
              "      <td>answer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22085</th>\n",
              "      <td>picrequests</td>\n",
              "      <td>Photo Restoration request complete-cannot find...</td>\n",
              "      <td>https://www.reddit.com/r/picrequests/comments/...</td>\n",
              "      <td>t1_c7sd1mi</td>\n",
              "      <td>Zmodem</td>\n",
              "      <td>Wow, just WOW! This is one of the most amazing...</td>\n",
              "      <td>t3_163amt</td>\n",
              "      <td>1</td>\n",
              "      <td>appreciation</td>\n",
              "      <td>t3_163amt</td>\n",
              "      <td>appreciation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22086</th>\n",
              "      <td>picrequests</td>\n",
              "      <td>Photo Restoration request complete-cannot find...</td>\n",
              "      <td>https://www.reddit.com/r/picrequests/comments/...</td>\n",
              "      <td>t1_c7sfq0g</td>\n",
              "      <td>Getusom32</td>\n",
              "      <td>Thanks very much everyone. Hope OP makes it ba...</td>\n",
              "      <td>t1_c7sd1mi</td>\n",
              "      <td>1</td>\n",
              "      <td>appreciation</td>\n",
              "      <td>t3_163amt</td>\n",
              "      <td>appreciation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22087</th>\n",
              "      <td>picrequests</td>\n",
              "      <td>Photo Restoration request complete-cannot find...</td>\n",
              "      <td>https://www.reddit.com/r/picrequests/comments/...</td>\n",
              "      <td>t1_c7sc5pq</td>\n",
              "      <td>NoGodisReal</td>\n",
              "      <td>Let the upvotes begin!!</td>\n",
              "      <td>t3_163amt</td>\n",
              "      <td>1</td>\n",
              "      <td>appreciation</td>\n",
              "      <td>t3_163amt</td>\n",
              "      <td>announcement</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22088</th>\n",
              "      <td>picrequests</td>\n",
              "      <td>Photo Restoration request complete-cannot find...</td>\n",
              "      <td>https://www.reddit.com/r/picrequests/comments/...</td>\n",
              "      <td>t1_c7sc66g</td>\n",
              "      <td>Th3MadCreator</td>\n",
              "      <td>Great job! I just searched the entire subreddi...</td>\n",
              "      <td>t3_163amt</td>\n",
              "      <td>1</td>\n",
              "      <td>appreciation</td>\n",
              "      <td>t3_163amt</td>\n",
              "      <td>appreciation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22089</th>\n",
              "      <td>picrequests</td>\n",
              "      <td>Photo Restoration request complete-cannot find...</td>\n",
              "      <td>https://www.reddit.com/r/picrequests/comments/...</td>\n",
              "      <td>t1_c7sc464</td>\n",
              "      <td>JCSUPERMAN</td>\n",
              "      <td>Awesome Job!!</td>\n",
              "      <td>t3_163amt</td>\n",
              "      <td>1</td>\n",
              "      <td>appreciation</td>\n",
              "      <td>t3_163amt</td>\n",
              "      <td>appreciation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22090</th>\n",
              "      <td>picrequests</td>\n",
              "      <td>Photo Restoration request complete-cannot find...</td>\n",
              "      <td>https://www.reddit.com/r/picrequests/comments/...</td>\n",
              "      <td>t1_c7sd0hx</td>\n",
              "      <td>WastedBabiez</td>\n",
              "      <td>Bro, that's amazing!! I hope OP finds it too.</td>\n",
              "      <td>t3_163amt</td>\n",
              "      <td>1</td>\n",
              "      <td>appreciation</td>\n",
              "      <td>t3_163amt</td>\n",
              "      <td>appreciation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22091</th>\n",
              "      <td>picrequests</td>\n",
              "      <td>Photo Restoration request complete-cannot find...</td>\n",
              "      <td>https://www.reddit.com/r/picrequests/comments/...</td>\n",
              "      <td>t1_c7sd74s</td>\n",
              "      <td>InitechSecurity</td>\n",
              "      <td>Wow, awesome!</td>\n",
              "      <td>t3_163amt</td>\n",
              "      <td>1</td>\n",
              "      <td>appreciation</td>\n",
              "      <td>t3_163amt</td>\n",
              "      <td>appreciation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22092</th>\n",
              "      <td>picrequests</td>\n",
              "      <td>Photo Restoration request complete-cannot find...</td>\n",
              "      <td>https://www.reddit.com/r/picrequests/comments/...</td>\n",
              "      <td>t1_c7sk2dc</td>\n",
              "      <td>starlinguk</td>\n",
              "      <td>I find the habit of removing requests on here ...</td>\n",
              "      <td>t3_163amt</td>\n",
              "      <td>1</td>\n",
              "      <td>appreciation</td>\n",
              "      <td>t3_163amt</td>\n",
              "      <td>agreement</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22093</th>\n",
              "      <td>picrequests</td>\n",
              "      <td>Photo Restoration request complete-cannot find...</td>\n",
              "      <td>https://www.reddit.com/r/picrequests/comments/...</td>\n",
              "      <td>t1_c7spqmg</td>\n",
              "      <td>Getusom32</td>\n",
              "      <td>Perhaps I should have made a comment that \"wor...</td>\n",
              "      <td>t1_c7sk2dc</td>\n",
              "      <td>2</td>\n",
              "      <td>elaboration</td>\n",
              "      <td>t1_c7sk2dc</td>\n",
              "      <td>announcement</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22094</th>\n",
              "      <td>picrequests</td>\n",
              "      <td>Photo Restoration request complete-cannot find...</td>\n",
              "      <td>https://www.reddit.com/r/picrequests/comments/...</td>\n",
              "      <td>t1_c7supz7</td>\n",
              "      <td>girlparachronism</td>\n",
              "      <td>This is absolutely amazing! Well done!</td>\n",
              "      <td>t3_163amt</td>\n",
              "      <td>1</td>\n",
              "      <td>appreciation</td>\n",
              "      <td>t3_163amt</td>\n",
              "      <td>appreciation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22095</th>\n",
              "      <td>picrequests</td>\n",
              "      <td>Photo Restoration request complete-cannot find...</td>\n",
              "      <td>https://www.reddit.com/r/picrequests/comments/...</td>\n",
              "      <td>t1_c7t4941</td>\n",
              "      <td>Getusom32</td>\n",
              "      <td>You are very kind. Thanks very much. Appears O...</td>\n",
              "      <td>t1_c7supz7</td>\n",
              "      <td>2</td>\n",
              "      <td>appreciation</td>\n",
              "      <td>t1_c7supz7</td>\n",
              "      <td>appreciation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22096</th>\n",
              "      <td>picrequests</td>\n",
              "      <td>Photo Restoration request complete-cannot find...</td>\n",
              "      <td>https://www.reddit.com/r/picrequests/comments/...</td>\n",
              "      <td>t1_c7scjcn</td>\n",
              "      <td>DumpsterJuice</td>\n",
              "      <td>Bro, do you even shop?\\n\\nAwesome job man! Kee...</td>\n",
              "      <td>t3_163amt</td>\n",
              "      <td>1</td>\n",
              "      <td>appreciation</td>\n",
              "      <td>t3_163amt</td>\n",
              "      <td>appreciation</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>19812 rows × 11 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            subreddit                                              title  \\\n",
              "0         photography  Is it against the law to stop on the side of a...   \n",
              "1         photography  Is it against the law to stop on the side of a...   \n",
              "2         photography  Is it against the law to stop on the side of a...   \n",
              "3         photography  Is it against the law to stop on the side of a...   \n",
              "4         photography  Is it against the law to stop on the side of a...   \n",
              "5         photography  Is it against the law to stop on the side of a...   \n",
              "6         photography  Is it against the law to stop on the side of a...   \n",
              "7         photography  Is it against the law to stop on the side of a...   \n",
              "8         photography  Is it against the law to stop on the side of a...   \n",
              "9         photography  Is it against the law to stop on the side of a...   \n",
              "11        photography  Is it against the law to stop on the side of a...   \n",
              "12        photography  Is it against the law to stop on the side of a...   \n",
              "13             twitch        Help! Can't have a stable stream on Twitch!   \n",
              "14             twitch        Help! Can't have a stable stream on Twitch!   \n",
              "15             twitch        Help! Can't have a stable stream on Twitch!   \n",
              "16             twitch        Help! Can't have a stable stream on Twitch!   \n",
              "17     whatsthisplant  We got this for my grandma for Mother's Day, a...   \n",
              "18     whatsthisplant  We got this for my grandma for Mother's Day, a...   \n",
              "19     whatsthisplant  We got this for my grandma for Mother's Day, a...   \n",
              "20     whatsthisplant  We got this for my grandma for Mother's Day, a...   \n",
              "21     whatsthisplant  We got this for my grandma for Mother's Day, a...   \n",
              "22     whatsthisplant  We got this for my grandma for Mother's Day, a...   \n",
              "23          smashbros                               Doof is the slickest   \n",
              "25          smashbros                               Doof is the slickest   \n",
              "27            nexus6p  PSA: Verizon Travel Pass not compatible with N...   \n",
              "28            nexus6p  PSA: Verizon Travel Pass not compatible with N...   \n",
              "29            nexus6p  PSA: Verizon Travel Pass not compatible with N...   \n",
              "30            nexus6p  PSA: Verizon Travel Pass not compatible with N...   \n",
              "31            nexus6p  PSA: Verizon Travel Pass not compatible with N...   \n",
              "32            nexus6p  PSA: Verizon Travel Pass not compatible with N...   \n",
              "...               ...                                                ...   \n",
              "22066       childfree                         Children dislike = racism?   \n",
              "22067       childfree                         Children dislike = racism?   \n",
              "22068       childfree                         Children dislike = racism?   \n",
              "22069       childfree                         Children dislike = racism?   \n",
              "22070       childfree                         Children dislike = racism?   \n",
              "22071       childfree                         Children dislike = racism?   \n",
              "22072       childfree                         Children dislike = racism?   \n",
              "22073       childfree                         Children dislike = racism?   \n",
              "22074       childfree                         Children dislike = racism?   \n",
              "22075       childfree                         Children dislike = racism?   \n",
              "22077       childfree                         Children dislike = racism?   \n",
              "22078       childfree                         Children dislike = racism?   \n",
              "22079       childfree                         Children dislike = racism?   \n",
              "22080       childfree                         Children dislike = racism?   \n",
              "22081       childfree                         Children dislike = racism?   \n",
              "22082       childfree                         Children dislike = racism?   \n",
              "22083       childfree                         Children dislike = racism?   \n",
              "22084     picrequests  Photo Restoration request complete-cannot find...   \n",
              "22085     picrequests  Photo Restoration request complete-cannot find...   \n",
              "22086     picrequests  Photo Restoration request complete-cannot find...   \n",
              "22087     picrequests  Photo Restoration request complete-cannot find...   \n",
              "22088     picrequests  Photo Restoration request complete-cannot find...   \n",
              "22089     picrequests  Photo Restoration request complete-cannot find...   \n",
              "22090     picrequests  Photo Restoration request complete-cannot find...   \n",
              "22091     picrequests  Photo Restoration request complete-cannot find...   \n",
              "22092     picrequests  Photo Restoration request complete-cannot find...   \n",
              "22093     picrequests  Photo Restoration request complete-cannot find...   \n",
              "22094     picrequests  Photo Restoration request complete-cannot find...   \n",
              "22095     picrequests  Photo Restoration request complete-cannot find...   \n",
              "22096     picrequests  Photo Restoration request complete-cannot find...   \n",
              "\n",
              "                                                     url          id  \\\n",
              "0      https://www.reddit.com/r/photography/comments/...   t3_1ds5ds   \n",
              "1      https://www.reddit.com/r/photography/comments/...  t1_c9tbz9b   \n",
              "2      https://www.reddit.com/r/photography/comments/...  t1_c9tcqh8   \n",
              "3      https://www.reddit.com/r/photography/comments/...  t1_c9thky3   \n",
              "4      https://www.reddit.com/r/photography/comments/...  t1_c9twaqh   \n",
              "5      https://www.reddit.com/r/photography/comments/...  t1_c9tg275   \n",
              "6      https://www.reddit.com/r/photography/comments/...  t1_c9tbwts   \n",
              "7      https://www.reddit.com/r/photography/comments/...  t1_c9tgn0x   \n",
              "8      https://www.reddit.com/r/photography/comments/...  t1_c9thtlv   \n",
              "9      https://www.reddit.com/r/photography/comments/...  t1_c9tboi2   \n",
              "11     https://www.reddit.com/r/photography/comments/...  t1_c9tdysp   \n",
              "12     https://www.reddit.com/r/photography/comments/...  t1_c9tefwu   \n",
              "13     https://www.reddit.com/r/Twitch/comments/3zqn2...   t3_3zqn2h   \n",
              "14     https://www.reddit.com/r/Twitch/comments/3zqn2...  t1_cyo9jnz   \n",
              "15     https://www.reddit.com/r/Twitch/comments/3zqn2...  t1_cyoec0s   \n",
              "16     https://www.reddit.com/r/Twitch/comments/3zqn2...  t1_cyrgl47   \n",
              "17     https://www.reddit.com/r/whatsthisplant/commen...   t3_29iuc0   \n",
              "18     https://www.reddit.com/r/whatsthisplant/commen...  t1_cilcnto   \n",
              "19     https://www.reddit.com/r/whatsthisplant/commen...  t1_cilco1s   \n",
              "20     https://www.reddit.com/r/whatsthisplant/commen...  t1_cilfeh3   \n",
              "21     https://www.reddit.com/r/whatsthisplant/commen...  t1_cilm80g   \n",
              "22     https://www.reddit.com/r/whatsthisplant/commen...  t1_ciowx0a   \n",
              "23     https://www.reddit.com/r/smashbros/comments/2n...   t3_2n8y2m   \n",
              "25     https://www.reddit.com/r/smashbros/comments/2n...  t1_cmbj3jn   \n",
              "27     https://www.reddit.com/r/Nexus6P/comments/3xrg...   t3_3xrgxo   \n",
              "28     https://www.reddit.com/r/Nexus6P/comments/3xrg...  t1_cy7cln5   \n",
              "29     https://www.reddit.com/r/Nexus6P/comments/3xrg...  t1_cy7cqud   \n",
              "30     https://www.reddit.com/r/Nexus6P/comments/3xrg...  t1_cy7gf54   \n",
              "31     https://www.reddit.com/r/Nexus6P/comments/3xrg...  t1_cy7rz1p   \n",
              "32     https://www.reddit.com/r/Nexus6P/comments/3xrg...  t1_cy74tm2   \n",
              "...                                                  ...         ...   \n",
              "22066  https://www.reddit.com/r/childfree/comments/49...  t1_d0u3mx5   \n",
              "22067  https://www.reddit.com/r/childfree/comments/49...  t1_d0u00n9   \n",
              "22068  https://www.reddit.com/r/childfree/comments/49...  t1_d0tve8s   \n",
              "22069  https://www.reddit.com/r/childfree/comments/49...  t1_d0tvio1   \n",
              "22070  https://www.reddit.com/r/childfree/comments/49...  t1_d0twvyq   \n",
              "22071  https://www.reddit.com/r/childfree/comments/49...  t1_d0txehy   \n",
              "22072  https://www.reddit.com/r/childfree/comments/49...  t1_d0tzcli   \n",
              "22073  https://www.reddit.com/r/childfree/comments/49...  t1_d0tvoup   \n",
              "22074  https://www.reddit.com/r/childfree/comments/49...  t1_d0tyis2   \n",
              "22075  https://www.reddit.com/r/childfree/comments/49...  t1_d0tv5sk   \n",
              "22077  https://www.reddit.com/r/childfree/comments/49...  t1_d0u4yyg   \n",
              "22078  https://www.reddit.com/r/childfree/comments/49...  t1_d0uc0ne   \n",
              "22079  https://www.reddit.com/r/childfree/comments/49...  t1_d0udytk   \n",
              "22080  https://www.reddit.com/r/childfree/comments/49...  t1_d0u5cwf   \n",
              "22081  https://www.reddit.com/r/childfree/comments/49...  t1_d0u1na3   \n",
              "22082  https://www.reddit.com/r/childfree/comments/49...  t1_d0u599t   \n",
              "22083  https://www.reddit.com/r/childfree/comments/49...  t1_d0uo6dl   \n",
              "22084  https://www.reddit.com/r/picrequests/comments/...   t3_163amt   \n",
              "22085  https://www.reddit.com/r/picrequests/comments/...  t1_c7sd1mi   \n",
              "22086  https://www.reddit.com/r/picrequests/comments/...  t1_c7sfq0g   \n",
              "22087  https://www.reddit.com/r/picrequests/comments/...  t1_c7sc5pq   \n",
              "22088  https://www.reddit.com/r/picrequests/comments/...  t1_c7sc66g   \n",
              "22089  https://www.reddit.com/r/picrequests/comments/...  t1_c7sc464   \n",
              "22090  https://www.reddit.com/r/picrequests/comments/...  t1_c7sd0hx   \n",
              "22091  https://www.reddit.com/r/picrequests/comments/...  t1_c7sd74s   \n",
              "22092  https://www.reddit.com/r/picrequests/comments/...  t1_c7sk2dc   \n",
              "22093  https://www.reddit.com/r/picrequests/comments/...  t1_c7spqmg   \n",
              "22094  https://www.reddit.com/r/picrequests/comments/...  t1_c7supz7   \n",
              "22095  https://www.reddit.com/r/picrequests/comments/...  t1_c7t4941   \n",
              "22096  https://www.reddit.com/r/picrequests/comments/...  t1_c7scjcn   \n",
              "\n",
              "                  author                                               body  \\\n",
              "0        sobeisforlovers  Edit: I'm in the Oklahoma City and Tulsa Oklah...   \n",
              "1          KevinAndEarth  are you in the USA?  i would say that unless t...   \n",
              "2                                                                 [deleted]   \n",
              "3         EnglishTraitor  Great info! It shows that stopping on a highwa...   \n",
              "4            pixelmonger  Lately, it's been my experience that taking ne...   \n",
              "5                 RKIvey  Speaking as a LEO, if you're not blocking traf...   \n",
              "6       m1ss1ontomars2k4  I'd assume it is, given that it's supposed to ...   \n",
              "7           projecthouse  At least in the US, highway is a very broad te...   \n",
              "8            texasphotog  I would be worried more about the danger assoc...   \n",
              "9                 Maxion  Yes, and no and in some places there probably ...   \n",
              "11                Maxion  *woosh*\\n\\nWith the OP not saying where he is ...   \n",
              "12             kickstand                             Where you live, it is.   \n",
              "13              CypherRS  I have 20mb upload speed but when I try to str...   \n",
              "14              erusch18  What program are you using to broadcast? Try a...   \n",
              "15              CypherRS  I've used OBS, OBS MP and XSplit (Free). All w...   \n",
              "16              CypherRS  Lol. Thanks a bunch. I've been contacting them...   \n",
              "17              nattylog                                                      \n",
              "18     I_Hate_Armageddon  Mandevilla Vine http://en.wikipedia.org/wiki/M...   \n",
              "19           autowikibot  #####&#009;\\n\\n######&#009;\\n\\n####&#009;\\n [*...   \n",
              "20              nattylog                                    thanks so much!   \n",
              "21            Orichalcon  Don't let it get sopping wet. Mandevillas have...   \n",
              "22              nattylog  thanks for all the tips! I'll let my grandma know   \n",
              "23              ivyburst  http://gfycat.com/SeparateDescriptiveFrenchbul...   \n",
              "25           arcticfire1                                         BASED DOOF   \n",
              "27                        So yeah. I was going to go with Project Fi. Th...   \n",
              "28              hogBelly  If you call customer service they can add at. ...   \n",
              "29                        Maybe I need to call again but I called  and s...   \n",
              "30           WinDroidGuy                       Which plan are you guys on?    \n",
              "31              hogBelly  I just joined vz, so I an on the 6gb Verizon p...   \n",
              "32           WinDroidGuy           What's the reason it's not compatible?!?   \n",
              "...                  ...                                                ...   \n",
              "22066      SecondHandToy  > Children... are not a belief\\n\\nBut Parentho...   \n",
              "22067             Mia_la  Childhood is TEMPORARY. If anything he's the r...   \n",
              "22068     shitlady-gamer  No, disliking children is not similar to being...   \n",
              "22069           astorwyn  I agree with you on all counts, maybe I should...   \n",
              "22070     shitlady-gamer  Well that makes this transgression worse - in ...   \n",
              "22071           astorwyn  > Friends don't manipulate other friends. Frie...   \n",
              "22072          cailian13  Nope. Racist implies you only like one type of...   \n",
              "22073          nicalyssa  Racism is definied as holding one race as more...   \n",
              "22074   PrincessPeach817  George is dead fucking wrong.\\n\\nIt would be b...   \n",
              "22075        casualLogic  That would be like me calling OP a racist beca...   \n",
              "22077                     nah, but way to go george for belittling real ...   \n",
              "22078            rpmbear  While it isn't racism (and neither is dislikin...   \n",
              "22079      Cmrade_Dorian  Well I mean I can see how he thinks that. Call...   \n",
              "22080           Amblonyx  This guy sounds \"way over the top\" himself. Th...   \n",
              "22081         apocalippo  I don't think it is racist, no, but technicall...   \n",
              "22082         will999909  I mean, there was really no point of you sayin...   \n",
              "22083        Luminaria19                    At *worst,* it's mildly ageist.   \n",
              "22084          Getusom32  Someone posted this image for a sister I belie...   \n",
              "22085             Zmodem  Wow, just WOW! This is one of the most amazing...   \n",
              "22086          Getusom32  Thanks very much everyone. Hope OP makes it ba...   \n",
              "22087        NoGodisReal                           Let the upvotes begin!!    \n",
              "22088      Th3MadCreator  Great job! I just searched the entire subreddi...   \n",
              "22089         JCSUPERMAN                                      Awesome Job!!   \n",
              "22090       WastedBabiez     Bro, that's amazing!! I hope OP finds it too.    \n",
              "22091    InitechSecurity                                      Wow, awesome!   \n",
              "22092         starlinguk  I find the habit of removing requests on here ...   \n",
              "22093          Getusom32  Perhaps I should have made a comment that \"wor...   \n",
              "22094   girlparachronism             This is absolutely amazing! Well done!   \n",
              "22095          Getusom32  You are very kind. Thanks very much. Appears O...   \n",
              "22096      DumpsterJuice  Bro, do you even shop?\\n\\nAwesome job man! Kee...   \n",
              "\n",
              "      majority_link  post_depth discourse_type in_reply_to       predict  \n",
              "0              none           0       question              announcement  \n",
              "1         t3_1ds5ds           1       question   t3_1ds5ds  disagreement  \n",
              "2        t1_c9tbz9b           2         answer  t1_c9tbz9b  announcement  \n",
              "3        t1_c9tcqh8           3   appreciation  t1_c9tcqh8        answer  \n",
              "4                             2    elaboration  t1_c9tbz9b      question  \n",
              "5         t3_1ds5ds           1         answer   t3_1ds5ds  announcement  \n",
              "6         t3_1ds5ds           1         answer   t3_1ds5ds        answer  \n",
              "7         t3_1ds5ds           1         answer   t3_1ds5ds        answer  \n",
              "8         t3_1ds5ds           1         answer   t3_1ds5ds        answer  \n",
              "9         t3_1ds5ds           1         answer   t3_1ds5ds        answer  \n",
              "11       t1_c9tdwa9           3    elaboration  t1_c9tdwa9      question  \n",
              "12        t3_1ds5ds           1         answer   t3_1ds5ds      question  \n",
              "13             none           0       question                    answer  \n",
              "14        t3_3zqn2h           1       question   t3_3zqn2h        answer  \n",
              "15       t1_cyo9jnz           2         answer  t1_cyo9jnz        answer  \n",
              "16       t1_cypkz1k           4   appreciation  t1_cypkz1k        answer  \n",
              "17             none           0       question               elaboration  \n",
              "18        t3_29iuc0           1         answer   t3_29iuc0   elaboration  \n",
              "19       t1_cilcnto           2    elaboration  t1_cilcnto   elaboration  \n",
              "20       t1_cilco1s           2   appreciation  t1_cilcnto  appreciation  \n",
              "21        t3_29iuc0           1         answer   t3_29iuc0   elaboration  \n",
              "22       t1_cilm80g           2   appreciation  t1_cilm80g  appreciation  \n",
              "23             none           0          humor                    answer  \n",
              "25        t3_2n8y2m           1          humor   t3_2n8y2m        answer  \n",
              "27             none           0       question               elaboration  \n",
              "28        t3_3xrgxo           1         answer   t3_3xrgxo   elaboration  \n",
              "29       t1_cy7cln5           2    elaboration  t1_cy7cln5   elaboration  \n",
              "30       t1_cy7cln5           2       question  t1_cy7cln5      question  \n",
              "31       t1_cy7gf54           3         answer  t1_cy7gf54  appreciation  \n",
              "32        t3_3xrgxo           1       question   t3_3xrgxo   elaboration  \n",
              "...             ...         ...            ...         ...           ...  \n",
              "22066    t1_d0tvdmh           2    elaboration  t1_d0tvdmh  disagreement  \n",
              "22067     t3_49q6tf           1         answer   t3_49q6tf  announcement  \n",
              "22068     t3_49q6tf           1         answer   t3_49q6tf   elaboration  \n",
              "22069    t1_d0tve8s           2      agreement  t1_d0tve8s     agreement  \n",
              "22070    t1_d0tvio1           3    elaboration  t1_d0tvio1  disagreement  \n",
              "22071    t1_d0twvyq           4      agreement  t1_d0twvyq     agreement  \n",
              "22072     t3_49q6tf           1         answer   t3_49q6tf   elaboration  \n",
              "22073     t3_49q6tf           1         answer   t3_49q6tf  disagreement  \n",
              "22074     t3_49q6tf           1         answer   t3_49q6tf  disagreement  \n",
              "22075     t3_49q6tf           1         answer   t3_49q6tf   elaboration  \n",
              "22077     t3_49q6tf           1         answer   t3_49q6tf  disagreement  \n",
              "22078     t3_49q6tf           1         answer   t3_49q6tf   elaboration  \n",
              "22079     t3_49q6tf           1         answer   t3_49q6tf   elaboration  \n",
              "22080     t3_49q6tf           1         answer   t3_49q6tf   elaboration  \n",
              "22081     t3_49q6tf           1         answer   t3_49q6tf  disagreement  \n",
              "22082     t3_49q6tf           1         answer   t3_49q6tf   elaboration  \n",
              "22083     t3_49q6tf           1         answer   t3_49q6tf     agreement  \n",
              "22084          none           0       question                    answer  \n",
              "22085     t3_163amt           1   appreciation   t3_163amt  appreciation  \n",
              "22086    t1_c7sd1mi           1   appreciation   t3_163amt  appreciation  \n",
              "22087     t3_163amt           1   appreciation   t3_163amt  announcement  \n",
              "22088     t3_163amt           1   appreciation   t3_163amt  appreciation  \n",
              "22089     t3_163amt           1   appreciation   t3_163amt  appreciation  \n",
              "22090     t3_163amt           1   appreciation   t3_163amt  appreciation  \n",
              "22091     t3_163amt           1   appreciation   t3_163amt  appreciation  \n",
              "22092     t3_163amt           1   appreciation   t3_163amt     agreement  \n",
              "22093    t1_c7sk2dc           2    elaboration  t1_c7sk2dc  announcement  \n",
              "22094     t3_163amt           1   appreciation   t3_163amt  appreciation  \n",
              "22095    t1_c7supz7           2   appreciation  t1_c7supz7  appreciation  \n",
              "22096     t3_163amt           1   appreciation   t3_163amt  appreciation  \n",
              "\n",
              "[19812 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "6EAHz5IzD0eM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Q4"
      ]
    },
    {
      "metadata": {
        "id": "i76TFQXli55I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### preparation"
      ]
    },
    {
      "metadata": {
        "id": "GLsSCJBFckx0",
        "colab_type": "code",
        "outputId": "6f001cd7-ea67-45da-ee7d-1bce3e611500",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "cell_type": "code",
      "source": [
        "discourse_train = \"coursework_discourse_train.json\"\n",
        "discourse_test = \"coursework_discourse_test.json\"\n",
        "  \n",
        "!gsutil cp gs://textasdata/coursework/coursework_discourse_train.json $discourse_train  \n",
        "!gsutil cp gs://textasdata/coursework/coursework_discourse_test.json  $discourse_test"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://textasdata/coursework/coursework_discourse_train.json...\n",
            "- [1 files][ 60.2 MiB/ 60.2 MiB]                                                \n",
            "Operation completed over 1 objects/60.2 MiB.                                     \n",
            "Copying gs://textasdata/coursework/coursework_discourse_test.json...\n",
            "- [1 files][ 15.1 MiB/ 15.1 MiB]                                                \n",
            "Operation completed over 1 objects/15.1 MiB.                                     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CL0D4S2lD16G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "def load_posts(file):\n",
        "  # A temporary variable to store the list of post content.\n",
        "  posts_tmp = list()\n",
        "\n",
        "  with open(file) as jsonfile:\n",
        "    for i, line in enumerate(jsonfile):\n",
        "     # if (i > 2): break\n",
        "      thread = json.loads(line)\n",
        "      initial_author = \"\"\n",
        "      num_of_com = 0\n",
        "      for post in thread['posts']:\n",
        "        num_of_com+=1\n",
        "        if(post.get('in_reply_to', \"\") == \"\"):\n",
        "          initial_author = post.get('author', \"\")\n",
        "      for post in thread['posts']:\n",
        "        if(post.get('author', \"\") == initial_author):\n",
        "          is_initial_author = True\n",
        "        else:\n",
        "          is_initial_author = False\n",
        "        \n",
        "        posts_tmp.append((thread['subreddit'], \n",
        "                          str(post.get('post_depth', 0)),\n",
        "                          str(num_of_com),\n",
        "                          str(is_initial_author),\n",
        "                          thread['title'],\n",
        "                          post.get('body', \"\"),\n",
        "                          \n",
        "                          thread['url'],\n",
        "                        post['id'], post.get('author', \"\"),  post.get(\"majority_link\", \"\"), \n",
        "                         post.get('majority_type', \"\"), # discourse type label \n",
        "                        post.get('in_reply_to', \"\") ))\n",
        "\n",
        "# Create the posts data frame.  \n",
        "  labels = ['subreddit', 'post_depth', 'num_of_com', 'is_initial_author',\n",
        "            \n",
        "            'title', 'body', 'url', 'id', 'author', 'majority_link', \n",
        "           'discourse_type', 'in_reply_to']\n",
        "  return pd.DataFrame(posts_tmp, columns=labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uSaCiieCejg2",
        "colab_type": "code",
        "outputId": "3c12cbe3-8eb7-4c67-b4ea-c30e1c5789df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        }
      },
      "cell_type": "code",
      "source": [
        "# Load the medium english model. \n",
        "import spacy\n",
        "!python -m spacy download en_core_web_md\n",
        "\n",
        "nlp = spacy.load('en_core_web_md', disable=['ner'])\n",
        "nlp.remove_pipe('tagger')\n",
        "nlp.remove_pipe('parser')\n",
        "\n",
        "#Tokenize\n",
        "def spacy_tokenize(string):\n",
        "  tokens = list()\n",
        "  doc = nlp(string)\n",
        "  for token in doc:\n",
        "    tokens.append(token)\n",
        "  return tokens\n",
        "\n",
        "#Normalize\n",
        "def normalize(tokens):\n",
        "  normalized_tokens = list()\n",
        "  for token in tokens:\n",
        "    normalized = token.text.lower().strip()\n",
        "    if ((token.is_alpha or token.is_digit)):\n",
        "      normalized_tokens.append(normalized)\n",
        "  return normalized_tokens\n",
        "  return normalized_tokens\n",
        "\n",
        "#Tokenize and normalize\n",
        "def tokenize_normalize(string):\n",
        "  return normalize(spacy_tokenize(string))\n",
        "\n",
        "# Select feature\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class ItemSelector(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"For data grouped by feature, select subset of data at a provided key.    \"\"\"\n",
        "\n",
        "    def __init__(self, key):\n",
        "        self.key = key\n",
        "\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, data_dict):\n",
        "        return data_dict[self.key]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_md==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.0.0/en_core_web_md-2.0.0.tar.gz#egg=en_core_web_md==2.0.0 in /usr/local/lib/python3.6/dist-packages (2.0.0)\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/en_core_web_md -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/en_core_web_md\n",
            "\n",
            "    You can now load the model via spacy.load('en_core_web_md')\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "u6s6EIQ_pk8h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_posts = load_posts(discourse_train)\n",
        "# Filter out empty labels\n",
        "train_posts = train_posts[train_posts['discourse_type'] != \"\"]\n",
        "\n",
        "test_posts = load_posts(discourse_test)\n",
        "# Filter out empty labels\n",
        "test_posts = test_posts[test_posts['discourse_type'] != \"\"]\n",
        "\n",
        "train_labels = train_posts['discourse_type']\n",
        "test_labels = test_posts['discourse_type']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "meNkL5ISGXLq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Evaluation: accuracy, macro-averaged precision, recall, and F1 measures\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import fbeta_score\n",
        "\n",
        "def evaluation_summary(description, predictions, true_labels):\n",
        "  print(\"Evaluation for: \" + description)\n",
        "  precision = precision_score(predictions, true_labels,average='macro')\n",
        "  recall = recall_score(predictions, true_labels, average='macro')\n",
        "  accuracy = accuracy_score(predictions, true_labels)\n",
        "  f1 = fbeta_score(predictions, true_labels, 1, average='macro') \n",
        "  print(\"Classifier '%s' has Acc=%0.3f P=%0.3f R=%0.3f F1=%0.3f\" % (description,accuracy,precision,recall,f1))\n",
        "  print(classification_report(predictions, true_labels, digits=3))\n",
        "  print('\\nConfusion matrix:\\n',confusion_matrix(true_labels, predictions))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Doc_8Q1peus3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Feature 1"
      ]
    },
    {
      "metadata": {
        "id": "FgjAf-_gdMfs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Additional feature 1: subreddit\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Use FeatureUnion to combine the features from post body, thread title and post author id\n",
        "prediction_pipeline_a = Pipeline([\n",
        "        ('union', FeatureUnion(\n",
        "          transformer_list=[\n",
        "            ('body', Pipeline([\n",
        "              ('selector', ItemSelector(key='body')),\n",
        "              ('tf-idf', TfidfVectorizer(tokenizer=tokenize_normalize)), \n",
        "              ])),\n",
        "            ('title', Pipeline([\n",
        "              ('selector', ItemSelector(key='title')),\n",
        "              ('tf-idf', TfidfVectorizer(tokenizer=tokenize_normalize)), \n",
        "              ])),\n",
        "            ('subreddit', Pipeline([\n",
        "              ('selector', ItemSelector(key='subreddit')),\n",
        "              ('tf-idf', TfidfVectorizer(tokenizer=tokenize_normalize)), \n",
        "              ])),\n",
        "        ])\n",
        "        )\n",
        "    ])\n",
        "\n",
        "# Define the features\n",
        "X_train = prediction_pipeline_a.fit_transform(train_posts)\n",
        "X_test = prediction_pipeline_a.transform(test_posts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ICBrzMfGd_LN",
        "colab_type": "code",
        "outputId": "8261dc10-a1d1-4eac-eeae-7a4795653238",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 603
        }
      },
      "cell_type": "code",
      "source": [
        "# LogisticRegression,TF-IDF\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "logreg = LogisticRegression(C=10000, solver='saga', multi_class='auto')\n",
        "logreg_model = logreg.fit(X_train, train_posts['discourse_type'])\n",
        "predictions = logreg_model.predict(X_test)  \n",
        "evaluation_summary(\"LogisticRegression  with TF-IDF\", predictions, test_posts['discourse_type'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Evaluation for: LogisticRegression  with TF-IDF\n",
            "Classifier 'LogisticRegression  with TF-IDF' has Acc=0.417 P=0.242 R=0.283 F1=0.253\n",
            "                  precision    recall  f1-score   support\n",
            "\n",
            "       agreement      0.216     0.284     0.245       723\n",
            "    announcement      0.115     0.126     0.120       334\n",
            "          answer      0.562     0.519     0.540      8608\n",
            "    appreciation      0.538     0.607     0.570      1523\n",
            "    disagreement      0.064     0.126     0.085       326\n",
            "     elaboration      0.267     0.266     0.267      3642\n",
            "           humor      0.050     0.139     0.074       165\n",
            "negativereaction      0.088     0.231     0.127       117\n",
            "           other      0.074     0.163     0.102       172\n",
            "        question      0.449     0.366     0.403      4202\n",
            "\n",
            "       micro avg      0.417     0.417     0.417     19812\n",
            "       macro avg      0.242     0.283     0.253     19812\n",
            "    weighted avg      0.442     0.417     0.428     19812\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[ 205   11  340   58   16  168    7    9    6  131]\n",
            " [   5   42  151   10    3   81    2    1    9   61]\n",
            " [ 206   89 4465  202  123 1411   42   23   29 1350]\n",
            " [  50   28  285  925   14  177   14   11   30  186]\n",
            " [  22    5  290    7   41  156    0    6    1  113]\n",
            " [ 132   65 1556  148   59  970   33   19   36  612]\n",
            " [   8   16  191   35    9   85   23    4    8   78]\n",
            " [   9   11   93   23    7   67    8   27    8   54]\n",
            " [  11    9  114   34    5   76   13    7   28   79]\n",
            " [  75   58 1123   81   49  451   23   10   17 1538]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jjlHoZ4le0DD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Feature 2"
      ]
    },
    {
      "metadata": {
        "id": "RYhwf6cgdMp1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Additional feature 2: post_depth\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Use FeatureUnion to combine the features from post body, thread title and post author id\n",
        "prediction_pipeline_a = Pipeline([\n",
        "        ('union', FeatureUnion(\n",
        "          transformer_list=[\n",
        "            ('body', Pipeline([\n",
        "              ('selector', ItemSelector(key='body')),\n",
        "              ('tf-idf', TfidfVectorizer(tokenizer=tokenize_normalize)), \n",
        "              ])),\n",
        "            ('title', Pipeline([\n",
        "              ('selector', ItemSelector(key='title')),\n",
        "              ('tf-idf', TfidfVectorizer(tokenizer=tokenize_normalize)), \n",
        "              ])),\n",
        "            ('post_depth', Pipeline([\n",
        "              ('selector', ItemSelector(key='post_depth')),\n",
        "              ('tf-idf', TfidfVectorizer(tokenizer=tokenize_normalize)), \n",
        "              ])),\n",
        "        ])\n",
        "        )\n",
        "    ])\n",
        "\n",
        "# Define the features\n",
        "X_train = prediction_pipeline_a.fit_transform(train_posts)\n",
        "X_test = prediction_pipeline_a.transform(test_posts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nhq4B6R7es4w",
        "colab_type": "code",
        "outputId": "559b4804-1b39-49ce-b132-cab6f012ddb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 603
        }
      },
      "cell_type": "code",
      "source": [
        "# LogisticRegression,TF-IDF\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "logreg = LogisticRegression(C=10000, solver='saga', multi_class='auto')\n",
        "logreg_model = logreg.fit(X_train, train_posts['discourse_type'])\n",
        "predictions = logreg_model.predict(X_test)  \n",
        "evaluation_summary(\"LogisticRegression  with TF-IDF\", predictions, test_posts['discourse_type'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Evaluation for: LogisticRegression  with TF-IDF\n",
            "Classifier 'LogisticRegression  with TF-IDF' has Acc=0.534 P=0.335 R=0.374 F1=0.348\n",
            "                  precision    recall  f1-score   support\n",
            "\n",
            "       agreement      0.274     0.331     0.300       788\n",
            "    announcement      0.510     0.581     0.543       320\n",
            "          answer      0.708     0.634     0.669      8871\n",
            "    appreciation      0.548     0.605     0.575      1558\n",
            "    disagreement      0.117     0.198     0.147       379\n",
            "     elaboration      0.393     0.373     0.383      3817\n",
            "           humor      0.053     0.125     0.074       192\n",
            "negativereaction      0.088     0.199     0.122       136\n",
            "           other      0.082     0.136     0.103       228\n",
            "        question      0.578     0.562     0.570      3523\n",
            "\n",
            "       micro avg      0.534     0.534     0.534     19812\n",
            "       macro avg      0.335     0.374     0.348     19812\n",
            "    weighted avg      0.562     0.534     0.546     19812\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[ 261    0  262   77   36  220   10    6   11   68]\n",
            " [   0  186    0    0    0    0    0    0    2  177]\n",
            " [ 134    0 5622  200   91 1167   46   29   44  607]\n",
            " [  63    0  320  942   15  194   15   10   44  117]\n",
            " [  38    0  209   13   75  224    4    9    3   66]\n",
            " [ 199    0 1296  164   99 1425   39   23   43  342]\n",
            " [  17    7  179   33   10  105   24    6   13   63]\n",
            " [   8    0  110   27    7   72    9   27    5   42]\n",
            " [  12    9  133   35    3   67   14   10   31   62]\n",
            " [  56  118  740   67   43  343   31   16   32 1979]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7NrCl6DHe6gm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Feature 3"
      ]
    },
    {
      "metadata": {
        "id": "mG4Q-q7xdMzN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Additional Feature 3: num_of_com\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Use FeatureUnion to combine the features from post body, thread title and post author id\n",
        "prediction_pipeline_a = Pipeline([\n",
        "        ('union', FeatureUnion(\n",
        "          transformer_list=[\n",
        "            ('body', Pipeline([\n",
        "              ('selector', ItemSelector(key='body')),\n",
        "              ('tf-idf', TfidfVectorizer(tokenizer=tokenize_normalize)), \n",
        "              ])),\n",
        "            ('title', Pipeline([\n",
        "              ('selector', ItemSelector(key='title')),\n",
        "              ('tf-idf', TfidfVectorizer(tokenizer=tokenize_normalize)), \n",
        "              ])),\n",
        "            ('num_of_com', Pipeline([\n",
        "              ('selector', ItemSelector(key='num_of_com')),\n",
        "              ('tf-idf', TfidfVectorizer(tokenizer=tokenize_normalize)), \n",
        "              ])),\n",
        "        ])\n",
        "        )\n",
        "    ])\n",
        "\n",
        "# Define the features\n",
        "X_train = prediction_pipeline_a.fit_transform(train_posts)\n",
        "X_test = prediction_pipeline_a.transform(test_posts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4lP1lVVrgf4H",
        "colab_type": "code",
        "outputId": "99a6296d-4b7c-41df-f16c-1eaa0211b0d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 603
        }
      },
      "cell_type": "code",
      "source": [
        "# LogisticRegression,TF-IDF\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "logreg = LogisticRegression(C=10000, solver='saga', multi_class='auto')\n",
        "logreg_model = logreg.fit(X_train, train_posts['discourse_type'])\n",
        "predictions = logreg_model.predict(X_test)  \n",
        "evaluation_summary(\"LogisticRegression  with TF-IDF\", predictions, test_posts['discourse_type'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Evaluation for: LogisticRegression  with TF-IDF\n",
            "Classifier 'LogisticRegression  with TF-IDF' has Acc=0.412 P=0.244 R=0.281 F1=0.255\n",
            "                  precision    recall  f1-score   support\n",
            "\n",
            "       agreement      0.224     0.282     0.250       755\n",
            "    announcement      0.107     0.137     0.120       285\n",
            "          answer      0.540     0.515     0.527      8320\n",
            "    appreciation      0.531     0.582     0.555      1568\n",
            "    disagreement      0.078     0.146     0.102       343\n",
            "     elaboration      0.304     0.268     0.285      4123\n",
            "           humor      0.050     0.141     0.074       163\n",
            "negativereaction      0.094     0.199     0.128       146\n",
            "           other      0.085     0.160     0.111       200\n",
            "        question      0.429     0.376     0.401      3909\n",
            "\n",
            "       micro avg      0.412     0.412     0.412     19812\n",
            "       macro avg      0.244     0.281     0.255     19812\n",
            "    weighted avg      0.430     0.412     0.419     19812\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[ 213   15  339   61   19  165    7    7    6  119]\n",
            " [   4   39  158    6    2   69    2    2    7   76]\n",
            " [ 215   76 4286  235  125 1648   44   34   42 1235]\n",
            " [  45   24  268  913    9  214   14   11   32  190]\n",
            " [  28    5  269    8   50  171    5    9    3   93]\n",
            " [ 135   54 1481  147   74 1105   25   22   40  547]\n",
            " [  14   12  179   35    6  102   23    7   11   68]\n",
            " [  12    6  103   25    7   62    5   29    5   53]\n",
            " [  11    7  122   42    4   80   12    8   32   58]\n",
            " [  78   47 1115   96   47  507   26   17   22 1470]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nyGrYk4Lg-C4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Feature 4"
      ]
    },
    {
      "metadata": {
        "id": "Ap4Lic0vdM7n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Additional feature 4: is_initial_author\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Use FeatureUnion to combine the features from post body, thread title and post author id\n",
        "prediction_pipeline_a = Pipeline([\n",
        "        ('union', FeatureUnion(\n",
        "          transformer_list=[\n",
        "            ('body', Pipeline([\n",
        "              ('selector', ItemSelector(key='body')),\n",
        "              ('tf-idf', TfidfVectorizer(tokenizer=tokenize_normalize)), \n",
        "              ])),\n",
        "            ('title', Pipeline([\n",
        "              ('selector', ItemSelector(key='title')),\n",
        "              ('tf-idf', TfidfVectorizer(tokenizer=tokenize_normalize)),\n",
        "              ])),\n",
        "           \n",
        "            ('is_initial_author', Pipeline([\n",
        "              ('selector', ItemSelector(key='is_initial_author')),\n",
        "              ('tf-idf', TfidfVectorizer(tokenizer=tokenize_normalize)), \n",
        "              ])),\n",
        "            \n",
        "        ])\n",
        "        )\n",
        "    ])\n",
        "\n",
        "# Define the features\n",
        "X_train = prediction_pipeline_a.fit_transform(train_posts)\n",
        "X_test = prediction_pipeline_a.transform(test_posts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U8ETe94IhKP_",
        "colab_type": "code",
        "outputId": "c3c48f7d-0d75-4cd9-fb0d-0d8a1fbe5016",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 603
        }
      },
      "cell_type": "code",
      "source": [
        "# LogisticRegression,TF-IDF\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "logreg = LogisticRegression(C=10000, solver='saga', multi_class='auto')\n",
        "logreg_model = logreg.fit(X_train, train_posts['discourse_type'])\n",
        "predictions = logreg_model.predict(X_test)  \n",
        "evaluation_summary(\"LogisticRegression  with TF-IDF\", predictions, test_posts['discourse_type'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Evaluation for: LogisticRegression  with TF-IDF\n",
            "Classifier 'LogisticRegression  with TF-IDF' has Acc=0.445 P=0.265 R=0.299 F1=0.275\n",
            "                  precision    recall  f1-score   support\n",
            "\n",
            "       agreement      0.223     0.304     0.257       698\n",
            "    announcement      0.247     0.265     0.256       339\n",
            "          answer      0.597     0.543     0.569      8735\n",
            "    appreciation      0.534     0.596     0.563      1543\n",
            "    disagreement      0.069     0.141     0.092       313\n",
            "     elaboration      0.279     0.270     0.274      3747\n",
            "           humor      0.033     0.089     0.048       169\n",
            "negativereaction      0.085     0.198     0.119       131\n",
            "           other      0.077     0.146     0.101       198\n",
            "        question      0.504     0.438     0.468      3939\n",
            "\n",
            "       micro avg      0.445     0.445     0.445     19812\n",
            "       macro avg      0.265     0.299     0.275     19812\n",
            "    weighted avg      0.473     0.445     0.457     19812\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[ 212   12  380   57   19  153    8    6    5   99]\n",
            " [   4   90   74   10    2   57    3    0    3  122]\n",
            " [ 200   52 4743  201  125 1507   51   35   37  989]\n",
            " [  44   15  293  919    8  179   18   12   33  199]\n",
            " [  27    1  312    9   44  147    1    7    3   90]\n",
            " [ 117   73 1599  144   58 1011   29   16   41  542]\n",
            " [  10    5  204   35    7   96   15    8   11   66]\n",
            " [  10    3  109   29    8   65   11   26    5   41]\n",
            " [  10   10  129   38    2   73   11    8   29   66]\n",
            " [  64   78  892  101   40  459   22   13   31 1725]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZBuwD_4fhPRv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Feature 5"
      ]
    },
    {
      "metadata": {
        "id": "OserzXj3dNOA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Additional feature 5: character ngram\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Use FeatureUnion to combine the features from post body, thread title and post author id\n",
        "prediction_pipeline_a = Pipeline([\n",
        "        ('union', FeatureUnion(\n",
        "          transformer_list=[\n",
        "            ('body', Pipeline([\n",
        "              ('selector', ItemSelector(key='body')),\n",
        "              ('tf-idf', TfidfVectorizer(tokenizer=tokenize_normalize)), \n",
        "              ])),\n",
        "            ('title', Pipeline([\n",
        "              ('selector', ItemSelector(key='title')),\n",
        "              ('tf-idf', TfidfVectorizer(tokenizer=tokenize_normalize)), \n",
        "              ])),\n",
        "            ('body2', Pipeline([\n",
        "              ('selector', ItemSelector(key='body')),\n",
        "              ('tf-idf', TfidfVectorizer(tokenizer=tokenize_normalize, ngram_range = (1,4))), \n",
        "              ])),\n",
        "           \n",
        "        ])\n",
        "        )\n",
        "    ])\n",
        "\n",
        "# Define the features\n",
        "X_train = prediction_pipeline_a.fit_transform(train_posts)\n",
        "X_test = prediction_pipeline_a.transform(test_posts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yT-LKNQhh-oQ",
        "colab_type": "code",
        "outputId": "f3e3d24b-e306-4140-996f-ac43dacafd77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 603
        }
      },
      "cell_type": "code",
      "source": [
        "# LogisticRegression,TF-IDF\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "logreg = LogisticRegression(C=10000, solver='saga', multi_class='auto')\n",
        "logreg_model = logreg.fit(X_train, train_posts['discourse_type'])\n",
        "predictions = logreg_model.predict(X_test)  \n",
        "evaluation_summary(\"LogisticRegression  with TF-IDF\", predictions, test_posts['discourse_type'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Evaluation for: LogisticRegression  with TF-IDF\n",
            "Classifier 'LogisticRegression  with TF-IDF' has Acc=0.503 P=0.291 R=0.363 F1=0.302\n",
            "                  precision    recall  f1-score   support\n",
            "\n",
            "       agreement      0.246     0.441     0.316       531\n",
            "    announcement      0.178     0.125     0.147       521\n",
            "          answer      0.694     0.551     0.614      9993\n",
            "    appreciation      0.599     0.706     0.648      1459\n",
            "    disagreement      0.066     0.235     0.102       179\n",
            "     elaboration      0.180     0.329     0.233      1986\n",
            "           humor      0.050     0.232     0.083        99\n",
            "negativereaction      0.094     0.299     0.144        97\n",
            "           other      0.125     0.224     0.160       210\n",
            "        question      0.682     0.493     0.572      4737\n",
            "\n",
            "       micro avg      0.503     0.503     0.503     19812\n",
            "       macro avg      0.291     0.363     0.302     19812\n",
            "    weighted avg      0.589     0.503     0.534     19812\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[ 234   16  407   52    5   97    6    7   12  115]\n",
            " [   3   65  169    4    2   45    2    0    6   69]\n",
            " [ 101  146 5508  144   60  694   18   16   34 1219]\n",
            " [  37   31  294 1030    2  125    9    5   30  157]\n",
            " [  11   11  384    5   42   85    0    5    4   94]\n",
            " [  84  123 2031   99   33  653   16   15   47  529]\n",
            " [  10   23  219   26    5   50   23   10   11   80]\n",
            " [   6    8  103   22    5   48    5   29    6   75]\n",
            " [  12   16  136   39    2   47    7    6   47   64]\n",
            " [  33   82  742   38   23  142   13    4   13 2335]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JuxOTnCZh_kc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Feature 6"
      ]
    },
    {
      "metadata": {
        "id": "SG3QCN0IHN-Z",
        "colab_type": "code",
        "outputId": "17ac6035-0e65-424f-eef0-dac5716aebd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        }
      },
      "cell_type": "code",
      "source": [
        "# Use the tokenizer to extract all tokens from the body of the posts.\n",
        "# Flatten the tokens in the post into a single list of all the tokens.\n",
        "!pip install --upgrade gensim\n",
        "import gensim\n",
        "import itertools\n",
        "all_tokens = []\n",
        "all_posts_tokenized = train_posts.body.apply(tokenize_normalize)\n",
        "all_tokens = list(itertools.chain.from_iterable(all_posts_tokenized))\n",
        "\n",
        "model = gensim.models.Word2Vec(all_posts_tokenized, size=50, window=5, \\\n",
        "                               min_count=5, sg=0, alpha=0.025, iter=10, batch_words=10000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gensim\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/b9/6c93685bed0026b6a1cce55ab173f6b617f6db0d1325d25489c2fd43e711/gensim-3.7.1-cp36-cp36m-manylinux1_x86_64.whl (24.2MB)\n",
            "\u001b[K    100% |████████████████████████████████| 24.2MB 1.7MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: smart-open>=1.7.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.8.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.14.6)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.11.0)\n",
            "Requirement already satisfied, skipping upgrade: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.7.0->gensim) (2.49.0)\n",
            "Requirement already satisfied, skipping upgrade: bz2file in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.7.0->gensim) (0.98)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.7.0->gensim) (2.18.4)\n",
            "Requirement already satisfied, skipping upgrade: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.7.0->gensim) (1.9.106)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.7.0->gensim) (2.6)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.7.0->gensim) (1.22)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.7.0->gensim) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.7.0->gensim) (2018.11.29)\n",
            "Requirement already satisfied, skipping upgrade: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.7.0->gensim) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: botocore<1.13.0,>=1.12.106 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.7.0->gensim) (1.12.106)\n",
            "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.7.0->gensim) (0.9.4)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.106->boto3->smart-open>=1.7.0->gensim) (2.5.3)\n",
            "Requirement already satisfied, skipping upgrade: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.106->boto3->smart-open>=1.7.0->gensim) (0.14)\n",
            "Installing collected packages: gensim\n",
            "  Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-3.7.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lGL4WU00eMMT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class AverageEmbeddingVectorizer(BaseEstimator, TransformerMixin\n",
        "                                ):\n",
        "    def __init__(self, embedding_model):\n",
        "        self.embedding = embedding_model\n",
        "        self.dimension = embedding_model.vector_size\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        return self\n",
        "      \n",
        "    def transform(self, X):  \n",
        "      # Skip OOV terms. Return 0 if no words are in the vocabulary.\n",
        "      #print (X)\n",
        "      return np.array([ \n",
        "          np.mean([self.embedding[token] for token in doc if token in self.embedding]\n",
        "                or [np.zeros(self.dimension)], axis=0)\n",
        "          for doc in X\n",
        "      ])\n",
        "    \n",
        "    \n",
        "w2v_vectorizer = AverageEmbeddingVectorizer(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_qzsO_IoIJ-e",
        "colab_type": "code",
        "outputId": "822387b1-0c0e-40e4-c00b-8aac11afee37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "def create_w2v(x):\n",
        "  doc = tokenize_normalize(x)\n",
        "  X = [doc]\n",
        "  w2c_vector = w2v_vectorizer.transform(X)\n",
        "  string = \"\"\n",
        "  for v in w2c_vector[0]:\n",
        "    string += str(v) + \" \"\n",
        "  return string\n",
        "  \n",
        "train_posts['w2v'] = train_posts.body.apply(create_w2v)\n",
        "test_posts['w2v'] = test_posts.body.apply(create_w2v)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "kScj0g1KQPzv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Additional feature 6: word2vector\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Use FeatureUnion to combine the features from post body, thread title and post author id\n",
        "prediction_pipeline_a = Pipeline([\n",
        "        ('union', FeatureUnion(\n",
        "          transformer_list=[\n",
        "            ('body', Pipeline([\n",
        "              ('selector', ItemSelector(key='body')),\n",
        "              ('tf-idf', TfidfVectorizer(tokenizer=tokenize_normalize)), \n",
        "              ])),\n",
        "            ('title', Pipeline([\n",
        "              ('selector', ItemSelector(key='title')),\n",
        "              ('tf-idf', TfidfVectorizer(tokenizer=tokenize_normalize)), \n",
        "              ])),\n",
        "            ('w2v', Pipeline([\n",
        "              ('selector', ItemSelector(key='w2v')),\n",
        "              ('tf-idf', TfidfVectorizer(tokenizer=spacy_tokenize)), \n",
        "              ])), \t\n",
        "        ])\n",
        "        )\n",
        "    ])\n",
        "\n",
        "# Define the features\n",
        "X_train = prediction_pipeline_a.fit_transform(train_posts)\n",
        "X_test = prediction_pipeline_a.transform(test_posts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WYCNahafSbc7",
        "colab_type": "code",
        "outputId": "4933cf2d-72b0-4127-f940-0973eaaa07f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 658
        }
      },
      "cell_type": "code",
      "source": [
        "# LogisticRegression,TF-IDF\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "logreg = LogisticRegression()\n",
        "logreg_model = logreg.fit(X_train, train_posts['discourse_type'])\n",
        "predictions = logreg_model.predict(X_test)  \n",
        "evaluation_summary(\"LogisticRegression  with TF-IDF\", predictions, test_posts['discourse_type'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Evaluation for: LogisticRegression  with TF-IDF\n",
            "Classifier 'LogisticRegression  with TF-IDF' has Acc=0.509 P=0.235 R=0.505 F1=0.248\n",
            "                  precision    recall  f1-score   support\n",
            "\n",
            "       agreement      0.183     0.563     0.276       309\n",
            "    announcement      0.008     1.000     0.016         3\n",
            "          answer      0.830     0.510     0.632     12919\n",
            "    appreciation      0.587     0.764     0.664      1321\n",
            "    disagreement      0.017     0.282     0.032        39\n",
            "     elaboration      0.185     0.304     0.230      2217\n",
            "           humor      0.007     0.214     0.013        14\n",
            "negativereaction      0.026     0.471     0.049        17\n",
            "           other      0.037     0.400     0.068        35\n",
            "        question      0.466     0.543     0.501      2938\n",
            "\n",
            "       micro avg      0.509     0.509     0.509     19812\n",
            "       macro avg      0.235     0.505     0.248     19812\n",
            "    weighted avg      0.673     0.509     0.561     19812\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[ 174    0  509   33    2  139    0    1    0   93]\n",
            " [   0    3  230   10    2   78    0    0    1   41]\n",
            " [  52    0 6591   85   11  648    3    0    4  546]\n",
            " [  14    0  429 1009    0  160    1    2    6   99]\n",
            " [   6    0  467    3   11   94    0    1    0   59]\n",
            " [  35    0 2488   62    6  673    0    0    8  358]\n",
            " [   3    0  304   23    1   70    3    1    1   51]\n",
            " [   3    0  174   15    1   59    0    8    0   47]\n",
            " [   5    0  230   24    1   49    2    2   14   49]\n",
            " [  17    0 1497   57    4  247    5    2    1 1595]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "k_35uBhkjzyH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Features combined"
      ]
    },
    {
      "metadata": {
        "id": "bkmN3Z_2jwA1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import FeatureUnion\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Use FeatureUnion to combine the features from post body, thread title and post author id\n",
        "prediction_pipeline_a = Pipeline([\n",
        "        ('union', FeatureUnion(\n",
        "          transformer_list=[\n",
        "            ('body', Pipeline([\n",
        "              ('selector', ItemSelector(key='body')),\n",
        "              ('tf-idf', TfidfVectorizer(tokenizer=tokenize_normalize)), \n",
        "              ])),\n",
        "            ('title', Pipeline([\n",
        "              ('selector', ItemSelector(key='title')),\n",
        "              ('tf-idf', TfidfVectorizer(tokenizer=tokenize_normalize)), \n",
        "              ])),\n",
        "            ('is_initial_author', Pipeline([\n",
        "              ('selector', ItemSelector(key='is_initial_author')),\n",
        "              ('tf-idf', TfidfVectorizer(tokenizer=tokenize_normalize)), \n",
        "              ])), \n",
        "            ('num_of_com', Pipeline([\n",
        "              ('selector', ItemSelector(key='num_of_com')),\n",
        "              ('tf-idf', TfidfVectorizer(tokenizer=tokenize_normalize)), \n",
        "              ])), \t\n",
        "            ('subreddit', Pipeline([\n",
        "              ('selector', ItemSelector(key='subreddit')),\n",
        "              ('tf-idf', TfidfVectorizer(tokenizer=tokenize_normalize)), \n",
        "              ])), \t\n",
        "            ('post_depth', Pipeline([\n",
        "              ('selector', ItemSelector(key='post_depth')),\n",
        "              ('tf-idf', TfidfVectorizer(tokenizer=tokenize_normalize)), \n",
        "              ])), \t\n",
        "            ('body2', Pipeline([\n",
        "              ('selector', ItemSelector(key='body')),\n",
        "              ('tf-idf', TfidfVectorizer(tokenizer=tokenize_normalize, ngram_range = (1,4))), \n",
        "              ])),\n",
        "            ('w2v', Pipeline([\n",
        "              ('selector', ItemSelector(key='w2v')),\n",
        "              ('tf-idf', TfidfVectorizer(tokenizer=spacy_tokenize)), \n",
        "              ])), \t\n",
        "             \n",
        "        ])\n",
        "        )\n",
        "    ])\n",
        "\n",
        "# Define the features\n",
        "X_train = prediction_pipeline_a.fit_transform(train_posts)\n",
        "X_test = prediction_pipeline_a.transform(test_posts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1zTKRnkjjwKe",
        "colab_type": "code",
        "outputId": "eeae588a-2cc8-4f3a-e048-7c46aa008a29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 658
        }
      },
      "cell_type": "code",
      "source": [
        "# LogisticRegression,TF-IDF\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "logreg = LogisticRegression()\n",
        "logreg_model = logreg.fit(X_train, train_posts['discourse_type'])\n",
        "predictions = logreg_model.predict(X_test)  \n",
        "evaluation_summary(\"LogisticRegression  with TF-IDF\", predictions, test_posts['discourse_type'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Evaluation for: LogisticRegression  with TF-IDF\n",
            "Classifier 'LogisticRegression  with TF-IDF' has Acc=0.624 P=0.324 R=0.554 F1=0.353\n",
            "                  precision    recall  f1-score   support\n",
            "\n",
            "       agreement      0.279     0.567     0.374       467\n",
            "    announcement      0.189     0.734     0.301        94\n",
            "          answer      0.868     0.652     0.744     10576\n",
            "    appreciation      0.601     0.778     0.678      1327\n",
            "    disagreement      0.062     0.354     0.106       113\n",
            "     elaboration      0.496     0.433     0.463      4162\n",
            "           humor      0.026     0.364     0.049        33\n",
            "negativereaction      0.029     0.429     0.055        21\n",
            "           other      0.037     0.483     0.069        29\n",
            "        question      0.649     0.744     0.693      2990\n",
            "\n",
            "       micro avg      0.624     0.624     0.624     19812\n",
            "       macro avg      0.324     0.554     0.353     19812\n",
            "    weighted avg      0.714     0.624     0.656     19812\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[ 265    0  274   41    3  322    2    2    1   41]\n",
            " [   0   69    0    0    0    0    0    0    0  296]\n",
            " [  42    0 6892   55   15  811    4    0    2  119]\n",
            " [  22    0  397 1033    2  208    4    4    4   46]\n",
            " [  19    0  243    9   40  295    0    1    0   34]\n",
            " [  73    0 1525   79   27 1802    3    0    6  115]\n",
            " [   7    4  190   29    4  156   12    1    1   53]\n",
            " [   7    0  144   17    4  101    3    9    0   22]\n",
            " [   6    4  167   28    2  111    2    2   14   40]\n",
            " [  26   17  744   36   16  356    3    2    1 2224]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}